

# Erklärung zur Barrierefreiheit

Diese Erklärung zur Barrierefreiheit gilt für die unter
[https://doc.zih.tu-dresden.de](https://doc.zih.tu-dresden.de),
[https://doc.hpc.tu-dresden.de](https://doc.hpc.tu-dresden.de),
[https://compendium.hpc.tu-dresden.de](https://compendium.hpc.tu-dresden.de) und
[https://hpc-wiki.zih.tu-dresden.de](https://hpc-wiki.zih.tu-dresden.de) veröffentlichte Website
der Technischen Universität Dresden.
Als öffentliche Stelle im Sinne des Barrierefreie-Websites-Gesetz (BfWebG) ist die Technische
Universität Dresden bemüht, ihre Websites und mobilen Anwendungen im Einklang mit den Bestimmungen
des Barrierefreie-Websites-Gesetz (BfWebG) in Verbindung mit der
Barrierefreie-Informationstechnik-Verordnung (BITV 2.0) barrierefrei zugänglich zu machen.

## Erstellung dieser Erklärung zur Barrierefreiheit

Diese Erklärung wurde am 17.09.2020 erstellt und zuletzt am 17.09.2020 aktualisiert. Grundlage der
Erstellung dieser Erklärung zur Barrierefreiheit ist eine am 17.09.2020 von der TU Dresden
durchgeführte Selbstbewertung.

## Stand der Barrierefreiheit

Es wurde bisher noch kein BITV-Test für die Website durchgeführt. Dieser ist bis 30.11.2020 geplant.

## Kontakt

Sollten Ihnen Mängel in Bezug auf die barrierefreie Gestaltung auffallen, können Sie uns diese über
das Formular [Barriere melden](https://tu-dresden.de/barrierefreiheit/barriere-melden) mitteilen und
im zugänglichen Format anfordern. Alternativ können Sie sich direkt an die Meldestelle für Barrieren
wenden (Koordinatorin: Mandy Weickert, E-Mail: <barrieren@tu-dresden.de>, Telefon: +49 351
463-42022, Fax: +49 351 463-42021, Besucheradresse: Nöthnitzer Straße 46, APB 1102, 01187 Dresden).

## Durchsetzungsverfahren

Wenn wir Ihre Rückmeldungen aus Ihrer Sicht nicht befriedigend bearbeiten, können Sie sich an die
Sächsische Durchsetzungsstelle wenden:

Beauftragter der Sächsischen Staatsregierung für die Belange von Menschen mit Behinderungen
Albertstraße 10
01097 Dresden
Postanschrift: Archivstraße 1, 01097 Dresden
E-Mail: <info.behindertenbeauftragter@sk.sachsen.de>
Telefon: +49 351 564-12161
Fax: +49 351 564-12169
Webseite: [https://www.inklusion.sachsen.de/](https://www.inklusion.sachsen.de/)


# Datenschutzerklärung

Zur Bereitstellung des Dienstes werden folgende personenbeziehbaren Daten verarbeitet: IP Addresse.

Eine Nutzung dieser Daten für andere Zwecke erfolgt nicht. Eine Speicherung dieser Daten erfolgt nur
zur Fehleranalyse. Eine Übermittlung dieser Daten an Dritte erfolgt nur, wenn dies gesetzlich
bestimmt ist.

Jeder Nutzer kann sich jederzeit an den [Datenschutzbeauftragten der TU
Dresden](https://tu-dresden.de/tu-dresden/organisation/gremien-und-beauftragte/beauftragte/datenschutzbeauftragter)
sowie an die [zuständige Aufsichtsbehörde für den Datenschutz](https://www.saechsdsb.de/) wenden.

Weiterhin besteht die Möglichkeit jederzeit Auskunft über die zu seiner Person verarbeiteten Daten
zu verlangen und es steht eine Antwort mit der Frist von einem Monat nach Eingang des
Auskunftsersuchens zu.


# ZIH HPC Documentation

This is the documentation of the HPC systems and services provided at
[TU Dresden/ZIH](https://tu-dresden.de/zih/).

This documentation will be continuously updated, since we try
to incorporate more information with increasing experience and with every question you ask us.

If the provided HPC systems and services helped to advance your research, please cite us. Why this
is important and acknowledgment examples can be found in the section
[Acknowledgement](application/acknowledgement.md).

## Contribution

The HPC team invites you to take part in the improvement of these pages by correcting or adding
useful information. Your contributions are highly welcome!

The easiest way for you to contribute is to report issues via
the GitLab
[issue tracking system](https://gitlab.hrz.tu-chemnitz.de/zih/hpcsupport/hpc-compendium/-/issues).
Please check for any already existing issue before submitting your issue in order to avoid duplicate
issues.

Please also find out the other ways you could contribute in our
[guidelines how to contribute](contrib/howto_contribute.md).

!!! tip "Reminder"

    Non-documentation issues and requests need to be send to
    [hpc-support@tu-dresden.de](mailto:hpc-support@tu-dresden.de).

## News

* **2024-12-13** Regular user operation of the new
  [GPU cluster `Capella`](jobs_and_resources/capella.md) started
* **2024-11-18** The GPU cluster [`Capella`](jobs_and_resources/hardware_overview.md#capella) was
  ranked #51 in the [TOP500](https://top500.org/system/180298/), #3 in the German systems and #5 in
  the [GREEN500](https://top500.org/lists/green500/list/2024/11/) lists of the world's fastest
  computers in November 2024.
* **2024-11-04** Slides from the HPC Introduction tutorial in October
 2024 are available for [download now](misc/HPC-Introduction.pdf)

## Training and Courses

We offer a rich and colorful bouquet of courses from classical *HPC introduction*
([HPC introduction slides](misc/HPC-Introduction.pdf)) to various
*Performance Analysis* and *Machine Learning* trainings. Please refer to the page
[Training Offers](https://tu-dresden.de/zih/hochleistungsrechnen/nhr-training)
for a detailed overview of the courses and the respective dates at ZIH.

Furthermore, Center for Scalable Data Analytics and Artificial Intelligence
[ScaDS.AI](https://scads.ai) Dresden/Leipzig offers various trainings with HPC focus.
Current schedule and registration is available at the
[ScaDS.AI trainings page](https://scads.ai/transfer/teaching-and-training/).

## Support and Consultation

We offer regular
[public and personal consultation opportunities](support/support.md)
with HPC experts.


# Legal Notice

## Impressum

Es gilt das [Impressum der TU Dresden](https://tu-dresden.de/impressum) mit folgenden Änderungen:

### Ansprechpartner/Betreiber

Technische Universität Dresden
Zentrum für Informationsdienste und Hochleistungsrechnen
01062 Dresden

Tel.: +49 351 463-40000
Fax: +49 351 463-42328
E-Mail: servicedesk@tu-dresden.de

### Konzeption, Technische Umsetzung, Anbieter

Technische Universität Dresden
Zentrum für Informationsdienste und Hochleistungsrechnen
Prof. Dr. Wolfgang E. Nagel
01062 Dresden

Tel.: +49 351 463-35450
Fax: +49 351 463-37773
E-Mail: zih@tu-dresden.de

## License

This documentation and the repository have two licenses:

* All documentation is licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).
* All software components are licensed under [MIT license](license_mit.txt).


# Desktop Cloud Visualization (DCV)

NICE DCV enables remote accessing OpenGL 3D applications running on ZIH systems using the
server's GPUs. If you don't need OpenGL acceleration, you might also want to try our
[WebVNC](graphical_applications_with_webvnc.md) solution.

See [the official DCV documentation](https://docs.aws.amazon.com/dcv/latest/userguide/client-web.html)
if you want to know whether your browser is supported by DCV.

## Access with JupyterHub

**Check out our new documentation about** [Virtual Desktops](../software/virtual_desktops.md).

To start a JupyterHub session on the cluster `vis` (`vis[1-4]`) with one GPU, 2 CPU cores
and 2 GB memory per core, click on:

[https://jupyterhub.hpc.tu-dresden.de/hub/spawn#/~(cluster~'vis~nodes~'1~ntasks~'1~cpuspertask~'2~mempercpu~'2048)](https://jupyterhub.hpc.tu-dresden.de/hub/spawn#/~(cluster~'vis~nodes~'1~ntasks~'1~cpuspertask~'2~mempercpu~'2048))

Optionally, you can modify many different Slurm parameters. For this
follow the general [JupyterHub](../access/jupyterhub.md) documentation.

Your browser should load JupyterLab, which looks like this:

![JupyterLab and DCV](misc/jupyterlab_and_dcv.png)
{: align="center"}

Click on the `DCV` button. A new tab with the DCV client will be opened.

## Notes on GPU Support

- Check GPU support via:

```console hl_lines="4"
marie@compute$ glxinfo | head
name of display: :1
display: :1  screen: 0
direct rendering: Yes
[...]
```

If direct rendering is not set to `Yes`, please contact [HPC support](mailto:hpc-support@tu-dresden.de).

- Expand LD_LIBRARY_PATH:

```console
marie@compute$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64/nvidia
```


# Graphical Applications with WebVNC

While many of the applications that run on ZIH systems are used from the command line,
graphical user interfaces are sometimes beneficial for particular use cases.
In order to simplify the setup, few solutions are available, which are described in the following.

The solutions provided here are based on a Singularity container with a VNC setup that can be
used as an alternative to SSH's X-Forwarding option to start graphical applications.

Internally, the solution utilizes [noVNC](https://novnc.com) to offer a web-based client that you
can use with your browser, so there's no additional client software necessary.

## Access via JupyterHub

**Check out our new documentation about [Virtual Desktops](../software/virtual_desktops.md).**

To start a JupyterHub session on the cluster `vis` with one CPU
cores and  1,5 GB memory per core, click on:

[https://jupyterhub.hpc.tu-dresden.de/hub/spawn#/~(cluster~'vis~nodes~'1~ntasks~'1~cpuspertask~'1~mempercpu~'1536)](https://jupyterhub.hpc.tu-dresden.de/hub/spawn#/~(cluster~'vis~nodes~'1~ntasks~'1~cpuspertask~'1~mempercpu~'1536))

Optionally, you can modify many different Slurm parameters.
For this, follow the general [JupyterHub](../access/jupyterhub.md) documentation.

Your browser should load JupyterLab, which looks like this:

![JupyterLab and WebVNC](misc/jupyterlab_and_webvnc.png)
{: align="center"}

Click on the `WebVNC` button. A new tab with the noVNC client will be opened.

## Access via terminal

### Step 1

Start the `runVNC` script in our prepared container in an interactive batch job (here with 2 cores
and 1,5 GB of memory per core):

```console
marie@login$ srun --pty --mem-per-cpu=1536 --cpus-per-task=2 --time=4:00:00 singularity exec /scratch/singularity/xfce.sif runVNC
[...]
```

Of course, you can adjust the batch job parameters to your liking. Note that the default time limit
in cluster `vis` is only 1 hour, so you should specify a longer one with `--time` (or `-t`).

The script will automatically generate a self-signed SSL certificate and place it in your home
directory under the name `self.pem`. This path can be overridden via the parameter `--cert` to
`runVNC`.

On success, it will give you an URL and a one-time password:

```console
[...]
Note: Certificate file /home/user/self.pem already exists. Skipping generation.  Starting VNC
server...  Server started successfully.  Please browse to: https://172.24.146.46:5901/vnc.html
The one-time password is: 71149997
```

### Step 2

Direct access to the compute nodes is not allowed. Therefore, you have to create a tunnel from your
laptop or workstation to the specific compute node and port as follows.

```bash
marie@local$ ssh -NL <local port>:<compute node>:<remote port> barnard
```

e.g.

```console
marie@local$ ssh -NL 5901:172.24.146.46:5901 barnard
```

!!! important "SSH command"

    The previous SSH command requires that you have already set up your [SSH configuration
    ](../access/ssh_login.md#configuring-default-parameters-for-ssh).

### Step 3

Open your local web-browser and connect to the following URL, replacing `<local port>` with the
local port you specified in the previous step:

```
https://localhost:<local port>/vnc.html
```

e.g.

```
https://localhost:5901/vnc.html
```

Since you are using a self-signed certificate and the node does not have a public DNS name, your
browser will not be able to verify it and you will have to add an exception (via the "Advanced"
button).

### Step 4

On the website, click the `Connect` button and enter the one-time password that was previously
displayed in order to authenticate. You will then see an Xfce4 desktop and can start a terminal in
there, where you can use the "ml" or "module" command as usual to load and then run your graphical
applications. Enjoy!


# JupyterHub

With our JupyterHub service, we offer you a quick and easy way to work with
Jupyter notebooks on ZIH systems. This page covers starting and stopping
JupyterHub sessions, error handling and customizing the environment.

We also provide a comprehensive documentation on how to use
[JupyterHub for Teaching (git-pull feature, quickstart links, direct links to notebook files)](jupyterhub_for_teaching.md).

## Disclaimer

!!! warning

    The JupyterHub service is provided *as-is*, use at your own discretion.

Please understand that JupyterHub is a complex software system of which we are
not the developers and don't have any downstream support contracts for, so we
merely offer an installation of it but cannot give extensive support in every
case.

## Access

!!! note

    This service is only available for users with an active HPC project.
    See [Application for Login and Resources](../application/overview.md), if
    you need to apply for an HPC project.

JupyterHub is available at
[https://jupyterhub.hpc.tu-dresden.de](https://jupyterhub.hpc.tu-dresden.de).

## Login Page

At login page please use your ZIH credentials (without @tu-dresden.de).

![Simple form](misc/jupyterhub_loginpage_marie.png)
{: align="center"}

## Start a Session

Start a new session by clicking on the `Start` button.

### Standard Profiles

Our simple form offers you the most important settings to start quickly.

![Advanced form](misc/jupyterhub_profile_selector.png)
{: align="center"}

We have created three profiles for each cluster, namely:

| Cluster       | Resources                    | Optimized for   | Recommended for |
|-------|--------------------------------------|-----------------|---------------------------|
| Alpha | 1 core, 1.5 GB 1 hour                | x86_64 (AMD)    | Python programming        |
| Alpha | 2 core, 3 GB, 4 hours                | x86_64 (AMD)    | R programming             |
| Alpha | 4 core, 8 GB, 8 hours                | x86_64 (AMD)    |                           |
| Barnard | 1 core, 1.5 GB, 1 hour             | x86_64 (Intel)  | Python programming        |
| Barnard | 2 core, 3 GB, 4 hours              | x86_64 (Intel)  | Julia and R programming   |
| Barnard | 4 core, 8 GB, 8 hours              | x86_64 (Intel)  |                           |
| Capella | 1 core, 1.5 GB, 1 hour             | x86_64 (AMD)    | Python programming        |
| Capella | 2 core, 3 GB, 4 hours              | x86_64 (AMD)    | R programming             |
| Capella | 4 core, 8 GB, 8 hours              | x86_64 (AMD)    |                           |
| Romeo | 1 core, 1.5 GB 1 hour                | x86_64 (AMD)    | Python programming        |
| Romeo | 2 core, 3 GB, 4 hours                | x86_64 (AMD)    | R programming             |
| Romeo | 4 core, 8 GB, 8 hours                | x86_64 (AMD)    |                           |
| VIS | 1 core, 1.5 GB, 1 hours                | Visualization   | ANSYS                     |
| VIS | 2 core, 4 GB, 2 hours                  | Visualization   | ANSYS                     |
| VIS | 4 core, 8 GB, 6 hours                  | Visualization   | ANSYS                     |

### Advanced Options

Aside of the standard profiles there is the possibility to specify custom parameters to the job
spawner. This can be activated cby licking at the `Advanced` button located below to the profile
list.

![Advanced form](misc/jupyterhub_advanced_form.png)
{: align="center"}

## JupyterLab

After your session it is spawned you will be redirected to JupyterLab. The main interface looks
like as following:

![JupyterLab overview](misc/jupyterlab_overview_2023-12-19.png)
{: align="center"}

!!! note

    Your interface may differ from the picture above as not all icons (launchers) are available
    at all profiles.

The main workspace is used for multiple notebooks, consoles or
terminals. Those documents are organized with tabs and a very versatile
split screen feature. On the left side of the screen you can open
several views:

  - file manager
  - controller for running kernels and terminals
  - overview of commands and settings
  - details about selected notebook cell
  - list of open tabs

At the following table it's possible to see what is available at each cluster.

| Cluster              | Julia  | R  | RStudio | MATLAB | MATLAB Web | WebVNC | DCV |
|----------------------|--------|----|---------|--------|------------|--------|-----|
| Alpha                |   -    | OK |    OK   |   OK   |     OK*    |   OK*  |  -  |
| Barnard              |   OK   | OK |    OK   |   OK   |     OK*    |   OK*  |  -  |
| Capella              |   OK   | OK |    OK   |   OK   |     -      |   -    |  -  |
| Romeo                |   -    | OK |    OK   |   OK   |     OK*    |   OK*  |  -  |
| VIS                  |   OK   | OK |    OK   |   OK   |     OK*    |   OK*  |  OK |

!!! note "*Note"

    All small profiles for all clusters do not include MATLAB Web neither WebVNC.

## Jupyter Notebooks in General

In JupyterHub, you can create scripts in notebooks. Notebooks are programs which are split into
multiple logical code blocks. Each block can be executed individually. In between those code
blocks, you can insert text blocks for documentation. Each notebook is paired with a kernel running
the code.

We offer some custom kernel for Julia*, MATLAB and R.

!!! note

    Some kernels may not be available depending of the amounting of resources requested for the
    job or due to the availability of modules to an specific cluster.

### Version Control of Jupyter Notebooks with Git

Since Jupyter notebooks are files containing multiple blocks for input code,
documentation, output and further information, it is difficult to use them with
Git. Version tracking of the `.ipynb` notebook files can be improved with the
[Jupytext plugin](https://jupytext.readthedocs.io/en/latest/). Jupytext will
provide Markdown (`.md`) and Python (`.py`) conversions of notebooks on the fly,
next to `.ipynb`. Tracking these files will then provide a cleaner Git history.
A further advantage is that Python notebook versions can be imported, allowing
to split larger notebooks into smaller ones, based on chained imports.

!!! note

    The Jupytext plugin is not installed on the ZIH systems at the moment.
    Currently, it can be [installed](https://jupytext.readthedocs.io/en/latest/install.html)
    by the users with parameter `--user`.
    Therefore, `ipynb` files need to be made available in a repository for shared
    usage within the ZIH system.

## Stop a Session

It is good practice to stop your session once your work is done. This releases
resources for other users and your quota is less charged. If you just log out or
close the window, your server continues running and **will not stop** until the
Slurm job runtime hits the limit (usually 8 hours).

At first, you have to open the JupyterHub control panel.

=== "JupyterLab"

    Open the file menu and then click on `Logout`. You can
    also click on `Hub Control Panel`, which opens the control panel in a new tab instead.

    ![JupyterLab logout](misc/jupyterlab_logout.png)
    {: align="center"}

## Error Handling

We want to explain some errors that you might face sooner or later.
If you need help, open a ticket and ask for support as described in
[How to Ask for Support](../support/support.md).

### Error Message in JupyterLab

![JupyterLab error directory not found](misc/jupyterlab_error_directory_not_found.png)
{: align="center"}

If the connection to your notebook server unexpectedly breaks, you will get this
error message. Sometimes your notebook server might hit a batch system or
hardware limit and gets killed. Then, the log file of the corresponding
batch job usually contains useful information. These log files are located in your
home directory and have the name `jupyterhub-<clustername>.log`.

## Advanced Tips

### Loading Modules

Inside your terminal session you can load modules from the [module system](../software/modules.md).

### Custom Kernels

As you might have noticed, after launching Jupyter**Lab**,
there are several boxes with icons therein visible in the `Launcher`.
Each box therein represents a so called 'Kernel'.
(note that these are not to be confused with operating system kernel,
but similarly provide basic functionality for running your use cases,
e.g. Python or R)

You can also [create your own Kernels](jupyterhub_custom_environments.md).


# Custom Environments for JupyterHub

!!! info

    Interactive code interpreters which are used by Jupyter notebooks are called
    *kernels*. Creating and using your own kernel has the benefit, that you can
    install your own preferred Python packages and use them in your notebooks.

We currently have two different architectures at ZIH systems.
Build your kernel environment on the **same architecture** that you want to use
later on with the kernel. In the examples below, we use the name
"my-kernel" for our user kernel. We recommend to prefix your kernels
with keywords like `alpha`, `barnard`, `romeo`, `power9`, `venv`, `conda`.
This way, you can later recognize easier how you built the kernel and on which hardware it
will work. Depending on that hardware, allocate resources as follows.

| Cluster    | Architecture name       |
|------------|-------------------------|
| Alpha      | x86_64 (AMD)    |
| Barnard    | x86_64 (Intel)  |
| Capella    | x86_64 (AMD)    |
| Romeo      | x86_64 (AMD)    |
| Power9     | ppc64le (IBM)   |

## Preliminary Steps

    Start an interactive job

    ```console
    maria@login.<cluster>$ srun --pty --ntasks=1 --cpus-per-task=2 \
     --mem-per-cpu=2541 --time=02:00:00 bash -l
    ```

When creating a virtual environment in your home directory, you got to decide
to either use "Python virtualenv" or "conda environment".

!!! note
    Please keep in mind that Python virtualenv is the preferred way to create a Python
    virtual environment.
    For working with conda virtual environments, it may be necessary to configure your shell
    as described in [Python virtual environments](../software/python_virtual_environments.md#conda-virtual-environment)

## Python Virtualenv

While we have a general description on
[Python Virtual Environments](../software/python_virtual_environments.md), here we have a more detailed
description on using them with JupyterHub:

Depending on the Cluster that you are targeting, please choose the right modules:

=== "release/23.10"

    For use with Python version 3.10.4,
    please try to initialize your Python Virtual Environment like this:

    ```console
    [marie@barnard ~]$ module load release/23.10  GCC/11.3.0  Python/3.10.4
    Module GCC/11.3.0, Python/3.10.4 and 12 dependencies loaded.
    ```

=== "release/23.04"

    ```console
    [marie@barnard ~]$ module load release/23.04  GCC/11.3.0  Python/3.10.4
    Module GCC/11.3.0, Python/3.10.4 and 12 dependencies loaded.
    ```

Then continue with the steps below.

    ```console
    [marie@barnard ~]$ mkdir -p ~/usr/jlab-kernels # please use workspaces!
    [marie@barnard ~]$ cd ~/usr/jlab-kernels
    [marie@barnard jlab-kernels]$ python3 -m venv --system-site-packages my-kernel
    [marie@barnard jlab-kernels]$
    [marie@barnard jlab-kernels]$ source my-kernel/bin/activate
    (my-kernel) [marie@barnard jlab-kernels]$
    (my-kernel) [marie@barnard jlab-kernels]$ pip install ipykernel
    Collecting ipykernel
    [...]
    Successfully installed [...] ipykernel-x.x.x ipython-x.x.x [...]
    ```

After following the initialization of the environment (above),
the usage of Python's Package manager `pip` is the same:

```console
(my-kernel) marie@compute$ pip install --upgrade pip
(my-kernel) marie@compute$ python -m ipykernel install --user --name my-kernel --display-name="my kernel"
Installed kernelspec my-kernel in .../.local/share/jupyter/kernels/my-kernel
(my-kernel) marie@compute$ pip install [...] # now install additional packages for your notebooks
(my-kernel) marie@compute$ deactivate
```

## Conda Environment

Load the needed module depending on Cluster architecture:

=== "Nodes with x86_64 CPU"
    ```console
    marie@compute$ module load Anaconda3
    ```

=== "Nodes with ppc64le CPU"
    ```console
    marie@ml$ module load PythonAnaconda
    ```

!!! hint
    For working with conda virtual environments, it may be necessary to configure your shell as
    described in
    [Python virtual environments](../software/python_virtual_environments.md#conda-virtual-environment).

Continue with environment creation, package installation and kernel
registration:

```console
marie@compute$ mkdir user-kernel # please use workspaces!
marie@compute$ conda create --prefix $HOME/user-kernel/my-kernel python=3.8.6
Collecting package metadata: done
Solving environment: done
[...]
marie@compute$ conda activate $HOME/user-kernel/my-kernel
marie@compute$ conda install ipykernel
Collecting package metadata: done
Solving environment: done
[...]
marie@compute$ python -m ipykernel install --user --name my-kernel --display-name="my kernel"
Installed kernelspec my-kernel in [...]
marie@compute$ conda install [..] # now install additional packages for your notebooks
marie@compute$ conda deactivate
```

## Using your custom environment

Now you can start a new session and your kernel should be available.

=== "JupyterLab"
    Your kernels are listed on the launcher page:

    ![JupyterLab user kernel launcher](misc/jupyterlab_user_kernel_launcher.png)
    {: align="center"}

    You can switch kernels of existing notebooks in the menu:

    ![JupyterLab change kernel](misc/jupyterlab_change_kernel.png)
    {: align="center"}

=== "Classic Jupyter notebook"
    Your kernel is listed in the New menu:

    ![Jupyter notebook user kernel launcher](misc/jupyter_notebook_user_kernel_launcher.png)
    {: align="center"}

    You can switch kernels of existing notebooks in the kernel menu:

    ![Jupyter notebook change kernel](misc/jupyter_notebook_change_kernel.png)
    {: align="center"}

!!! note
    Both python venv and conda virtual environments will be mentioned in the same
    list.


# JupyterHub for Teaching

On this page, we want to introduce to you some useful features if you want to
use JupyterHub for teaching.

!!! note

    JupyterHub uses compute resources from ZIH systems.

Please be aware of the following notes:

- ZIH systems operate at a lower availability level than your usual Enterprise
Cloud VM. There can always be downtimes, e.g. of the filesystems or the batch
system.
- Scheduled downtimes are announced by e-mail. Please plan your courses
accordingly.
- Access to HPC resources is handled through projects. See your course as a
project. Projects need to be registered beforehand (more info on the page
[Access](../application/overview.md)).
- Don't forget to [add your users](../application/project_management.md#manage-project-members-dis-enable)
  (e.g. students or tutors) to your project.
- It might be a good idea to [request a reservation](../jobs_and_resources/overview.md#exclusive-reservation-of-hardware)
  of part of the compute resources for your project/course to avoid unnecessary
  waiting times in the batch system queue.

## Clone a Repository With a Link

This feature bases on [nbgitpuller](https://github.com/jupyterhub/nbgitpuller).
Further information can be found in the [external documentation about nbgitpuller](https://jupyterhub.github.io/nbgitpuller/).

This extension for Jupyter notebooks can clone every public Git repository into
the users work directory. It's offering a quick way to distribute notebooks and
other material to your students.

![Git pull progress screen](misc/gitpull_progress.png)
{: align="center"}

To create a shareable link, we recommend to use [URL encoding](https://en.wikipedia.org/wiki/Percent-encoding)
instead of plain text for the link in order to avoid defective links. The
[nbgitpuller link generator](https://jupyterhub.github.io/nbgitpuller/link?hub=https://jupyterhub.hpc.tu-dresden.de/)
supports you in generating valid links for sharing.

??? example
    A shareable link for this feature looks like this:
    ```
    https://jupyterhub.hpc.tu-dresden.de/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fjdwittenauer%2Fipython-notebooks&urlpath=tree%2Fipython-notebooks%2Fnotebooks%2Flanguage%2FIntro.ipynb
    ```

!!! warning
    For illustration purposes, we use plain text links in the following parts. In practice, we
    highly recommend to use URL encoded links instead.

![URL with git-pull parameters](misc/url-git-pull.png)
{: align="center"}

This example would clone the repository
[https://github.com/jdwittenauer/ipython-notebooks](https://github.com/jdwittenauer/ipython-notebooks)
and afterwards open the **Intro.ipynb** notebook in the given path.

The following parameters are available:

| Parameter | Info |
|---|---|
|`repo`    | path to Git repository|
|`branch`  | branch in the repository to pull from default: `master`|
|`urlpath` | URL to redirect the user to a certain file, [more info about parameter urlpath](https://jupyterhub.github.io/nbgitpuller/topic/url-options.html#urlpath)|
|`depth`   | clone only a certain amount of latest commits not recommended|

## Spawn Options Pass-through with URL Parameters

The spawn form now offers a quick start mode by passing URL parameters.

!!! example

    The following link would create a jupyter notebook session on the
    cluster `barnard` with 1 CPU and 1,5 GB RAM:

    ```
    https://jupyterhub.hpc.tu-dresden.de/hub/spawn#/~(cluster~'barnard~nodes~'1~ntasks~'1~cpuspertask~'1~mempercpu~'1536)
    ```

![URL with quickstart parameters](misc/url-quick-start.png)
{: align="center"}

Every parameter of the advanced form can be set with this parameter. If the
parameter is not mentioned, the default value will be loaded.

| Parameter       | Default Value                            |
|:----------------|:-----------------------------------------|
| `cluster`         | barnard                                  |
| `nodes`           | 1                                        |
| `ntasks`          | 1                                        |
| `cpuspertask`     | 1                                        |
| `gres`            | *empty* (no generic resources)           |
| `mempercpu`       | 1000                                     |
| `runtime`         | 1:00:00                                  |
| `reservation`     | *empty* (use no reservation)             |
| `project`         | *empty* (use default project)            |
| `workspace_scope` | *empty* (home directory)                 |

You can use the advanced form to generate a URL for the settings you want. The
address bar contains the encoded parameters starting with `#/`.

### Combination of Quickstart and Git-Pull Feature

You can combine both features in a single link:

```
https://jupyterhub.hpc.tu-dresden.de/hub/user-redirect/git-pull?repo=https://github.com/jdwittenauer/ipython-notebooks&urlpath=/tree/ipython-notebooks/notebooks/language/Intro.ipynb#/~(cluster~'barnard~nodes~'1~ntasks~'1~cpuspertask~'1~mempercpu~'1536)
```

![URL with quickstart parameters](misc/url-git-pull-and-quick-start.png)
{: align="center"}

## Open a Notebook Automatically with a Single Link

With the following link you will be redirected to a certain file in your
home directory.

[https://jupyterhub.hpc.tu-dresden.de/user-redirect/notebooks/demo.ipynb](https://jupyterhub.hpc.tu-dresden.de/user-redirect/notebooks/demo.ipynb)

The file needs to exist, otherwise a 404 error will be thrown.

![URL with git-pull and quickstart parameters](misc/url-user-redirect.png)
{: align="center"}

This link would redirect to
`https://jupyterhub.hpc.tu-dresden.de/user/{login}/notebooks/demo.ipynb`.

## Create a Shared Python Environment

To provide a consistent Python environment, you can create a shared [workspace](../data_lifecycle/workspaces.md)
and prepare a [Python virtual environment](../software/python_virtual_environments.md)
in it. Then use a custom Jupyter Kernel to use this environment in JupyterHub.
Please note the following:

- Set the correct permissions to the workspace and all relevant subdirectories
and files via `chmod`.

- Install all relevant Python packages in the shared Python virtual environment
(either pip or conda). Note that standard environments (as *production* or
*test*) are not available in that case.

- Modules can also be loaded in the Jupyter spawner via preload modules
(considering the Python version of your virtual environment).

Set up your shared Python virtual environment for JupyterHub.
!!! hint
    For working with conda virtual environments, it may be necessary to configure your shell as
    described in [Python virtual environments](../software/python_virtual_environments.md#conda-virtual-environment).

=== "virtualenv"

    ```console
    marie@compute$ module load Python #Load default Python
    [...]
    marie@compute$ ws_allocate -F scratch python_virtual_environment_teaching 1
    Info: creating workspace.
    /scratch/ws/1/python_virtual_environment_teaching
    [...]
    marie@compute$ virtualenv --system-site-packages /scratch/ws/1/python_virtual_environment_teaching/env #Create virtual environment
    [...]
    marie@compute$ source /scratch/ws/1/python_virtual_environment_teaching/env/bin/activate    #Activate virtual environment. Example output: (envtest) bash-4.2$
    marie@compute$ pip install ipykernel
    Collecting ipykernel
    [...]
    Successfully installed ... ipykernel-5.1.0 ipython-7.5.0 ...
    marie@compute$ pip install --upgrade pip
    marie@compute$ python -m ipykernel install --user --name my-teaching-kernel --display-name="my teaching kernel"
    Installed kernelspec my-teaching-kernel in .../.local/share/jupyter/kernels/my-teaching-kernel
    marie@compute$ pip install [...] #Now install additional packages for your notebooks
    marie@compute$ deactivate
    marie@compute$ chmod g+rx /scratch/ws/1/python_virtual_environment_teaching -R #Make the environment accesible for others

    ```

=== "conda"

    ```console
    marie@compute$ module load Anaconda3 #Load Anaconda
    [...]
    marie@compute$ ws_allocate -F scratch conda_virtual_environment_teaching 1
    Info: creating workspace.
    /scratch/ws/1/conda_virtual_environment_teaching
    [...]
    marie@compute$ conda create --prefix /scratch/ws/1/conda_virtual_environment_teaching/conda-env python=3.8 #create virtual environment with Python version 3.8
    [...]
    marie@compute$ conda activate /scratch/ws/1/conda_virtual_environment_teaching/conda-env #activate conda-env virtual environment
    marie@compute$ conda install ipykernel
    [...]
    marie@compute$ python -m ipykernel install --user --name my-teaching-kernel --display-name="my teaching kernel"
    Installed kernelspec my-teaching-kernel in .../.local/share/jupyter/kernels/my-teaching-kernel
    marie@compute$ conda install [...] # now install additional packages for your notebooks
    marie@compute$ conda deactivate
    marie@compute$ chmod g+rx /scratch/ws/1/conda_virtual_environment_teaching -R #Make the environment accesible for others

    ```

Now, users have to install the kernel in order to use the shared Python virtual
environment in JupyterHub:
=== "virtualenv"

    ```console
    marie@compute$ module load Python #Load default Python
    [...]
    marie@compute$ source /scratch/ws/1/python_virtual_environment_teaching/env/bin/activate #Activate virtual environment. Example output: (envtest) bash-4.2$
    marie@compute$ python -m ipykernel install --user --name my-teaching-kernel --display-name="my teaching kernel"
    Installed kernelspec my-teaching-kernel in .../.local/share/jupyter/kernels/my-teaching-kernel
    marie@compute$ deactivate

    ```

=== "conda"

    ```console
    marie@compute$ module load Anaconda3 #Load Anaconda
    [...]
    marie@compute$ conda activate /scratch/ws/1/conda_virtual_environment_teaching
    marie@compute$ python -m ipykernel install --user --name my-teaching-kernel --display-name="my teaching kernel"
    Installed kernelspec my-teaching-kernel in .../.local/share/jupyter/kernels/my-teaching-kernel
    marie@compute$ conda deactivate

    ```

After spawning the Notebook, you can select the kernel with the created Python
virtual environment.

!!! hint
    You can also execute the commands for installing the kernel from the Jupyter
    as described in [JupyterHub Teaching Example](jupyterhub_teaching_example.md). Then users do not
    have to use the command line interface after the preparation.


# JupyterHub Teaching Example

Setting up a Jupyter Lab Course involves additional steps, beyond JupyterHub, such as creating
course specific environments and allowing participants to link and activate these environments during
the course. This page includes a work through of these additional steps, with best practice examples
for each part.

## Context

- The common situation described here is that one or several Jupyter Lab Notebooks
(`ipynb` files) are available and prepared. Students are supposed to open these notebooks
through the [ZIH JupyterHub](../access/jupyterhub.md) and work through them during a course.

- These notebooks are typically prepared for specific dependencies (Python packages)
that need to be activated by participants in the course, when opening the notebooks.

- These environments can either be chosen based on the pre-configured
ZIH virtualenv/conda environments,
or built in advance. We will focus on the custom environment approach here.

## Prerequisites

- A public Git repository with the notebook files (`ipynb`) and all other starting files required
  by participants. One option to host the repository is the [GitLab of TU Chemnitz](https://gitlab.hrz.tu-chemnitz.de/).
- A [HPC project](../application/project_management.md) for teaching,
  with students as registered participants
- For the tutor, a shell access to the HPC resources and project folder.

## Preparation on the Lecturer's Side

The following part describes several steps for the preparation of a course with the JupyterHub at
ZIH.

### 1. Creating a custom Python environment

Prepare a Python virtual environment (`virtualenv`) or conda virtual environment as described in
[Python virtual environments](../software/python_virtual_environments.md). Note, for preparing a
custom environment for a Jupyter Lab course, all participants will need to have read-access to this
environment. This is best done by storing the environment in either a [workspace](../data_lifecycle/workspaces.md)
with a limited lifetime or in a projects folder (e.g. `/projects/p_lv_jupyter_course/`) without a
limited lifetime.

### 2. Clone the repository and store environment setup

First prepare the `requirements.txt` or the `environment.yml` to persist the environment as
described in [Python virtual environments](../software/python_virtual_environments.md).

Then clone the repository of your course to your home directory or into a directory in the projects
folder and add the file to the repository.

=== "virtualenv"
    ```console
    marie@compute$ git clone git@gitlab.hrz.tu-chemnitz.de:zih/jupyterlab_course.git /projects/p_lv_jupyter_course/clone_marie/
    [...]
    marie@compute$ cp requirements.txt /projects/p_lv_jupyter_course/clone_marie/jupyterlab_course
    marie@compute$ cd /projects/p_lv_jupyter_course/clone_marie/jupyterlab_course
    marie@compute$ git add requirements.txt
    marie@compute$ git commit
    marie@compute$ git push

    ```
=== "conda"
    ```console
    marie@compute$ git clone git@gitlab.hrz.tu-chemnitz.de:zih/jupyterlab_course.git /projects/p_lv_jupyter_course/clone_marie/
    [...]
    marie@compute$ cp requirements.txt /projects/p_lv_jupyter_course/clone_marie/jupyterlab_course
    marie@compute$ cd /projects/p_lv_jupyter_course/clone_marie/jupyterlab_course
    marie@compute$ git add environment.yml
    marie@compute$ git commit
    marie@compute$ git push

    ```

Now, you can re-create the environment and the whole course from the Git repository in the future.

To test the activation of the environment use:

=== "virtualenv"

    ```console
    marie@compute$ source /scratch/ws/1/python_virtual_environment_teaching/env/bin/activate #Activate virtual environment. Example output: (envtest) bash-4.2$

    ```
=== "conda"

    ```console
    marie@compute$ conda activate /scratch/ws/1/conda_virtual_environment_teaching

    ```

### 3. Prepare an activation file

Create a file to install the `ipykernel` to the user-folder, linking the central `workshop_env` to
the ZIH JupyterLab. An `activate_workshop_env.sh` should have the following content:

```console
/projects/jupyterlab_course/workshop_env/bin/python -m ipykernel install --user --name workshop_env --display-name="workshop_env"
```

!!! note
    The file for installing the kernel should also be added to the Git repository.

### 4. Prepare the spawn link

Have a look at the instructions to prepare
[a custom spawn link in combination with the git-pull feature](jupyterhub_for_teaching.md#combination-of-quickstart-and-git-pull-feature).

## Usage on the Student's Side

### Preparing activation of the custom environment in notebooks

When students open the notebooks (e.g. through a Spawn Link that pulls the Git files
and notebooks from our repository), the Python environment must be activated first by installing a
Jupyter kernel. This can be done inside the first notebook using a shell command (`.sh`).

Therefore the students will need to run the `activation_workshop_env.sh` file, which can be done
in the first cell of the first notebook (e.g. inside `01_intro.ipynb`).

In a code cell in `01_intro.ipynb`, add:

```console
!cd .. && sh activate_workshop_env.sh
```

When students run this file, the following output signals a successful setup.

![Installed kernelspec](misc/kernelspec.png)
{: align="center"}

Afterwards, the `workshop_env` Jupyter kernel can be selected in the top-right corner of Jupyter
Lab.

!!! note
    A few seconds may be needed until the environment becomes available in the list.

## Test spawn link and environment activation

During testing, it may be necessary to reset the workspace to the initial state. There are two steps
involved:

First, remove the cloned Git repository in user home folder.

!!! warning
    Check carefully the syntax below, to avoid removing the wrong files.

```console
cd ~
rm -rf ./jupyterlab_course.git
```

Second, the IPython Kernel must be un-linked from the user workshop_env.

```console
jupyter kernelspec uninstall workshop_env
```

## Summary

The following video shows an example of the process of opening the
spawn link and activating the environment, from the students perspective.
Note that this video shows the case for a conda virtual environment.

![type:video](./misc/startup_hub.webm)

!!! note
    - The spawn link may not work the first time a user logs in.

    - Students must be advised to _not_ click "Start My Server" or edit the form,
    if the server does not start automatically.

    - If the server does not start automatically, click (or copy & paste) the spawn link again.


# JupyterLab

## Access without JupyterHub

### Access with port forwarding

   ```console
   [marie@login ~]$ start_jupyterlab.sh
   ```

![profile select](misc/jupyterlab_wout_hub.png)
{: align="center"}

   ```console
   Starting JupyterLab...
   Submitted batch job 1984
   [marie@login ~]$
   ```

Do not disconnect/logout.
Wait until you receive a message with further instructions on how to connect to yours lab session.

   ```console
   Message from marie@login on <no tty> at 14:22 ...

    At your local machine, run:
    ssh marie@login.<cluster>.hpc.tu-dresden.de -NL 8946:<node>:8138

    and point your browser to http://localhost:8946/?token=M7SHy...5HnsY...GMaRj0e2X
    To stop this notebook, run 'scancel 1984'
   EOF
   ```

### Access with X11 forwarding

=== "Alpha Centauri"

    ```console
    marie@local$ ssh -XC marie@login1.alpha.hpc.tu-dresden.de
    marie@login$ ml release/23.04 GCCcore/12.2.0
    marie@login$ ml Python/3.10.8
    marie@login$ source /software/util/JupyterLab/alpha/jupyterlab-4.0.4/bin/activate
    marie@login$ srun --nodes=1 --ntasks=1 --cpus-per-task=4 --mem-per-cpu=8192 --x11 --pty --gres=gpu:1 bash -l
    marie@compute$ jupyter lab -y
    ```

=== "Barnard"

    ```console
    marie@local$ ssh -XC marie@login1.barnard.hpc.tu-dresden.de
    marie@login$ ml release/23.10 GCCcore/12.2.0
    marie@login$ ml Python/3.10.8
    marie@login$ source /software/util/JupyterLab/barnard/jupyterlab-4.0.4/bin/activate
    marie@login$ srun --nodes=1 --ntasks=1 --cpus-per-task=4 --mem-per-cpu=8192 --x11 --pty bash -l
    marie@compute$ jupyter lab -y
    ```

=== "Capella"

    ```console
    marie@local$ ssh -XC marie@login1.capella.hpc.tu-dresden.de
    marie@login$ module load release/24.04 GCCcore/12.2.0
    marie@login$ module load Python/3.10.8
    marie@login$ source /software/util/JupyterLab/barnard/jupyterlab-4.0.4/bin/activate
    marie@login$ srun --nodes=1 --ntasks=1 --cpus-per-task=4 --mem-per-cpu=8192 --x11 --pty --gres=gpu:1 bash -l
    marie@compute$ jupyter lab -y
    ```

=== "Romeo"

    ```console
    marie@local$ ssh -XC marie@login1.romeo.hpc.tu-dresden.de
    marie@login$ ml release/23.04 GCCcore/12.2.0
    marie@login$ ml Python/3.10.8
    marie@login$ source /software/util/JupyterLab/romeo/jupyterlab-4.0.4/bin/activate
    marie@login$ srun --nodes=1 --ntasks=1 --cpus-per-task=4 --mem-per-cpu=8192 --x11 --pty bash -l
    marie@compute$ jupyter lab -y
    ```

=== "Visualization"

    ```console
    marie@local$ ssh -XC marie@login4.barnard.hpc.tu-dresden.de
    marie@login$ ml release/23.10 GCCcore/12.2.0
    marie@login$ ml Python/3.10.8
    marie@login$ source /software/util/JupyterLab/vis/jupyterlab-4.0.4/bin/activate
    marie@login$ export SLURM_CONF=/software/util/dcv/etc/slurm/slurm.conf
    marie@login$ srun --nodes=1 --ntasks=1 --cpus-per-task=4 --mem-per-cpu=8192 --x11 --pty bash -l
    marie@compute$ jupyter lab -y --browser=chromium-browser
    ```

    If you then want to start a DCV session, click on the DCV tile in the lower section 'DC Apps'
    and wait for the new browser tab to appear. Copy the URL and paste it into your local browser
    instead of working with the slow X11-forwarded browser. After you have successfully logged in
    with your ZIH credentials, it should work fine.


# Custom JupyterLab

Is it now possible to install your custom JupyterLab.

!!! note

    It's supported at barnard only now..

## Disclaimer

!!! warning

    Please note that our technicias will not mantain your own JupyterLab installation.

Please understand that JupyterLab is a complex software system of which we are
not the developers and don't have any downstream support contracts for, so we
merely offer an installation of it and will not provide additional support.

Start a new JupyterLab session at: [jupyterhub](https://jupyterhub.hpc.tu-dresden.de) as
described at: [start session](https://doc.zih.tu-dresden.de/access/jupyterhub/#start-a-session)

## Prepare the Installation

Create a new notebook using the `Python3 (ipykernel)` kernel.

Include at the cells the following commands:

   ```console
   install_script = ! which install_home_jlab.sh
   result = ! {install_script[0]}
   %run {result[0]}
   ```

After execute the kernel you will see the following:

![cell_commands2](misc/jupyterlab_user_2.png)
{: align="center"}

Follow the instructions displayed for continue to the installation, the click the install
button.

At end you will see the resume of yours installation:

![cell_commands2](misc/jupyterlab_user_3.png)
{: align="center"}

This is all, next time you spawn a session at JupyterHub you will use your own JupyterLab.

!!! note

    For using the system installation please remove/rename the folder ~/usr/opt/jupyterlab.


# Key Fingerprints

!!! hint

    The key fingerprints of login and export nodes can occasionally change. This page holds
    up-to-date fingerprints.

Each cluster can be accessed via so-called **login nodes** using specific hostnames. All login nodes
of a cluster share common SSH key fingerprints that are unique. When connecting to one of our HPC
systems via the corresponding login nodes, please make sure that the provided fingerprint matches
one of the table. This is the only way to ensure you are connecting to the correct server from the
very beginning. If the **fingerprint differs**, please contact the
[HPC support team](../support/support.md).

In case an additional login node of the same cluster is used, the key needs to be checked and
approved again.

??? example "Connecting with SSH to Barnard"

    ```console
    marie@local$ ssh login1.barnard.hpc.tu-dresden.de
    The authenticity of host 'login1.barnard.hpc.tu-dresden.de (172.24.95.28)' can't be established.
    ECDSA key fingerprint is SHA256:8Coljw7yoVH6HA8u+K3makRK9HfOSfe+BG8W/CUEPp0.
    Are you sure you want to continue connecting (yes/no)?
    ```

    In this case, the fingerprint matches the one given in the table. Thus, you can proceed by
    typing 'yes'.

??? info "Verify key fingerprints without login"

    To verify the fingerprint without login, use `ssh-keyscan` and `ssh-keygen`:

    ```console
    marie@local$ ssh-keyscan login1.barnard.hpc.tu-dresden.de 2>/dev/null | ssh-keygen -l -f -
    3072 SHA256:lVQOvnci07jkxmFnX58pQf3cD7lz1mf4K4b9jZrAlVU login1.barnard.hpc.tu-dresden.de (RSA)
    256 SHA256:Xan2MYazewT0V5agNazaQfWzLKBD3P48zRwR6reoXhI login1.barnard.hpc.tu-dresden.de (ECDSA)
    256 SHA256:Gn4n5IX9eEvkpOGrtZzs9T9yAfJUB200bgRchchiKAQ login1.barnard.hpc.tu-dresden.de (ED25519)
    ```

## Barnard

The cluster [`Barnard`](../jobs_and_resources/hardware_overview.md#barnard) can be accessed via the
four login nodes `login[1-4].barnard.hpc.tu-dresden.de`. (Please choose one concrete login node when
connecting, see example below.)

| Key Type | Fingerprint                                           |
|:---------|:------------------------------------------------------|
| RSA      | `SHA256:lVQOvnci07jkxmFnX58pQf3cD7lz1mf4K4b9jZrAlVU`  |
| RSA      | `MD5:5b:39:ae:03:3a:60:15:21:4b:e8:ba:72:52:b8:a1:ad` |
| ECDSA    | `SHA256:Xan2MYazewT0V5agNazaQfWzLKBD3P48zRwR6reoXhI`  |
| ECDSA    | `MD5:02:fd:ab:c8:39:f9:94:cc:3f:e0:7e:78:5f:76:b8:4c` |
| ED25519  | `SHA256:Gn4n5IX9eEvkpOGrtZzs9T9yAfJUB200bgRchchiKAQ`  |
| ED25519  | `MD5:e8:10:96:67:e8:4c:fd:87:f0:c6:4e:e8:1f:53:a9:be` |
{: summary="List of valid fingerprints for Barnard login[1-4] nodes"}

## Romeo

The cluster [`Romeo`](../jobs_and_resources/hardware_overview.md#romeo) can be accessed via the two
login nodes `login[1-2].romeo.hpc.tu-dresden.de`. (Please choose one concrete login node when
connecting, see example below.)

| Key Type | Fingerprint                                           |
|:---------|:------------------------------------------------------|
|RSA       | `SHA256:BvYEYJtIYDGr3U0up58q5F7aog7JA2RP+w53XKmwO8I`  |
|RSA       | `MD5:5d:dc:40:3b:8b:89:77:5d:0f:29:84:31:0f:73:25:9f` |
|ECDSA     | `SHA256:lgxNRgGcKe7oDGuwf0WV9VPukA30kEqg0sNDLLQwu8Y`  |
|ECDSA     | `MD5:e1:bd:e4:77:06:97:f9:f3:03:18:56:66:14:5d:8d:18` |
|ED25519   | `SHA256:QNjH0ulelqykywMkt3UNTG4W1HzRkHqrhu0f6oq302I`  |
|ED25519   | `MD5:e4:4e:7a:76:aa:87:da:17:92:b1:17:c6:a1:25:29:7e` |
{: summary="List of valid fingerprints for Romeo login[1-2] node"}

## Capella

The cluster [`Capella`](../jobs_and_resources/hardware_overview.md#capella) can be accessed via the two
login nodes `login[1-2].capella.hpc.tu-dresden.de`. (Please choose one concrete login node when
connecting, see example below.)

| Key Type | Fingerprint                                           |
|:---------|:------------------------------------------------------|
|RSA       | ``  |
|RSA       | `` |
|ECDSA     | ``  |
|ECDSA     | `` |
|ED25519   | ``  |
|ED25519   | `` |
{: summary="List of valid fingerprints for Capella login[1-2] node"}

## Alpha Centauri

The cluster [`Alpha Centauri`](../jobs_and_resources/hardware_overview.md#alpha-centauri) can be
accessed via the two login nodes `login[1-2].alpha.hpc.tu-dresden.de`. (Please choose one concrete
login node when connecting, see example below.)

| Key Type | Fingerprint                                           |
|:---------|:------------------------------------------------------|
| RSA      | `SHA256:BvYEYJtIYDGr3U0up58q5F7aog7JA2RP+w53XKmwO8I`  |
| RSA      | `MD5:5d:dc:40:3b:8b:89:77:5d:0f:29:84:31:0f:73:25:9f` |
| ECDSA    | `SHA256:lgxNRgGcKe7oDGuwf0WV9VPukA30kEqg0sNDLLQwu8Y`  |
| ECDSA    | `MD5:e1:bd:e4:77:06:97:f9:f3:03:18:56:66:14:5d:8d:18` |
| ED25519  | `SHA256:QNjH0ulelqykywMkt3UNTG4W1HzRkHqrhu0f6oq302I`  |
| ED25519  | `MD5:e4:4e:7a:76:aa:87:da:17:92:b1:17:c6:a1:25:29:7e` |
{: summary="List of valid fingerprints for Alpha Centauri login[1-2] node"}

## Julia

The cluster [`Julia`](../jobs_and_resources/hardware_overview.md#julia) can be accessed via `julia.hpc.tu-dresden.de`.
(Note, there is no separate login node.)

| Key Type | Fingerprint                                           |
|:---------|:------------------------------------------------------|
| RSA      | `SHA256:MPRhcFxLstI76W8Sg/5KiQlGPVOHUGM/B0+qIZHFj9E`  |
| RSA      | `MD5:5f:22:e2:e2:c3:dc:b1:ee:a9:ba:61:af:34:f6:27:f0` |
| ECDSA    | `SHA256:LkHMlx6VewoPBvOhmNbue8kBsvlljtsEAjCZA8vxRWc`  |
| ECDSA    | `MD5:3f:5a:f2:f9:9d:30:5c:83:c0:e5:0e:87:42:1a:d8:b0` |
| ED25519  | `SHA256:k5zc2E0Y8zcLErDD1Ej3g0OqkoLDF22ADmfX+b+8Z9g`  |
| ED25519  | `MD5:41:03:a2:7c:e9:4d:1c:bd:30:77:7d:91:d7:93:9f:8c` |
{: summary="List of valid fingerprints for Julia login node"}

## Power9

The cluster [`Power9`](../jobs_and_resources/hardware_overview.md#power9) can be accessed via the
two login nodes `login[1-2].power9.hpc.tu-dresden.de`. (Please choose one concrete login node when
connecting, see example below.)

| Key Type | Fingerprint                                           |
|:---------|:------------------------------------------------------|
| RSA      | `SHA256:BvYEYJtIYDGr3U0up58q5F7aog7JA2RP+w53XKmwO8I`  |
| RSA      | `MD5:5d:dc:40:3b:8b:89:77:5d:0f:29:84:31:0f:73:25:9f` |
| ECDSA    | `SHA256:lgxNRgGcKe7oDGuwf0WV9VPukA30kEqg0sNDLLQwu8Y`  |
| ECDSA    | `MD5:e1:bd:e4:77:06:97:f9:f3:03:18:56:66:14:5d:8d:18` |
| ED25519  | `SHA256:QNjH0ulelqykywMkt3UNTG4W1HzRkHqrhu0f6oq302I`  |
| ED25519  | `MD5:e4:4e:7a:76:aa:87:da:17:92:b1:17:c6:a1:25:29:7e` |
{: summary="List of valid fingerprints for Power9 login[1-2] nodes"}

## Dataport Nodes

When you transfer files using the [dataport nodes](../data_transfer/dataport_nodes.md), please make
sure that the fingerprint shown matches one of the table.

| Key Type | Fingerprint                                           |
|:---------|:------------------------------------------------------|
| RSA      | `SHA256:t4Vl4rHRHbglZIm+hck2MWld+0smYAb2rx7EGWWmya0`  |
| RSA      | `MD5:59:7f:c1:7b:34:ec:c8:07:3d:fe:b8:b5:6a:96:ea:0c` |
| ECDSA    | `SHA256:Ga1dXpp1yM5GRJC77PgDCQwDy7oHdrTY7z11V1Eq1L8`  |
| ECDSA    | `MD5:43:1b:b8:f6:23:ab:4a:08:dc:a1:b3:09:8c:8c:be:f9` |
| ED25519  | `SHA256:SRYDLKt7YTQkXmkv+doWV/b55xQz1nT4ZZtXYGhydg4`  |
| ED25519  | `MD5:eb:96:21:d7:61:9f:39:10:82:b9:21:e9:4a:87:c2:9a` |
{: summary="List of valid fingerprints for dataport1.hpc.tu-dresden.de"}

| Key Type | Fingerprint                                           |
|:---------|:------------------------------------------------------|
| RSA      | `SHA256:aMVmc3E0+ndXPiQ8EpY6lFk5CFdfGJfjy/0UBcxor58`  |
| RSA      | `MD5:03:b8:8a:2b:10:e5:6b:c8:0b:78:ab:4e:5b:2c:0e:2c` |
| ECDSA    | `SHA256:6t69R86zhJjGDlIobdsSbKFn8Km3cs9JYWlDW22nEvI`  |
| ECDSA    | `MD5:58:70:7d:df:e5:c8:43:cf:a5:75:ad:f5:da:9f:1a:6d` |
| ED25519  | `SHA256:KAD9xwCRK5C8Ch6Idfnfy88XrzZcqEJ9Ms6O+AzGfDE`  |
| ED25519  | `MD5:2b:61:5a:89:fe:96:95:0d:3d:f6:29:40:55:ea:e6:11` |
{: summary="List of valid fingerprints for dataport2.hpc.tu-dresden.de"}


# Access to ZIH Systems

There are several different ways to access ZIH systems depending on the intended usage:

* [SSH connection](ssh_login.md) is the classical way to connect to the login nodes and work from
    the command line to set up experiments and manage batch jobs
* [Desktop Cloud Visualization](desktop_cloud_visualization.md) provides a virtual Linux desktop
  with access to GPU resources for OpenGL 3D applications
* [WebVNC service](graphical_applications_with_webvnc.md) allows better support for graphical
   applications than SSH with X forwarding
* [JupyterHub service](jupyterhub.md) offers a quick and easy way to work with Jupyter notebooks on
   ZIH systems.

!!! hint

    Prerequisite for accessing ZIH systems is a HPC project and a login. Please refer to the pages
    within [Application for Login and Resources](../application/overview.md) for detailed
    information.

For security reasons, ZIH systems are only accessible for hosts within the domains of TU Dresden.

To access the ZIH systems from outside the campus networks it is recommended to set up a Virtual
Private Network (VPN) connection to enter the campus network. While active, it allows the user
to connect directly to the HPC login nodes.

For more information on our VPN and how to set it up, please visit the corresponding
[ZIH service catalog page](https://tu-dresden.de/zih/dienste/service-katalog/arbeitsumgebung/zugang_datennetz/vpn).

The page [key fingerprints](key_fingerprints.md) holds the up-to-date fingerprints for the login
nodes. Make sure they match.


# Security Restrictions

As a result of a security incident the German HPC sites in Gauß Alliance have adjusted their
measurements to prevent infection and spreading of malware.

The most important items for ZIH systems are:

* All users (who haven't done so recently) have to
  [change their ZIH password](https://selfservice.zih.tu-dresden.de/l/index.php/pswd/change_zih_password).
    * **Login to ZIH systems is denied with an old password.**
* All old (private and public) keys have been moved away.
* All public ssh keys for ZIH systems have to
    * be re-generated using only the ED25519 algorithm (`ssh-keygen -t ed25519`)
    * **passphrase for the private key must not be empty**
* Ideally, there should be no private key on ZIH system except for local use.
* Keys to other systems must be passphrase-protected!
* **ssh to ZIH systems** is only possible from inside TU Dresden campus
  (`login[1,2].zih.tu-dresden.de` will be blacklisted). Users from outside can use
  [VPN](https://tu-dresden.de/zih/dienste/service-katalog/arbeitsumgebung/zugang_datennetz/vpn).
* **ssh from ZIH system** is only possible inside TU Dresden campus.
  (Direct SSH access to other computing centers was the spreading vector of the recent incident.)

Data transfer is possible via the [Dataport Nodes](../data_transfer/dataport_nodes.md).

We understand that all this will change convenient workflows. If the measurements would render your
work on ZIH systems completely impossible, please [contact the HPC support](../support/support.md).


# Connecting via Terminal (Linux, Mac, Windows)

Connecting via terminal works on every operating system. For Linux and Mac operating systems
no additional software is required. For users of a Windows OS a recent version of Windows is
required (Windows 10, Build 1809 and higher). It is possible to use
[Command Prompt](https://en.wikipedia.org/wiki/Cmd.exe) or [PowerShell](https://en.wikipedia.org/wiki/PowerShell)).
Ensure that [OpenSSH](https://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/factoryos/connect-using-ssh?view=windows-10)
is installed on the system.

SSH establishes secure connections using authentication and encryption. The login nodes accept
the following encryption algorithms: `aes128-ctr`, `aes192-ctr`, `aes256-ctr`,
`aes128-gcm@openssh.com`, `aes256-gcm@openssh.com`, `chacha20-poly1305@openssh.com`,
`chacha20-poly1305@openssh.com`.

## Before Your First Connection

We suggest to create an SSH key pair before you work with the ZIH systems. This ensures high
connection security.

```console
marie@local$ mkdir -p ~/.ssh
marie@local$ ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519
Generating public/private ed25519 key pair.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
[...]
```

Type in a passphrase for the protection of your key. The passphrase should be **non-empty**.
Copy the **public key** to the ZIH system (Replace placeholder `marie` with your ZIH login):

```console
marie@local$ ssh-copy-id -i ~/.ssh/id_ed25519.pub marie@login2.barnard.hpc.tu-dresden.de
The authenticity of host 'barnard.hpc.tu-dresden.de (141.30.73.104)' can't be established.
RSA key fingerprint is SHA256:HjpVeymTpk0rqoc8Yvyc8d9KXQ/p2K0R8TJ27aFnIL8.
Are you sure you want to continue connecting (yes/no)?
```

Compare the shown fingerprint with the [documented fingerprints](key_fingerprints.md). Make sure
they match. Then you can accept by typing `yes`.

!!! note "One `ssh-copy-id` command for all clusters"

    Since your home directory, where the file `.ssh/authorized_keys` is stored, is available on all HPC
    systems, this task is only required once and you can freely choose a target system for the
    `ssh-copy-id` command. Afterwards, you can access all clusters with this key file.

??? info "ssh-copy-id is not available"

    If `ssh-copy-id` is not available, you need to do additional steps:

    ```console
    marie@local$ scp ~/.ssh/id_ed25519.pub marie@login2.barnard.hpc.tu-dresden.de:
    The authenticity of host 'barnard.hpc.tu-dresden.de (141.30.73.104)' can't be established.
    RSA key fingerprint is SHA256:Gn4n5IX9eEvkpOGrtZzs9T9yAfJUB200bgRchchiKAQ.
    Are you sure you want to continue connecting (yes/no)?
    ```

    After that, you need to manually copy the key to the right place:

    ```console
    marie@local$ ssh marie@login2.barnard.hpc.tu-dresden.de
    [...]
    marie@login.barnard$ mkdir -p ~/.ssh
    marie@login.barnard$ touch ~/.ssh/authorized_keys
    marie@login.barnard$ cat id_ed25519.pub >> ~/.ssh/authorized_keys
    ```

### Configuring Default Parameters for SSH

After you have copied your key to the ZIH system, you should be able to connect using:

```console
marie@local$ ssh marie@login2.barnard.hpc.tu-dresden.de
[...]
marie@login.barnard$ exit
```

However, you can make this more comfortable if you prepare an SSH configuration on your local
workstation. Navigate to the subdirectory `.ssh` in your home directory and open the file `config`
(`~/.ssh/config`) in your favorite editor. If it does not exist, create it. Put the following lines
in it (you can omit lines starting with `#`):

```bash
Host barnard
  #For login (shell access)
  HostName login1.barnard.hpc.tu-dresden.de
  #Put your ZIH-Login after keyword "User":
  User marie
  #Path to private key:
  IdentityFile ~/.ssh/id_ed25519
  #Don't try other keys if you have more:
  IdentitiesOnly yes
  #Enable X11 forwarding for graphical applications and compression. You don't need parameter -X and -C when invoking ssh then.
  ForwardX11 yes
  Compression yes
Host dataport
  #For copying data without shell access
  HostName dataport1.hpc.tu-dresden.de
  #Put your ZIH-Login after keyword "User":
  User marie
  #Path to private key:
  IdentityFile ~/.ssh/id_ed25519
  #Don't try other keys if you have more:
  IdentitiesOnly yes
```

Afterwards, you can connect to the ZIH system using:

```console
marie@local$ ssh barnard
```

If you want to copy data from/to ZIH systems, please refer to the documentation
[Dataport Nodes: Transfer Data to/from ZIH's Filesystems](../data_transfer/dataport_nodes.md)
for more information on Dataport nodes.

!!! note "Gernalization to all HPC systems"

    In the above `.ssh/config` file, the HPC system `Barnard` is chosen as an example.
    The very same settings can be made for individuall or all ZIH systems, e.g. `Capella`, `Alpha`,
     `Julia`, `Romeo` etc.

## X11-Forwarding

If you plan to use an application with graphical user interface (GUI), you need to enable
X11-forwarding for the connection. If you use the SSH configuration described above, everything is
already prepared and you can simply use:

```console
marie@local$ ssh barnard
```

If you have omitted the last two lines in the default configuration above, you need to add the
option `-X` or `-XC` to your SSH command. The `-C` enables compression which usually improves
usability in this case:

```console
marie@local$ ssh -XC barnard
```

!!! info

    Also consider to use a [DCV session](desktop_cloud_visualization.md) for remote desktop
    visualization at ZIH systems.


# Connecting with MobaXterm (Windows)

[MobaXterm](https://mobaxterm.mobatek.net) is an enhanced terminal for Windows with an X11 server,
a tabbed SSH client, network tools and more.

## Download and install

To download go to [MobaXterm download page](https://mobaxterm.mobatek.net/download-home-edition.html)
and download a free home edition.

![Downloading MobaXterm](misc/mobaxterm1_download.png)

Pick the installer suiting best your current system and run it afterwards. Follow the instructions.
You should see the following interface after starting the MobaXterm application.

![First opening MobaXterm](misc/mobaxterm2_first.png)

## Configure local settings

Select the menu entry "Settings" &#8594; "Configuration" or click the button "Settings" in the
toolbar. A new window will open.

![Settings in MobaXterm](misc/mobaxterm3_config.png)

Here you can set different options in the following tabs:

- "General" - local path options for local MobaXterm-session,
- "Terminal" -  options, which alter your terminal, e.g. color scheme,
- "X11" - options for X11-forwarding. It is enabled by default,
- "SSH" - general SSH settings, e.g. keep-alive, SSH agent, browser-options,
- "Display" - general display-options for the application,
- "Toolbar" - customization of the toolbar,
- "Misc" - options to alter specific actions inside the MobaXterm-application.

## Start a new session

1.  Select the tab "Sessions"  &#8594; "New session" or click the button "Session" in the toolbar.

    ![Opening a new session in MobaXterm](misc/mobaxterm4_session.png)

1.  Select a SSH section. Insert "Remote host" (`login2.barnard.hpc.tu-dresden.de`), "Username"
    (replace `marie` with your ZIH login), and "Port" 22. Using the button right from the username
    option, you can store and manage credentials. To access a different cluster, change the name
    accordingly (e.g.`login1.alpha.hpc.tu-dresden.de`)

    ![Settings for SSH connection in MobaXterm](misc/MobaXterm_remote_host.png)

1.  Advanced settings can be configured in the same window below. These are
    - "Advanced SSH settings" - set defaults for this specific session. For example, set a SSH key
    or change the remote environment,
    - "Terminal settings" - change terminal options,
    - "Network settings" - configure how the connection is built over the network. For example, by
    adding a proxy as gateway to the targeted system,
    - "Bookmark settings" - specify how the session will be saved to your session list, which is
    afterwards accessible by the button "Sessions".

1.  Start the session by clicking the button "OK".

    Your previous sessions are saved in the bookmarks and can be accessed via the menu entry
    "Sessions"  &#8594;  "User sessions". Alternatively, double click on one of the previous
    sessions on the left panel.

    ![Opening a saved session in MobaXterm](misc/mobaxterm6_oldse.png)

1.  The last thing to do is to input your ZIH password in the command line and to press enter.
    The entered symbols of your password are invisible and will not appear as typed in.

    ![Saving your password in MobaXterm](misc/MobaXterm_password.png)

!!! Caution

    Do not forget to close the session after your jobs are finished. Just type `exit` in the
    command line and complete with pressing enter.


# Connecting with PuTTY (Windows)

PuTTY is a free and open-source terminal emulator, serial console and network file transfer
application, supports several network protocols, including SCP, SSH. Visit the
[homepage](https://www.putty.org) for more information.

## Download and install

Download the installer suiting best your current system and run it afterwards. Follow the
instructions for installation.

![Downloading PuTTY](misc/putty1_download.png)

## Start a new SSH session

1.  Start PuTTY and insert the "Host Name" (`login2.barnard.hpc.tu-dresden.de`) and leave the default
    port (22). To access a different cluster, change the name accordingly (e.g.`login1.alpha.hpc.tu-dresden.de`)

    ![Settings for SSH connection in PuTTY](misc/putty2_quickstart.png)

1.  Click "Open" to start a new session. A terminal window will open up.

    ![Login in PuTTY](misc/putty3_login.png)

1.  After entering your ZIH login and password you will be logged in to one of the login nodes.

## Connection Configuration (optional)

You can pre-configure some connection details additionally. It will save time in the future.

-   Set your user name. For that choose the tab "Connection" &#8594; "Data" in the navigation tree
    on the left. Insert your ZIH username in the text field "Auto-login username".

    ![Auto-login username in PuTTY](misc/putty4_username.png)

-   Configure SSH-key (recommended for security reason).

    ??? note "Generate your key pair"

        If you do not have your SSH key pair (public and private keys) yet, you can generate
        it using PuTTYgen program, which was installed together with the main PuTTY client.

        ![PuTTY generate key pair](misc/putty8_gen_key.png){: width=400}

        Click on the button "Generate" to create a new key pair. Move the mouse pointer in the
        respective field as requested. Afterwards save your public and private keys in separate
        files. It is recommended to use a passphrase for the private key.

    To configure the SSH key to use, navigate to "Connection" &#8594; "SSH" &#8594; "Auth" in the
    tree left. Insert the path to your local key-file in a text field "Private key file for
    authentication" or select it with "Browse...".

    ![SSH-key in PuTTY](misc/putty5_key.png)

    !!! note "Add public key to ZIH system"

        For being able to use a SSH key to login to ZIH system, you have to register the key
        on the system before!

        Login to the ZIH system using your password and add your public-key to
        `~/.ssh/authorized_keys`.

-   Enable X-forwarding. Navigate to "Connection" &#8594; "SSH" &#8594; "X11" in the tree on the
    left. Select the checkbox "Enable X11 forwarding".

    ![X-forwarding in PuTTY](misc/putty6_x11.png)

After editing the connection details save your configuration. Go back to the "Session" in the tree
left. Insert a session bookmark name into the text field "Saved Sessions" and click the button
"Save". Afterwards you will see the name in the list below.

![Saving settings in PuTTY](misc/putty7_save.png)

Now, you can start a configured session by double-clicking its name in the list.

You can change your saved configuration by selecting its name in the list and clicking the button
"Load". Make your changes and save it again under the same name. This will overwrite the old
configuration permanently.

You can delete saved configuration by clicking the button "Delete". This will remove the
configured session permanently.


# Acknowledgement

To provide you with modern and powerful HPC systems in future as well, we have to show that these
systems help to advance research. For that purpose we rely on your help. In most cases, the results
of your computations are used for presentations and publications, especially in peer-reviewed
magazines, journals, and conference proceedings. We kindly ask you to mention the HPC resource usage
in the acknowledgment section of all publications that are based on granted HPC resources of the
[NHR center at TU Dresden](https://tu-dresden.de/zih/hochleistungsrechnen/nhr-center). Examples:

!!! note "Standard case"

    The authors gratefully acknowledge the computing time made available to them on
    the high-performance computer <XY> at the NHR Center of TU Dresden. This center is jointly
    supported by the Federal Ministry of Education and Research and the state governments
    participating in the NHR (www.nhr-verein.de/unsere-partner).

!!! note "Two NHR centers"

    The authors gratefully acknowledge the computing time made available to them on
    the high-performance computers <XY> and <YZ> at the NHR Centers at TU Dresden and <YZ>.
    These centers are jointly supported by the Federal Ministry of Education and Research
    and the state governments participating in the NHR (www.nhr-verein.de/unsere-partner).

!!! note "German"

    Die Autoren bedanken sich für die ihnen zur Verfügung gestellte Rechenzeit auf
    dem Hochleistungsrechner <XY> am NHR-Zentrum der TU Dresden. Dieses wird gemeinsam durch
    das Bundesministerium für Bildung und Forschung und den am NHR beteiligten
    Landesregierungen (www.nhr-verein.de/unsere-partner) unterstützt.


# Application for HPC Resources

You will find comprehensive information regarding the application process on the webpage
[Project Application for using the HPC Systems](https://tu-dresden.de/zih/hochleistungsrechnen/zugang/projektantrag).

You will find in this section information about:

- [Terms of Use](terms_of_use.md)
- Manage members and their access to the project via
  [User Management for Project Leaders](project_management.md)
- [Acknowledgment in Publications](acknowledgement.md)

## NHR Center

Since 2021, HPC at universities has been restructured by the
[NHR network](https://www.nhr-verein.de/).
The network consists of nine centers which operate the systems and offer
a coordinated consulting service on the methodological competence of scientific HPC.
The aim is to provide scientists at German universities with computing capacity
for their research and to strengthen their skills in the efficient use of these resources.

In order to use the HPC systems installed at ZIH, it is necessary to apply for the resources.
The applicant (HPC project manager) is required to have a doctorate/PhD.
It is possible to apply for different
[project types](https://tu-dresden.de/zih/hochleistungsrechnen/zugang/projektantrag#section-1)
depending on the project demands.

However, the
[application workflow with JARDS](https://tu-dresden.de/zih/hochleistungsrechnen/zugang/projektantrag)
is identical for all types.

??? Info  "Who can apply?"

    Please note: Researchers with a doctorate from universities or universities of applied sciences
    in Germany are eligible to apply.

    We particularly invite researchers to apply with research projects related to our focus topics:

       * Life Sciences
       * Earth System Sciences
       * Methods for big data, data analysis and management
       * Machine Learning
       * Tiered storage architectures and I/O optimization
       * Performance and energy efficiency analysis and optimization
       * Research projects not related to our focus topics but from Universities within Saxony.

    We recommend **all other researchers** to review the focus topics of the remaining
    [NHR centers](https://www.nhr-verein.de/anwendungsunterstuetzung) and apply to the most suitable
    for your research topic. Working at the specialized NHR center related to your research topic
    provides you with the advantage of receiving topic specific support.


# User Management for Project Leaders

The HPC project leader (PI/PC) has overall responsibility for the project and for all activities
within the corresponding project on ZIH systems. In particular the project leader shall:

* add and remove users from the project,
* update contact details of the project members,
* monitor the resources of the project,
* inspect and store data of retiring users.

The project leader can appoint a *project administrator* with an HPC account to manage these
technical details.

The [project management site](#access) enables the project leader and the project administrator
to

* get a [project overview](#projects)
* [add and remove users from the project](#manage-project-members-dis-enable)
* [define a technical administrator](#manage-project-members-dis-enable)
* [view statistics (resource consumption)](#statistic)
* file a new HPC proposal,
* file results of the HPC project.

## Access

[Entry point to the project management system](https://hpcprojekte.zih.tu-dresden.de/managers)
The project leaders of an ongoing project and their accredited admins
are allowed to login to the system. In general each of these persons
should possess a ZIH login at the Technical University of Dresden, with
which it is possible to log in to the website. In some cases, it may
happen that a project leader of a foreign organization does not have a ZIH
login. For this purpose, it is possible to set a local password:
"[Missing Password](https://hpcprojekte.zih.tu-dresden.de/managers/members/missingPassword)".

![Login Screen](misc/external_login.png "Login Screen")
{: align="center"}

On the 'Missing Password' page, it is possible to reset the passwords of a 'non-ZIH-login'. For this
you write your login, which usually corresponds to your e-mail address, in the 'Login' field and
click on 'reset'. Within 10 minutes the system sends a signed e-mail from
<hpcprojekte@zih.tu-dresden.de> to the registered e-mail address. this e-mail contains a link to
reset the password.

![Password Reset](misc/password.png "Password Reset")
{: align="center"}

## Projects

After login you reach an overview that displays all available projects. In each of these listed
projects, you are either project leader or an assigned project administrator. From this list, you
have the option to view the details of a project or make a follow-up application (extension). The
latter is only possible if a project has been approved and is active or was. In the upper right
area you will find a red button to log out from the system.

![Project Overview](misc/overview.png "Project Overview")
{: align="center"}

The project details provide information about the requested and allocated resources. The other tabs
show the employee and the statistics about the project.

![Project Details](misc/project_details.png "Project Details")
{: align="center"}

### Manage Project Members (dis-/enable)

The project members can be managed under the tab 'employee' in the project details. This page gives
an overview of all ZIH logins that are a member of a project and its status. If a project member
marked in green, it can work on all authorized HPC machines when the project has been approved. If
an employee is marked in red, this can have several causes:

* the employee was manually disabled by project managers, project administrator
  or ZIH staff
* the employee was disabled by the system because its ZIH login expired
* confirmation of the current HPC-terms is missing

You can specify a user as an administrator. This user can then access the project management system.
Next, you can disable individual project members. This disabling is only a "request of disabling"
and has a time delay of 5 minutes. A user can add or reactivate itself within a specific project by
clicking the link on the end of the page. To prevent misuse this link is valid for 2 weeks and
will then be renewed automatically.

![Project Members](misc/members.png "Project Members")
{: align="center"}

The link leads to a page where you can sign in to a project by accepting the terms of use. You also
need a valid ZIH-Login. After this step it can take 1-1.5 h to transfer the login to all cluster
nodes.

![Add Member](misc/add_member.png "Add Member")
{: align="center"}

### Statistic

The statistic is located under the tab 'Statistic' in the project details. The data will be updated
once a day and shows used CPU-time and used disk space of a project. Follow-up projects also show
the data of their predecessor(s).

![Project Statistic](misc/stats.png "Project Statistic")
{: align="center"}


# Terms of Use

!!! attention

    Only the German version of the *Terms of Use* is binding.

These new Terms of Use are valid from April 1, 2018: [HPC-Nutzungsbedingungen_20180305.pdf][1]

The key points are are:

- For support reasons, we store your contact data according to our [identity management system][5].
  (Will be anonymized at least 15 months after the cancellation of the HPC login.)
  The data of the HPC project (incl. contact of project leader) will be kept for
  further reference.
    - Our HPC resources may only be used according to the project description.
- Responsibilities for the project leader:
    - She has to assign a team member with an HPC login as the technical project
      administrator. She can do this herself if she has a login at our systems.
    - The project leader or the administrator will have to add/remove members to
      their project. She has access to accounting data for her project.
- These issues cover the data storage in our systems:
    - Please work in the scratch filesystems.
    - Upon request, the project leader or the administrator can be given access
      to a user's directory.
    - The scratch filesystems (`/tmp`, `/scratch`, `/lustre/ssd`) are for
      temporary use only. After a certain time, files may be removed
      automatically (for `/tmp` after 7 days, parallel `/scratch`: after 100 days).
    - Before a user leaves a project the leader/administrator has to store away
      worthy data. For this, the storage services of ZIH (long term storage,
      intermediate archive) can be used.
- Project termination (**new**)
    - At project's end, jobs cannot be submitted and started any longer.
    - Logins are valid for 30 more days for saving data.
    - Hundred days after project termination, it's files will be deleted in
      the HPC filesystems.
- The HPC user agrees to follow the instructions and hints of the support
  team. In case of non-compliance, she can be disabled for the batch system
  or banned from the system.

- Working with logs and HPC performance data (**new**)
    - For HPC related research ZIH will collect and analyze
      performance data. Anonymized, it might be shared with research partners.
    - Log data will be kept for long term analyzes.

These key points are only a brief summary. If in doubt, please consult the German original.

## History

| Valid                           | Document                                     |
|:--------------------------------|:---------------------------------------------|
| 1 April 2018 -                  | [HPC-Nutzungsbedingungen_20180305.pdf][1]    |
| 1 October 2016 - 31 March 2018  | [HPC-Nutzungsbedingungen_20160901.pdf][2]    |
| 5 June 2014 - 30 September 2016 | [HPC-Nutzungsbedingungen_20140606.pdf][3]    |

- Not binding translation in english: [Terms-of-use-HPC-20180305-engl.pdf][4]

[1]: misc/HPC-Nutzungsbedingungen_20180305.pdf
[2]: misc/HPC-Nutzungsbedingungen_20160901.pdf
[3]: misc/HPC-Nutzungsbedingungen_20140606.pdf
[4]: misc/Terms-of-use-HPC-20180305-engl.pdf
[5]: https://tu-dresden.de/zih/dienste/service-katalog/zugangsvoraussetzung


# Content Rules

!!! cite "George Bernard Shaw"

    The golden rule is that there are no golden rules.

## Motivation and Rationale

This page holds rules regarding the layout, content, and writing of this
documentation. The goals are to provide a comprehensive, consistent and well-written
documentation that is pure joy to read and use. It shall help to find answers and provide knowledge
instead of being the bottleneck and a great annoyance. Therefore, we set up some rules which
are outlined in the following.

!!! tip

    Following these rules when contributing speeds up the review process. Furthermore, your
    changes will not be blocked by the automatic checks implemented in the CI pipeline.

## Responsibility and License

This documentation and the repository have two licenses (cf. [Legal Notice](../legal_notice.md)):

* All documentation is licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).
* All software components are licensed under [MIT license](../license_mit.txt).

These licenses also apply to your contributions.

!!! note

    If you contribute, you are fully and solely responsible for the content you create and have to
    ensure that you have the right to create it under the laws which apply.

If you are in doubt, please contact us either via
[GitLab Issue](https://gitlab.hrz.tu-chemnitz.de/zih/hpcsupport/hpc-compendium/-/issues)
or via [e-mail](mailto:hpc-support@tu-dresden.de).

## Quick Overview

* All documentation is written in [Markdown](#markdown).
* Use spaces (not tabs) both in Markdown files and in `mkdocs.yml`.
* Respect the line length limit of 100 characters (exception: links).
* Do not add large binary files or high-resolution images to the repository (cf.
   [adding images and attachments](#graphics-and-videos)).
* [Admonitions](#special-feature-admonitions) may be actively used for longer code examples,
   warnings, tips, important information, etc.
* Respect the [writing style](#writing-style) and the rules for
  [spelling and technical wording](#spelling-and-technical-wording).
* For code blocks:
    * Use [syntax highlighting and appropriate prompts](#code-blocks-and-command-prompts).
    * Respect [data privacy](#data-privacy-and-generic-names).
    * Stick to the [rules on optional and required arguments](#code-styling-rules).
* Save attachments, graphics and videos within the respective `misc` subdirectory.

## Detailed Overview

### Writing Style

* Assume that a future reader is eager to start typing commands. Thus, encourage the reader by
  addressing him/her directly:
    * Example: Use "You can/should ..." instead of "Users can/should ..."
    * Example: Use "Your contribution is highly welcome" instead of "Contributions from user-side
      are highly welcome"
* Be brief! Provide the main idea/commands first, and special cases later. If it is not necessary to
  know how a special piece of software works, don't explain it.
* Provide the often-used commands first.
* Use active over passive voice
    * Write with confidence. This confidence should be reflected in the documentation so that
      the readers trust and follow it.
    * Example: "We recommend something" instead of "Something is recommended."
* Capitalize headings, e.g. *Exclusive Reservation of Hardware*
* Give keywords in link texts, e.g. [Code Blocks](#code-blocks-and-syntax-highlighting) is more
  descriptive than [this subsection](#code-blocks-and-syntax-highlighting)
* Avoid using tabs both in Markdown files and in `mkdocs.yaml`. Type spaces instead.

### Pages Structure and New Page

The documentation and pages structure is defined in the configuration file
[`mkdocs.yml`](https://gitlab.hrz.tu-chemnitz.de/zih/hpcsupport/hpc-compendium/-/blob/main/doc.zih.tu-dresden.de/mkdocs.yml):

```Markdown
nav:
  - Home: index.md
  - Application for Login and Resources:
    - Overview: application/overview.md
    - Terms of Use: application/terms_of_use.md
    - Request for Resources: application/request_for_resources.md
    - Project Request Form: application/project_request_form.md
    - Project Management: application/project_management.md
    - Acknowledgement: application/acknowledgement.md
  - Access to ZIH Systems:
    - Overview: access/overview.md
  [...]
```

Follow these two steps to **add a new page** to the documentation:

1. Create a new Markdown file under `docs/subdir/file_name.md` and put the documentation inside.
The sub-directory structure represents different topics of the documentation. Try to fit the
contribution into the existing structure. The file name should reflect the title of the
documentation page, i. e. `shortened_page_heading.md`.
1. Add `subdir/file_name.md` to the configuration file `mkdocs.yml` by updating the navigation
   section. Yes, the order of files is crucial and defines the structure of the content. Thus,
   carefully consider the right spot and section for the new page.

Make sure that the new page **is not floating**, i.e., it can be reached directly from
the documentation structure.

#### Preserve URLs

For several reasons it is important to preserve URLs within this documentation, e.g., pages with
description of specific hardware might be used as references in papers. Therefore, existing pages
shall not be renamed or moved on directory level. Outdated pages are marked with "Outdated" tag
and moved to the archive by changing the page's navigation entry in the `mkdocs.yaml` file.

### Markdown

All documentation is written in Markdown. Please keep things simple, i.e., avoid using fancy
Markdown dialects.

#### Brief How-To on Markdown

| Purpose | Markdown | Rendered HTML |
|---|---|---|
| Bold text | `**Bold Text**`  | **Bold Text**  |
| Italic text | `*Italic Text*`   | *Italic Text*  |
| Headings | `# Level 1`, `## Level 2`, `### Level 3`, ...   |  |
| External link | `[website of TU Dresden](https://tu-dresden.de)` | [website of TU Dresden](https://tu-dresden.de) |
| Internal link | `[Slurm page](../jobs_and_resources/slurm.md)` | [Slurm page](../jobs_and_resources/slurm.md)|
| Internal link to section | `[section on batch jobs](../jobs_and_resources/slurm.md#batch-jobs)` | [section on batch jobs](../jobs_and_resources/slurm.md#batch-jobs) |

Further tips can be found on this
[cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet).

#### Attachments

Of course, you can provide attachments in sections and pages.
Such attachment documents may contain information that are more detailed and go far beyond the
scope of the compendium, e.g. user manuals for application-specific software.

Save attachments within the `misc` subdirectory of the corresponding section.

!!! note "Syntax for attachments"

    The syntax for attachments is the very same as for links. As the attachment is within the `misc`
    subdirectory, you can refer to it as local file.

    ```markdown
    [<description>](misc/<attachment_file_name>)
    ```

    Since the `<description>` is rendered as link text, you should choose a clear and precise text:

    ```markdown
    [slides of HPC introduction](misc/HPC-Introduction.pdf)
    ```

#### Graphics and Videos

Please use graphics and videos for illustration purposes and to improve comprehensibility.
All graphics and attachments are saved within `misc` directory of the respective subdirectory in
`docs`.
For video attachments please use either webm or mp4 format. We make use of the
[mkdocs-video extension](https://github.com/soulless-viewer/mkdocs-video).

!!! note "Syntax for graphics and videos"

    The syntax to insert a **graphic** into a page is

    ```markdown
    ![Alternative text](misc/graphics_file.png)
    ```

    The syntax to insert a **video** attachment into a page is

    ```html
    ![type:video](misc/terminate-virtual-desktop-dcv.mp4)
    ```

It is possible to add captions for tables and figures using `{: summary="This is a table caption"}`.
The `summary` and `align` parameters can be combined as well:
`{: summary="This is a table caption" align="top"}`.

##### Resizing and Alignment of Graphics

In general, graphics and images should be added to the repository with the desired size.

!!! warning

    Do not add large binary files or high-resolution images to the repository. See this valuable
    document for [image optimization](https://web.dev/fast/#optimize-your-images).

We recommend the well-know Linux package [ImageMagick](https://imagemagick.org/) for resizing
graphics.

!!! example "Resize image using ImageMagick"

    The command

    ```console
    marie@local$ magick cluster.jpeg -resize 600 cluster_600.jpeg
    ```

    will resize the graphic `cluster.jpeg` to a width of 600 pixels keeping the aspect ratio.
    Depending on the resolution of the original file, the resulting file can be way smaller in terms
    of memory foot print.

Nevertheless you can explicitly specify the size a graphic. The syntax is as follows

```markdown
![Alternative text](misc/graphics_file.png){: style="width:150px"}
```

By default, graphics are left-aligned. In most cases, this is not elegant and you probably wish to
center-align your graphics. **Alignment** of graphics can be controlled via the `{: align=<value>}`
attribute. Possible values are `left`, `right` and `center`. **Note:** It is crucial to
have `{: align=center}` on a new line and the value without quotation marks.

Resize and alignment specification can be combined as depicted in the following example.

!!! example "Resize image to 150px width and specify alignment"

    The three tabs show the Markdown syntax to resize the image of the beautiful
    [cluster `Barnard`](../jobs_and_resources/hardware_overview.md#barnard) to a height of 150
    pixels keeping the aspect ratio and left, center and right-align it, respectively.

    === "Scale and default-align"

        ```markdown
        ![Beauty Barnard](misc/barnard.jpeg){: style="height:150px"}
        ```

        ![Beauty Barnard](misc/barnard.jpeg){: style="height:150px"}

    === "Scale and center-align"

        ```markdown
        ![Beauty Barnard](misc/barnard.jpeg){: style="height:150px"}
        {: align="center"}
        ```

        ![Beauty Barnard](misc/barnard.jpeg){: style="height:150px"}
        {: align="center"}

    === "Scale and right-align"

        ```markdown
        ![Beauty Barnard](misc/barnard.jpeg){: style="height:150px"}
        {: align="right"}
        ```

        ![Alternative text](misc/barnard.jpeg){: style="height:150px"}
        {: align="right"}

#### Special Feature: Admonitions

[Admonitions](https://squidfunk.github.io/mkdocs-material/reference/admonitions/), also known as
call-outs, may be actively used to highlight examples, warnings, tips, important information, etc.
Admonitions are an excellent choice for including side content without significantly interrupting
the document flow.

Several different admonitions are available within the used
[material theme](https://squidfunk.github.io/mkdocs-material/), e.g., `note`, `info`, `example`,
`tip`, `warning`, and `cite`. Please refer to the
[documentation page](https://squidfunk.github.io/mkdocs-material/reference/admonitions/#supported-types)
for a comprehensive overview.

!!! example "Syntax"

    All admonitions blocks start with `!!! <type>` and the following content block is indented by
    (exactly) four spaces.
    If no title is provided, the title corresponds to the admonition type.

    ```markdown
    !!! note "Descriptive title"

        Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod
        tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At
        vero eos et accusam et justo duo dolores et ea rebum.
    ```

!!! note Folding

    Admonitions can be made foldable by using `???` instead of `!!!`. A small toggle on the right
    side is displayed. The block is open by default if `???+` is used. Long code examples should be
    folded by default.

### Spelling and Technical Wording

To provide consistent and high-quality documentation, and help users to find the right pages,
there is a list of conventions w.r.t. spelling and technical wording.

* Language settings: en_us

| Do | Don't |
|----|-------|
| I/O | IO |
| Slurm | SLURM |
| filesystem(s) | file system(s) |
| ZIH system(s) | Taurus, HRSK II, our HPC systems, etc. |
| workspace | work space |
|       | HPC-DA |
| cluster `romeo` | ROMEO cluster, romeo cluster, `romeo` cluster, "romeo" cluster, etc. |

### Code Blocks and Command Prompts

* Use ticks to mark code blocks and commands, not an italic font.
* Specify language for code blocks ([see below](#code-blocks-and-syntax-highlighting)).
* All code blocks and commands should be runnable from a login node or a node within a specific
  cluster (e.g., `alpha`).
* It should be clear from the [prompt](#list-of-prompts), where the command is run (e.g., local
  machine, login node, or specific cluster).

#### Code Blocks and Syntax Highlighting

Providing code blocks and snippets is the meat and bones of this documentation.
Code blocks and command examples should give the general idea of invocation and be as precise as
possible, i.e., allowing for copy-and-paste. Please mark replaceable code parts and optional and
required arguments as outlined in the section [required and optional arguments](#code-styling-rules)
below. Long, non-meaningful output should be omitted.

We make use of the extension
[pymdownx.highlight](https://squidfunk.github.io/mkdocs-material/reference/code-blocks/) for syntax
highlighting. There is a complete list of supported
[language short codes](https://pygments.org/docs/lexers/).

??? note "Syntax for command line"

    For normal commands executed in the terminal, use the language short code `console`.

    ````markdown
    ```console
    marie@login$ module list
    [...]
    ```
    ````

??? note "Syntax for job files and scripts"

    Use the language short code `bash` for job files and shell scripts.

    ````markdown
    ```bash
    #!/bin/bash
    #SBATCH --nodes=1
    #SBATCH --time=01:00:00
    #SBATCH --output=slurm-%j.out

    module load foss

    srun a.out
    ```
    ````

??? note "Syntax for Python"

    `python` for Python source code:

    ````markdown
    ```python
    from time import gmtime, strftime
    print(strftime("%Y-%m-%d %H:%M:%S", gmtime()))
    ```
    ````

    And `pycon` for Python console:

    ````markdown
    ```pycon
    >>> from time import gmtime, strftime
    >>> print(strftime("%Y-%m-%d %H:%M:%S", gmtime()))
    2021-08-03 07:20:33
    ```
    ````

??? note "Line numbers"

    More sugar can be applied by adding line numbers.

    ````markdown
    ```bash linenums="1"
    #!/bin/bash

    #SBATCH --nodes=1
    #SBATCH --ntasks=23
    #SBATCH --time=02:10:00

    srun a.out
    ```
    ````

    _Result_:

    ![lines](misc/lines.png)

    Specific Lines can be highlighted by using

    ````markdown
    ```bash hl_lines="2 3"
    #!/bin/bash

    #SBATCH --nodes=1
    #SBATCH --ntasks=23
    #SBATCH --time=02:10:00

    srun a.out
    ```
    ````

    _Result_:

    ![lines](misc/highlight_lines.png)

#### Data Privacy and Generic Names

Where possible, replace login, project name, and other private data with clearly recognizable
placeholders. In particular, use the generic placeholders depicted in the following table.
The table also holds a second placeholder, if, e.g., you need a second login to formulate an example.

| Description | Placeholder | 2nd Placeholder |
|---|---|---|
| Username | Marie | Martin |
| Login | `marie` | `martin` |
| E-mail | marie@tu-dresden.de | martin@tu-dresden.de |
| Project title | `p_number_crunch` | `p_long_computations` |
| Workspace title | `number_crunch` | `long_computations` |
| Job ID | `123456` | `456789` |
{: summary="Generic placeholders", align="bottom"}

!!! example "Output of `ls` command"

    The following code listing depicts the usage of the generic user names and projects as well as
    recognizable placeholders for files and directory names.

    ```console
    marie@login$ ls -l
    drwxr-xr-x   3 marie p_number_crunch      4096 Jan 24  2020 code
    drwxr-xr-x   3 marie p_number_crunch      4096 Feb 12  2020 data
    -rw-rw----   1 marie p_number_crunch      4096 Jan 24  2020 readme.md
    ```

!!! info "Marie"

    We choose *marie* as generic login and placeholder. There is no magic story on this decision.
    Feel free to associate this generic login for example with
    physicist and chemist [Marie Curie](https://en.wikipedia.org/wiki/Marie_Curie),
    and [Marianne](https://en.wikipedia.org/wiki/Marianne), symbol of France standing for liberty,
    equality and fraternity.

    The very same holds for the generic login *martin*.

#### Placeholders

Placeholders represent arguments or code parts that can be adapted to the user's needs. Use them to
give a general idea of how a command or code snippet can be used, e. g. to explain the meaning of
some command argument:

```bash
marie@login$ sacct -j <job id>
```

Here, a placeholder explains the intention better than just a specific value:

```console
marie@login$ sacct -j 4041337
```

Please be aware, that a reader often understands placeholders easier if you also give an example.
Therefore, always add an example!

#### Mark Omissions

If showing only a snippet of a long output, omissions are marked with `[...]`.

#### Code Styling Rules

* Stick to the Unix rules on optional and required arguments, and selection of item sets:

    * `<required argument or value>`
    * `[optional argument or value]`
    * `{choice1|choice2|choice3}`

* Please use following style guidelines while writing code blocks:

    * Shell: [Shell style guide](https://google.github.io/styleguide/shellguide.html)
    * Python: [PEP-0008 style guide](https://peps.python.org/pep-0008/)
    * MATLAB: [MATLAB programming style guide](https://www.researchgate.net/publication/316479241_Best_practices_for_scientific_computing_and_MATLAB_programming_style_guidelines)
    * R: [R style guide](https://google.github.io/styleguide/Rguide.html)
    * C++: [C++ style guide](https://google.github.io/styleguide/cppguide.html)
    * Java: [Java style guide](https://google.github.io/styleguide/javaguide.html)

#### List of Prompts

We follow these rules regarding prompts to make clear where a certain command or example is executed.
This should help to avoid errors.

| Host/Partition         | Prompt           |
|------------------------|------------------|
| Localhost              | `marie@local$`   |
| Login nodes            | `marie@login$`   |
| Arbitrary compute node | `marie@compute$` |
| Compute node `Capella` | `marie@capella$` |
| Login node `Capella`   | `marie@login.capella$`  |
| Compute node `Barnard` | `marie@barnard$` |
| Login node `Barnard`   | `marie@login.barnard$`  |
| Compute node `Alpha`   | `marie@alpha$`   |
| Login node `Alpha`     | `marie@login.alpha$`   |
| Node `Julia`           | `marie@julia$`   |
| Compute node `Romeo`   | `marie@romeo$`   |
| Login node `Romeo`     | `marie@login.romeo$`   |
| Compute node `Power9`  | `marie@power9$`  |
| Login node `Power9`    | `marie@login.power9$`  |
| Partition `dcv`        | `marie@dcv$`     |

* **Always use a prompt**, even if there is no output provided for the shown command.
* All code blocks which specify some general command templates, e.g. containing `<` and `>`
  (see [placeholders](#placeholders) and [Code Styling Rules](#code-styling-rules)), should use
  `bash` for the code block. Additionally, an example invocation, perhaps with output, should be
  given with the normal `console` code block. See also
  [Code Block description below](#code-blocks-and-syntax-highlighting).
* Using some magic, the prompt as well as the output is identified and will not be copied!
* Stick to the [generic user name](#data-privacy-and-generic-names) `marie`.

#### Long Options

The general rule is to provide long over short parameter names where possible to ease
understanding. This holds especially for Slurm options, but also other commands.

??? example

    | Do | Don't |
    |----|-------|
    | `srun --nodes=2 --ntasks-per-node=4 [...]`| `srun -N 2 -n 4 [...]` |
    | `module load [...]` | `ml [...]` |

#### Equal Signs in Command-Line Options

Some tools with CLI (command-line interface) prefer specification of the argument with an equal
sign (`=`) between the option name and the value, e.g. `--long_option=value`. Others prefer a
whitespace, e.g. `--long_option value`. We respect the design decisions of the tool
developers and document the desired mimic for [long options](#long-options). If you are in doubt,
calling `tool --help` might provide the answer.

| Tool  | Preference             |
|-------|------------------------|
| Slurm | w/ Equal sign          |
| HPC-Workspace | w/o equal sign |

### Customize Search

The
[documentation for the search plugin](https://squidfunk.github.io/mkdocs-material/setup/setting-up-site-search/)
of the material theme is quite comprehensive. The search is realized as client-side search using the
open-source tool on [lunr](https://lunrjs.com/). The ranking of pages in search results bases on
so-called scoring. Please refer to
[lunrjs documentation](https://lunrjs.com/guides/searching.html#scoring) for details.

From time to time it might be necessary to **tweak the search priority of certain pages**.
For example, pages from the archive section should be ranked very low in search results. This can be
achieved by adding the front matter `search.boost` property added to the top of the Markdown file of
interest:

```Markdown
---
search:
  boost: 2
---

# Document Title

[...]
```

The documentation of this plugin gives no range for the boost values. We recommend to use this
feature carefully starting with low values.


# Contribute via Browser

In the following, it is outlined how to contribute to the
[HPC documentation](https://compendium.hpc.tu-dresden.de/) of
[TU Dresden/ZIH](https://tu-dresden.de/zih/) by means of GitLab's web interface using a standard web
browser only.

## Preparation

First of all, you need an account on [gitlab.hrz.tu-chemnitz.de](https://gitlab.hrz.tu-chemnitz.de).
Secondly, you need access to the project
[ZIH/hpcsupport/hpc-compendium](https://gitlab.hrz.tu-chemnitz.de/zih/hpcsupport/hpc-compendium).

The project is publicly visible, i.e., it is open to the world and any signed-in user has the
[Guest role](https://gitlab.hrz.tu-chemnitz.de/help/user/permissions.md) on this repository. Guests
have only very
[limited permissions](https://gitlab.hrz.tu-chemnitz.de/help/user/permissions.md#project-members-permissions).
In particular, as guest, you can contribute to the documentation by
[creating issues](howto_contribute.md#contribute-via-issue), but you cannot edit files and create
new branches.

To be granted the role **Developer**, please request access by clicking the corresponding button.

![Request access to the repository](misc/request_access.png)

Once you are granted the developer role, choose "ZIH/hpcsupport/hpc-compendium" in your project list.

!!! hint "Git basics"

    If you are not familiar with the basics of Git-based document revision control yet, please have
    a look at [GitLab tutorials](https://docs.gitlab.com/ee/tutorials/).

## Create a Branch

Your contribution starts by creating your own branch of the repository that will hold your edits and
additions. Create your branch by clicking on "+" near "preview->hpc-compendium/" as depicted in
the figure and click "New branch".

![create new branch](misc/cb_create_new_branch.png)

By default, the new branch should be created from the `preview` branch, as pre-selected.

Define a branch name that briefly describes what you plan to change, e.g., `edits-in-document-xyz`.
Then, click on "Create branch" as depicted in this figure:

![set branch name](misc/cb_set_branch_name.png)

As a result, you should now see your branch's name on top of your list of repository files as
depicted here:

![branch indicator](misc/cb_branch_indicator.png)

## Editing Existing Articles

Navigate the depicted document hierarchy under `doc.zih.tu-dresden.de/docs` until you find the
article to be edited. A click on the article's name opens a textual representation of the article.
In the top right corner of it, you find the button "Edit" to be clicked in order to make changes.
Once you completed your changes, click on "Commit changes". Please add meaningful comment about the
changes you made under "Commit message". Feel free to do as many changes and commits as you wish in
your branch of the repository.

## Adding New Article

Navigate the depicted document hierarchy under `doc.zih.tu-dresden.de/docs` to find a topic that
fits best to your article. To start a completely new article, click on "+ New file" as depicted
here:

![create new file](misc/cb_create_new_file.png)

Set a file name that corresponds well to your article like `application_xyz.md`.
(The file name should follow the pattern `fancy_title_and_more.md`.)
Once you completed your initial edits, click on "commit".

![commit new file](misc/cb_commit_file.png)

Finally, the new article needs to be added to the navigation section of the configuration file
`doc.zih.tu-dresden.de/mkdocs.yaml`.

## Submitting Articles for Publication

Once you are satisfied with your edits, you are ready for publication.
Therefore, your edits need to undergo an internal review process and pass the CI/CD pipeline tests.
This process is triggered by creating a "merge request", which serves the purpose of merging your edits
into the `preview` branch of the repository.

* Click on "Merge requests" (in the menu to the left) as depicted below.
* Then, click on the button "New merge request".
* Select your source branch (for example `edits-in-document-xyz`) and click on "Compare branches and
  continue". (The target branch is always `preview`. This is pre-selected - do not change!)
* The next screen will give you an overview of your changes. Please provide a meaningful
  description of the contributions. Once you checked them, click on "Create merge request".

![new merge request](misc/cb_new_merge_request.png)

## Revision of Articles

As stated earlier, all changes undergo a review process.
This covers automated checks contained in the CI/CD pipeline and the review by a maintainer.
This is to ensure the quality of all contributions, e. g. by checking our
[content rules](content_rules.md).
You can follow this process under
[Merge requests](https://gitlab.hrz.tu-chemnitz.de/zih/hpcsupport/hpc-compendium/-/merge_requests)
(where you initiated your merge request).
If you are asked to make corrections or changes, follow the directions as indicated.
Once your merge request has been accepted, the merge request will be closed and the branch will be deleted.
At this point, there is nothing else to do for you.
Except probably for waiting a little while until your changes become visible on the official web site.


# Contribute via Local Clone

In the following, it is outlined how to contribute to the
[HPC documentation](https://compendium.hpc.tu-dresden.de/) of
[TU Dresden/ZIH](https://tu-dresden.de/zih/) via a local clone of the Git repository. Although, this
document might seem very long describing complex steps, contributing is quite easy - trust us.

## Initial Setup of your Local Clone

Please follow this standard Git procedure for working with a local clone:

1. Fork the project on
[https://gitlab.hrz.tu-chemnitz.de/zih/hpcsupport/hpc-compendium](https://gitlab.hrz.tu-chemnitz.de/zih/hpcsupport/hpc-compendium)
or request access to the project.
1. Change to a local (unencrypted) filesystem. (We have seen problems running the container on an
ecryptfs filesystem. So you might want to use e.g. `/tmp` as the start directory.)
1. Clone the Git repository:
    1. `git clone git@gitlab.hrz.tu-chemnitz.de:zih/hpcsupport/hpc-compendium.git`
    1. If you forked the repository, instead use:
        - `git clone git@gitlab.hrz.tu-chemnitz.de:<YOUR_LOGIN>/hpc-compendium.git`
1. Change into the new directory:
    - `cd hpc-compendium`
1. If you forked the repository, add the original repository as a so-called remote:
    - `git remote add upstream-zih git@gitlab.hrz.tu-chemnitz.de:zih/hpcsupport/hpc-compendium.git`

## Working with your Local Clone

1. Whenever you start working on an issue, first make sure that your local data is up to date:
    1. `git checkout preview`
    1. `git pull origin preview`
    1. `git pull upstream-zih preview` (only required when you forked the project)
1. Create a new feature branch for you to work in. Ideally, name it like the file you want to
modify or the issue you want to work on, e.g.:
`git checkout -b 174-check-contribution-documentation` for issue 174 with title "Check contribution
documentation". (If you are uncertain about the name of a file, please look into `mkdocs.yaml`.)
1. Improve the documentation with your preferred editor, i.e. add new files and correct mistakes.
1. Use `git add <FILE>` to select your improvements for the next commit.
1. Commit the changes with `git commit -m "<DESCRIPTION>"`. The description should be a meaningful
description of your changes. If you work on an issue, please also add "Closes 174" (for issue 174).
1. Push the local changes to the GitLab server, e.g. with
`git push origin 174-check-contribution-documentation`.
1. As an output you get a link to create a merge request against the preview branch.
1. When the merge request is created, a continuous integration (CI) pipeline automatically checks
your contributions. If you forked the repository, these automatic checks are not available, but you
can [run checks locally](#run-the-proposed-checks-inside-container).

!!! tip

    When you contribute, please follow our [content rules](content_rules.md) to make incorporating
    your changes easy. We also check these rules via continuous integration checks and/or reviews.
    You can find the details and commands to [preview your changes](#start-the-local-web-server) and
    [apply checks](#run-the-proposed-checks-inside-container).

## Merging of Forked Repositories

When you have forked the repository as mentioned above, the process for merging is a bit different
from internal merge requests. Because branches of forks are not automatically checked by CI,
someone with at least developer access needs to do some more steps to incorporate the changes of
your MR:

1. The developer informs you about the start of merging process.
1. The developer needs to review your changes to make sure that your changes are specific and don't introduce
problems, such as changes in the Dockerfile or any script could.
1. The developer needs to create a branch in our repository. Let's call this "internal MR branch".
1. The developer needs to change the target branch of your MR from "preview" to "internal MR branch".
1. The developer needs to merge it.
1. The developer needs to open another MR from "internal MR branch" to "preview" to check whether
   the changes pass the CI checks.
1. The developer needs to fix things that were found by CI.
1. The developer informs you about the MR or asks for your support while fixing the CI.

When you follow our [content rules](content_rules.md) and
[run checks locally](#run-the-proposed-checks-inside-container), you are making this process
faster.

## Tools to Ensure Quality

Assuming you already have a working Docker installation and have cloned the repository as mentioned
above, a few more steps are necessary.

Build the Docker image. This might take a bit longer, as `mkdocs` and other necessary software
needs to be downloaded, but you have to run it only once in a while.
Building a container could be done with the following steps:

```console
marie@local$ cd hpc-compendium
marie@local$ doc.zih.tu-dresden.de/util/download-newest-mermaid.js.sh
marie@local$ docker build -t hpc-compendium .
```

To avoid a lot of retyping, set the following Git aliases once inside your local Git clone:

```console
marie@local$ git config alias.wikiscript '!docker run --name=hpc-compendium --rm -w /docs --mount src=${PWD},target=/docs,type=bind hpc-compendium'
marie@local$ git config alias.wiki '!docker run --name=hpc-compendium -p 8000:8000 --rm -w /docs --mount src=${PWD}/doc.zih.tu-dresden.de,target=/docs,type=bind hpc-compendium'
```

## Working with the Docker Container

Here is a suggestion of a workflow which might be suitable for you.

### Start the Local Web Server

The command(s) to start the dockerized web server is this:

```console
marie@local$ git wiki mkdocs serve -a 0.0.0.0:8000
```

You can view the documentation via `http://localhost:8000` in your browser, now.

!!! note

    You can keep the local web server running in this shell to always have the opportunity to see
    the result of your changes in the browser. Simply open another terminal window for other
    commands.
    If you cannot see the page in your browser, check if you can get the URL for your browser's
    address bar from a different terminal window:

    ```console
    marie@local$ echo http://$(docker inspect -f "{{.NetworkSettings.IPAddress}}" $(docker ps -qf "name=hpc-compendium")):8000
    ```

You can now update the contents in you preferred editor. The running container automatically takes
care of file changes and rebuilds the documentation whenever you save a file.

With the details described below, it will then be easy to follow the guidelines for local
correctness checks before submitting your changes and requesting the merge.

### Run the Proposed Checks Inside Container

In our continuous integration (CI) pipeline, a merge request triggers the automated check of

* correct links,
* correct spelling,
* correct text format.

These checks ensure a high quality and consistency of the content and follow our
[content rules](content_rules.md). If one of them fails, the merge request will not be accepted. To
prevent this, you can run these checks locally and adapt your files accordingly.

You are now ready to use the different checks, however we suggest to try the pre-commit hook.

#### Pre-commit Git Hook

We have several checks on the Markdown sources to ensure for a consistent and high quality of the
documentation. We recommend to automatically run checks whenever you try to commit a change. In this
case, failing checks prevent commits (unless you use option `--no-verify`). This can be accomplished
by adding a pre-commit hook to your local clone of the repository. The following code snippet shows
how to do that:

```console
marie@local$ cp doc.zih.tu-dresden.de/util/pre-commit .git/hooks/
```

!!! note

    The pre-commit hook only works, if you can use Docker without using `sudo`. If this is not
    already the case, use the command `adduser $USER docker` to enable Docker commands without
    `sudo` for the current user. Restart the Docker daemons afterwards.

Read on if you want to run a specific check.

#### Linter

If you want to check whether the Markdown files are formatted properly, use the following command:

```console
marie@local$ git wiki markdownlint docs
```

#### Spell Checker

For spell-checking a single file, e.g.
`doc.zih.tu-dresden.de/docs/software/big_data_frameworks.md`, use:

```console
marie@local$ git wikiscript doc.zih.tu-dresden.de/util/check-spelling.sh doc.zih.tu-dresden.de/docs/software/big_data_frameworks.md
```

For spell-checking all files, use:

```console
marie@local$ git wikiscript doc.zih.tu-dresden.de/util/check-spelling.sh -a
```

This outputs all words of all files that are unknown to the spell checker.
To let the spell checker "know" a word, append it to
`doc.zih.tu-dresden.de/wordlist.aspell`.

#### Check Pages Structure

The script `util/check-no-floating.sh` first checks the hierarchy depth of the pages structure and
the second check tests if every Markdown file is included in the navigation section of the
`mkdocs.yaml` file. Invoke it as follows:

```console
marie@local$ git wikiscript doc.zih.tu-dresden.de/util/check-no-floating.sh doc.zih.tu-dresden.de
```

#### Link Checker

!!! quote "Unknown programmer"

    No one likes dead links.

Therefore, we check the internal and external links within the Markdown source files.
With the script `doc.zih.tu-dresden.de/util/check-links.sh`, you can either check
[a single file](#single-file), [all modified files](#all-modified-files) or
[all files](#all-files) of the compendium.

##### Single File

To check the links within a single file, e.g.
`doc.zih.tu-dresden.de/docs/software/big_data_frameworks.md`, use:

```console
marie@local$ git wikiscript doc.zih.tu-dresden.de/util/check-links.sh docs/software/big_data_frameworks.md
```

##### All Modified Files

The script can also check the links in all modified files, i.e., Markdown files which are part
of the repository and different to the `preview` branch. Use this script before committing your
changes to make sure your commit passes the CI/CD pipeline.

```console
marie@local$ git wikiscript doc.zih.tu-dresden.de/util/check-links.sh -c
```

##### All Files

Checking the links of all Markdown files takes a moment of time:

```console
marie@local$ git wikiscript doc.zih.tu-dresden.de/util/check-links.sh -a
```


# How-To Contribute

!!! cite "Chinese proverb"

    Ink is better than the best memory.

In this section you will find information about the technical setup of this documentation, the
content rules that apply, the Git workflow, and specific ways to contribute.

Your contributions are highly welcome. This can range from fixing typos, improving the phrasing and
wording to adopting examples, command lines and adding new content. Our goal is to provide a
general, consistent and up to date documentation. Thus, it is by no means a static documentation.
Moreover, is is constantly reviewed and updated.

## Technical Setup

This documentation is written in Markdown and translated into static html pages using
[mkdocs](https://www.mkdocs.org/). The single configuration file `mkdocs.yml` contains the page
structure as well as the specification of the theme and extensions.

We manage all essential files (Markdown pages, graphics, configuration, theme, etc.) within a
[public Git repository](https://gitlab.hrz.tu-chemnitz.de/zih/hpcsupport/hpc-compendium),
allowing for collaborative working and revision control. GitLab's features offer different
possibilities of contribution and ensure up-to-date and consistent content by including a review
process. There are three possible ways how you can contribute to this documentation.
These are described below.

!!! tip "Before you start"

    Before you start your very first commit, please make sure that you are familiar with our
    [Git workflow](#git-workflow) and that you have at least skimmed through the
    [Content Rules](content_rules.md).

## Git Workflow

We employ a so-called Git feature workflow with a development branch. In our case, the working branch
is called `preview` and is kept in parallel to the `main` branch.

All contributions, e.g., new content, improved wording, fixed typos, etc., are added to separate
feature branches which base on `preview`. If the contribution is ready, you will have to create a
merge request back to the `preview` branch. A member of the ZIH team will review the changes
(four-eyes principle) and finally merge your changes to `preview`. All contributions need to pass
through the CI pipeline consisting of several checks to ensure compliance with the content rules.
Please, don't worry too much about the checks. The ZIH staff will help you with that. You can find
more information about the [CI/CD pipeline](#cicd-pipeline) in the eponymous subsection.

In order to publish the updates and make them visible in the compendium,
the changes on `preview` branch are either automatically merged into the `main` branch on every
Monday via a pipeline schedule, or manually by admin staff. Moreover, the `main` branch is deployed
to [https://compendium.hpc.tu-dresden.de](https://compendium.hpc.tu-dresden.de) and always reflects
a production-ready state. Manual interventions are only necessary in case of merge conflicts.
This process is handled by the admins.

???+ note "Graphic on Git workflow"

    The applied Git workflow is depicted in the following graphic. Here, two feature branches `foo`
    and `bar` are created basing on `preview`. Three individual commits are added to branch `foo`
    before it is ready and merged back to `preview`. The contributions on `bar` consist only one
    commit. In the end, all contribution are merged to the `main` branch.

    ```mermaid
    %% Issues:
    %% - showCommitLabel: false does not work; workaround is to use `commit id: " "`%%
    %% - Changing the theme does not effect the rendered output. %%
    %%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'showCommitLabel': false} }%%
    gitGraph
        commit
        branch preview
        checkout preview
        commit
        branch foo
        checkout foo
        commit
        commit
        checkout preview
        branch bar
        checkout bar
        commit
        checkout preview
        merge bar
        checkout foo
        commit
        checkout preview
        merge foo
        checkout main
        merge preview
    ```

## Content Rules

To ensure a high-quality and consistent documentation, and to make it easier for readers to
understand all content, we have established [Content rules](content_rules.md). Please follow
these rules regarding Markdown syntax and writing style when contributing! Furthermore, reviewing
your changes takes less time and your improvements appear faster on the official documentation.

!!! note

    If you contribute, you are fully and solely responsible for the content you create and have to
    ensure that you have the right to create it under applicable laws.

## Contribute via Issue

You can contribute to the documentation using
[GitLab's issue tracking system](https://gitlab.hrz.tu-chemnitz.de/zih/hpcsupport/hpc-compendium/-/issues).
For that, open an issue to report typos and missing documentation or request for more precise
wording etc. ZIH staff will get in touch with you to resolve the issue and improve the
documentation.

??? tip "Create an issue in GitLab"

    ![GIF showing how to create an issue in GitLab](misc/create_gitlab_issue.gif)
    {: align=center}

!!! warning "HPC support"

    Non-documentation issues and requests need to be send as ticket to
    [hpc-support@tu-dresden.de](mailto:hpc-support@tu-dresden.de).

## Contribute via Web IDE

If you have a web browser (most probably you are using it to read this page) and want to contribute
to the documentation, you are good to go. GitLab offers a rich and versatile web interface for
working with repositories. To start fixing typos and edit source files, you can find more
information on the page [Contributing via web browser](contribute_browser.md).

## Contribute via Local Clone

For experienced Git users, we provide a Docker container that includes all the checks of the CI
engine used in the backend. Using them should ensure that merge requests are not blocked due to
automatic checks.  The page [Contributing via local clone](contribute_container.md) provides you
with the details about how to set up and use your local clone of the repository.

## CI/CD Pipeline

All contributions need to pass through the CI pipeline which consists of various checks to ensure
that the [content rules](content_rules.md) have been followed.

The stages of the CI/CD pipeline are defined in a `.gitlab.yaml` file. For security reasons, this
file is maintained in a second, private repository.


# Sharing Data

This page should provide you some commands to share your data with other users or projects.

## Grant access on some file or directory to persons in your project

If all persons that should be able to access your data are in the same project, you can give them
access to your workspace, e. g. `input-data` via the following commands:

```console
marie@login$ id --group --name
p_number_crunch
marie@login$ chown -R marie:p_number_crunch /scratch/ws/1/marie-input-data
```

Now, everyone who is in project `p_number_crunch` should be able to access your data. If this is not
the case, you should check whether the file that your colleague wants to access is readable for the
group (`r` permission is set for the group) and every parent directory of that file is searchable
for the group (`x` permission is set for the group). For example, in the following case, a colleague
of `marie` cannot access `data-file` because the base directory `.` is not searchable for the group
as it does not have the `x` permission, even though the file has the permission `r` set for the
group. Thus, `marie` has to make the directory searchable by using `chmod`:

```console
marie@login$ ls -la /scratch/ws/1/marie-input-data
drwxr-----   4 marie    p_number_crunch   4096 27. Jun 17:13 .
drwxr-xr-x 444 operator adm             151552 14. Jul 09:41 ..
-rw-r-----   2 marie    p_number_crunch   4096 27. Jun 17:13 data-file
marie@login$ chmod g+x /scratch/ws/1/marie-input-data
marie@login$ ls -la /scratch/ws/1/marie-input-data
drwxr-x---   4 marie    p_number_crunch   4096 27. Jun 17:13 .
drwxr-xr-x 444 operator adm             151552 14. Jul 09:41 ..
-rw-r-----   2 marie    p_number_crunch   4096 27. Jun 17:13 data-file
```

!!! danger "New file inherits group and permissions of the creator"

    When a user creates a file, the created file is associated to that user and inherits the user's
    default group. If the user is in multiple groups/projects, he/she has to ensure, that the new
    file is associated with the project's group. This can be done using `chown` and `chmod` as shown
    above. Another possibility is to use an environment file `env.sh` with the following content:

    ```bash
    newgrp p_number_crunch  # files should have this group by default
    umask o-rwx             # prevent creating files that allow persons not in this group (a.k.a. others) to read, write or execute something
    ```

    Before creating new files, users can now load this file using `source` in order to ensure that
    new files automatically get the right group:

    ```console
    marie@login$ cd /scratch/ws/1/marie-input-data
    marie@login$ source /projects/p_number_crunch/env.sh
    bash-4.2$ touch new-file    #create a new file
    ```

Read on, if you want to restrict access to specific persons outside of your group, but don't want to
permit everyone to access your data.

## Grant access on some file or directory to persons from various projects

[Access Control Lists](https://en.wikipedia.org/wiki/Access-control_list) (ACLs) can be used, when
`chmod` is not sufficient anymore, e. g. because you want to permit accessing a particular file for
persons from your project and also some persons outside of your project, but not everyone.

!!! note

    At the moment `setfacl` is only working on our Lustre filesystems (`/data/horse` and
    `/data/walrus`).

The command `setfacl` is used to manage access rights for workspaces. To view the current access
rights, use the command `getfacl`. The following commands are used to grant a user access to the
workspace.

If you are unsure what your group/project is, you can use the following command to find out:

```console
marie@login$ id --group --name
p_number_crunch
```

If you are in multiple projects, you could see all of them using `--groups` instead of `--group`:

```console
marie@login$ id --groups --name
p_number_crunch
```

!!! example "Grant a user full access to the workspace folder"

    ```console
    marie@login$ setfacl --modify=u:<username>:rwx <path_to_workspace>
    ```

    For example, if `marie` wants to give her colleague `martin` access to the workspace
    `input-data` she has created, she would use the following command:

    ```console
    marie@login$ setfacl --modify=u:martin:rwx /scratch/ws/1/marie-input-data
    ```

!!! example "Inherit these same rights to all newly created files and folders"

    ```console
    marie@login$ setfacl --modify=d:u:<username>:rwx <path_to_workspace>
    ```

!!! example "Grant a project full access to the workspace folder"

    ```console
    marie@login$ setfacl --modify=g:<projectname>:rwx <path_to_workspace>
    ```

    For example, if `marie` wants to give all colleagues in `martin`'s project `p_long_computations`
    access to the workspace `input-data` she has created, she would use the following command:

    ```console
    marie@login$ setfacl --modify=g:p_long_computations:rwx /scratch/ws/1/marie-input-data
    ```

!!! example "Inherit these same rights to all newly created files and folders"

    ```console
    marie@login$ setfacl --modify=d:g:<projectname>:rwx <path_to_workspace>
    ```

If you already have files inside your workspace, remember to use the `-R` or `--recursive` options
to apply these ACL changes to all files.

!!! example "Remove access rights for a particular user"

    If you want to remove a user's access then use:

    ```console
    marie@login$ setfacl --remove=u:<username> <path_to_workspace>
    ```

    Remember to also remove the default access rights, if you added them previously:

    ```console
    marie@login$ setfacl --remove=d:u:<username> <path_to_workspace>
    ```

    For example, if `marie` wants to remove access to the workspace `input-data` she has given
    `martin` earlier:

    ```console
    marie@login$ setfacl --remove=u:martin /scratch/ws/1/marie-input-data
    ```

    And just to be sure, she would also remove default access rights for him:

    ```console
    marie@login$ setfacl --remove=d:u:martin /scratch/ws/1/marie-input-data
    ```

More details on ACLs can be found on the [setfacl man page](https://man.archlinux.org/man/setfacl.1).


# Filesystems

As soon as you have access to ZIH systems, you have to manage your data. Several filesystems are
available. Each filesystem serves for special purpose according to their respective capacity,
performance and permanence.

We differentiate between **permanent filesystems** and **working filesystems**:

* The [permanent filesystems](permanent.md), i.e. `/home` and `/projects`, are meant to hold your
source code, configuration files, and other permanent data.
* The [working filesystems](working.md), i.e, `horse`, `walrus`, etc., are designed as scratch
filesystems holding your working and temporary data, e.g., input and output of your compute
jobs.

## Recommendations for Filesystem Usage

To work as efficient as possible, consider the following points

- Save source code etc. in `/home` or `/projects`
- Store checkpoints and other temporary data in [workspaces](workspaces.md) on `horse`
- Compilation should be executed in `/dev/shm` or `/tmp`

Getting high I/O-bandwidth

- Use many clients
- Use many processes (writing in the same file at the same time is possible)
- Use large I/O transfer blocks
- Avoid reading many small files. Use data container e. g.
  [ratarmount](../software/utilities.md#direct-archive-access-without-extraction-using-ratarmount)
  to bundle small files into one

## Cheat Sheet for Debugging Filesystem Issues

Users can select from the following commands to get some idea about
their data.

### General

For the first view, you can use the command `df`.

```console
marie@login$ df
```

Alternatively, you can use the command `findmnt`, which is also able to report space usage
by adding the parameter `-D`:

```console
marie@login$ findmnt -D
```

Optionally, you can use the parameter `-t` to specify the filesystem type or the parameter `-o` to
alter the output.

!!! important

    Do **not** use the `du`-command for this purpose. It is able to cause issues
    for other users, while reading data from the filesystem.


# Long-Term Preservation of Research Data

## Why should research data be preserved?

There are several reasons. On the one hand, research data should be preserved to make the results
reproducible. On the other hand research data could be used a second time for investigating another
question. In the latter case persistent identifiers (like DOI) are needed to make these data
findable and citable. In both cases it is important to add meta-data to the data.

## Which research data should be preserved?

Since large quantities of data are nowadays produced it is not possible to store everything. The
researcher needs to decide which data are worth and important to keep.

In case these data come from simulations, there are two possibilities: 1 Storing the result of the
simulations 1 Storing the software and the input-values

Which of these possibilities is preferable depends on the time the simulations need and on the size
of the result of the calculations. Here one needs to estimate, which possibility is cheaper.

**This is, what DFG says** (translated from
<http://www.dfg.de/download/pdf/foerderung/programme/lis/ua_inf_empfehlungen_200901.pdf>, page 2):

*Primary research data are data, which were created in the course* *of studies of sources,
experiments, measurements or surveys. They are the* *basis of scholarly publications*. *The
definition of primary research data depends on the subject*. *Each community of researchers should
decide by itself, if raw data are* *already primary research data or at which degree of aggregation
data* *should be preserved. Further it should be agreed upon the granularity* *of research data: how
many data yield one set of data, which will be* *given a persistent identifier*.

## Why should I add Meta-Data to my data?

Many researchers think, that adding meta-data is time-consuming and senseless but that isn't true.
On the contrary, adding meta-data is very important, since they should enable other researchers to
know, how and in which circumstances these data are created, in which format they are saved, and
which software in which version is needed to view the data, and so on, so that other researchers can
reproduce these data or use them for new investigations. Last but not least meta-data should enable
you in ten years time to know what your data describe, which you created such a long time ago.

## What are Meta-Data?

Meta-data means data about data. Meta-data are information about the stored file. There can be
administrative meta-data, descriptive meta-data, technical meta-data and so on. Often meta-data are
stored in XML-format but free text is also possible. There are some meta-data standards like
[Dublin Core](http://dublincore.org/) or
[LMER](https://www.dnb.de/DE/Professionell/Standardisierung/Standards/_content/lmer_uof.html)
Below are some examples:

- possible meta-data for a book would be:
    - Title
    - Author
    - Publisher
    - Publication
    - year
    - ISBN
- possible meta-data for an electronically saved image would be:
    - resolution of the image
    - information about the color depth of the picture
    - file format (jpg or tiff or ...)
    - file size how was this image created (digital camera, scanner, ...)
    - description of what the image shows
    - creation date of the picture
    - name of the person who made the picture
- meta-data for the result of a calculation/simulation could be:
    - file format
    - file size
    - input data
    - which software in which version was used to calculate the result/to do the simulation
    - configuration of the software
    - date of the calculation/simulation (start/end or start/duration)
    - computer on which the calculation/simulation was done
    - name of the person who submitted the calculation/simulation
    - description of what was calculated/simulated

## Where can I get more information about management of research data?

Please visit the wiki [forschungsdaten.org](https://www.forschungsdaten.org/en/) to learn more about
all of the different aspects of research data management.

For questions or individual consultations regarding research data management in general or any of
its certain aspects, you can contact the
[Service Center Research Data](https://tu-dresden.de/forschung-transfer/services-fuer-forschende/kontaktstelle-forschungsdaten?set_language=en)
(Kontaktstelle Forschungsdaten) of TU Dresden.

## I want to archive my research data at ZIH safely. How can I do that?

For TU Dresden there exist two different services at ZIH for archiving research data. Both of
them ensure high data safety by duplicating data internally at two separate locations and
require some data preparation (e.g. packaging), but serve different use cases:

### Storing very infrequently used data during the course of the project

The intermediate archive is a tape storage easily accessible as a directory
(`/archiv/<HRSK-project>/` or `/archiv/<login>/`) using the
[Dataport Nodes](../data_transfer/dataport_nodes.md)
and
[Datamover tools](../data_transfer/datamover.md) to move your data to.
For detailed information please visit the
[ZIH intermediate archive documentation](https://tu-dresden.de/zih/dienste/service-katalog/arbeitsumgebung/backup_archiv/archivierung_am_zih#section-2-1).

!!! note

    The usage of the HRSK-project-related archive is preferable to the login-related archive, as
    this enables assigning access rights and responsibility across multiple researchers, due to the
    common staff turnover in research.

The use of the intermediate archive usually is limited by the end of the corresponding
research project. Afterwards data is required to be removed, tidied up and submitted to a
long-term repository (see next section).

The intermediate archive is the preferred service when you keep large, mostly unused data volumes
during the course of your research project; if you want or need to free storage capacities, but
you are still not able to define certain or relevant datasets for long-term archival.

If you are able to identify complete and final datasets, which you probably won't use actively
anymore, then repositories as described in the next section may be the more appropriate selection.

### Archiving data beyond the project lifetime, for 10 years and above

According to good scientific practice (cf.
[DFG guidelines, #17](https://www.dfg.de/download/pdf/foerderung/rechtliche_rahmenbedingungen/gute_wissenschaftliche_praxis/kodex_gwp.pdf))
and
[TU Dresden research data guidelines](https://tu-dresden.de/tu-dresden/qualitaetsmanagement/ressourcen/dateien/wisprax/Leitlinien-fuer-den-Umgang-mit-Forschungsdaten-an-der-TU-Dresden.pdf),
relevant research data needs to be archived at least for 10 years. The
[OpARA service](https://opara.zih.tu-dresden.de/xmlui/) (Open Access Repository and Archive) is the
joint research data repository service for Saxon universities to address this requirement.

Data can be uploaded and, to comply to the demands of long-term understanding of data, additional
metadata and description must be added. Large datasets may be optionally imported beforehand. In
this case, please contact the
[TU Dresden Service Desk](mailto:servicedesk@tu-dresden.de?subject=OpARA:%20Data%20Import).
Optionally, data can also be **published** by OpARA. To ensure data quality, data submissions
undergo a review process.

Beyond OpARA, it is also recommended to use discipline-specific data repositories for data
publications. Usually those are well known in a scientific community, and offer better fitting
options of data description and classification. Please visit [re3data.org](https://re3data.org)
to look up a suitable one for your discipline.


# Lustre

## Good Practices

!!! hint "Avoid accessing metadata information"

    Querying metadata information such as file and directory attributes is a resource intensive task
    in Lustre filesystems. When these tasks are performed frequently or over large directories, it
    can degrade the filesystem's performance and thus affect all users.

In this sense, you should minimize the usage of system calls querying or modifying file
and directory attributes, e.g. `stat()`, `statx()`, `open()`, `openat()` etc.

Please, also avoid commands basing on the above mentioned system calls such as `ls -l` and
`ls --color`. Instead, you should invoke `ls` or `ls -l <filename>` to reduce metadata operations.
This also holds for commands walking the filesystems recursively performing massive metadata
operations such as `ls -R`, `find`, `locate`, `du` and `df`.

Lustre offers a number of commands that are suited to its architecture.

| Good | Bad |
|:-----|:----|
| `lfs df` | `df` |
| `lfs find` | `find` |
| `ls -l <filename>` | `ls -l` |
| `ls` | `ls --color` |

In case commands such as `du` are needed, for example to identify large
directories, these commands should be applied to as little data as
possible. You should not just query the main directory in general, you
should try to work in the sub directories first. The deeper in the
structure, the better.

### Searching the Directory Tree

The command `lfs find` searches the directory tree for files matching the specified parameters.

```console
marie@login$ lfs find <root directory> [options]
```

If no option is provided, `lfs find` will efficiently list all files in a given directory and its
subdirectories, without fetching any file attributes.

Useful options:

* `--atime n` file was last accessed n*24 hours ago
* `--ctime n` file was last changed n*24 hours ago
* `--mtime n` file was last modified n*24 hours ago
* `--maxdepth n` limits find to descend at most n levels of directory tree
* `--print0|-0` print full file name to standard output if it matches the specified parameters,
  followed by a NUL character.
* `--name arg` filename matches the given filename (supporting regular expression and wildcards)
* `--type [b|c|d|f|p|l|s]` file has type: **b**lock, **c**haracter, **d**irectory, **f**ile,
  **p**ipe, sym**l**ink, or **s**ocket.

??? example "Example: List files older than 30 days"

    The follwing command will find and list all files older than 30 days in the workspace
    `/scratch/ws/0/marie-number_crunch`:

    ```console
    marie@login lfs find /scratch/ws/0/marie-number_crunch --mtime +30 --type f
    /scratch/ws/0/marie-number_crunch/jobfile.sh
    /scratch/ws/0/marie-number_crunch/0001.dat
    /scratch/ws/0/marie-number_crunch/load_profile.sh
    /scratch/ws/0/marie-number_crunch/mes0001
    /scratch/ws/0/marie-number_crunch/d3dump01.0002
    /scratch/ws/0/marie-number_crunch/mes0032
    /scratch/ws/0/marie-number_crunch/dump01.0003
    /scratch/ws/0/marie-number_crunch/slurm-1234567.log
    ```

## Useful Commands for Lustre

These commands work for Lustre filesystems `/data/horse` and `/data/walrus`. In order to hold this
documentation as general as possible we will use `<filesystem>` as a placeholder for the Lustre
filesystems. Just replace it when invoking the commands with the Lustre filesystem of interest.

Lustre's `lfs` client utility provides several options for monitoring and configuring your Lustre
environment.

`lfs` can be used in interactive and in command line mode. To enter the interactive mode, you just
call `lfs` and enter your commands. Since, both modes provide identical options, we use the command
line mode within this documentation.

!!! hint "Filesystem vs. Path"

    If you provide a path to the lfs commands instead of a filesystem, the lfs option is applied to
    the filesystem this path is in. Thus, the passed information refers to the whole filesystem,
    not the path.

You can retrieve a complete list of available options:

```console
marie@login lfs --list-commands
setstripe           getstripe           setdirstripe        getdirstripe
mkdir               rm_entry            pool_list           find
check               osts                mdts                df
[...]
```

To get more information on a specific option, enter `help` followed by the option of interest:

```console
marie@login lfs help df
df: report filesystem disk space usage or inodes usage of each MDS and all OSDs or a batch belonging to a specific pool.
Usage: df [--inodes|-i] [--human-readable|-h] [--lazy|-l]
          [--pool|-p <fsname>[.<pool>]] [path]
```

More comprehensive documentation can be found in the man pages of lfs (`man lfs`).

### Listing Disk Space Usage

The command `lfs df` lists the filesystems disk space usage:

```console
marie@login$ lfs df -h <filesystem>
```

Useful options:

* `-h` outputs the units in human readable format.
* `-i` reports inode usage for each target and in summary.

!!! example "Example disk space usage at `/scratch`"

    At one moment in time, the disk space usage of the Lustre filesystem `/scratch` was:

    ```console
    lfs df -h /scratch
    UUID                       bytes        Used   Available Use% Mounted on
    scratch2-MDT0000_UUID        4.0T      502.8G        3.6T  13% /lustre/scratch2[MDT:0]
    scratch2-MDT0001_UUID      408.0G      117.7G      290.3G  29% /lustre/scratch2[MDT:1]
    scratch2-OST0000_UUID       28.9T       25.1T        3.7T  88% /lustre/scratch2[OST:0]
    scratch2-OST0001_UUID       28.9T       24.7T        4.1T  86% /lustre/scratch2[OST:1]
    scratch2-OST0002_UUID       28.9T       25.0T        3.9T  87% /lustre/scratch2[OST:2]
    scratch2-OST0003_UUID       28.9T       25.1T        3.8T  87% /lustre/scratch2[OST:3]
    [...]
    scratch2-OST008d_UUID       28.9T       25.0T        3.8T  87% /lustre/scratch2[OST:141]
    scratch2-OST008e_UUID       28.9T       24.9T        4.0T  87% /lustre/scratch2[OST:142]
    scratch2-OST008f_UUID       28.9T       25.3T        3.6T  88% /lustre/scratch2[OST:143]

    filesystem_summary:         4.1P        3.5P      571.8T  87% /lustre/scratch2
    ```

    The disk space usage is displayed separately for each MDS and OST as well in total. You can see
    that the usage is quite balanced between all MDSs and OSTs.

    If very large files are not properly stripped across several OSTs, the filesystem might become
    unbalanced with one server near 100% full.

### Listing Personal Disk Usages and Limits

To list your personal filesystem usage and limits (quota), invoke

```console
marie@login$ lfs quota -h -u $USER <filesystem>
```

Useful options:

* `-h` outputs the units in human readable format.
* `-u|-g|-p <arg>` displays quota for specific user, group or project.
* `-v` displays the usage on each OST.

### Listing OSTs

You can list all OSTs available in a particular Lustre filesystem using `lfs osts`:

```console
marie@login$ lfs osts <filesystem>
```

If a path is specified, only OSTs belonging to the specified path are displayed.

### View Striping Information

```console
marie@login$ lfs getstripe myfile
marie@login$ lfs getstripe -d mydirectory
```

The argument `-d` will also display striping for all files in the directory.


# Data Life Cycle Management

Correct organization of the structure of an HPC project is a straightforward way to the efficient
work of the whole team. There have to be rules and regulations that every member should follow. The
uniformity of the project can be achieved by taking into account and setting up correctly

* the same **set of software** (modules, compiler, packages, libraries, etc),
* a defined **data life cycle management** including the same **data storage** or set of them,
* and **access rights** to project data.

The used set of software within an HPC project can be managed with environments on different
levels either defined by [modules](../software/modules.md), [containers](../software/containers.md)
or by [Python virtual environments](../software/python_virtual_environments.md).
In the following, a brief overview on relevant topics w.r.t. data life cycle management is provided.

## Data Storage and Management

In general, you should separate your data and store it on the appropriate storage and filesystem.
What is the appropriate storage and filesystem depends on the amount/volume of data and its kind,
and might differ over time. Please note the following rules of thumb:

* Use your personal `/home` directory for the limited amount of *personal data*, e.g., simple
  examples and the results of calculations. Your `/home` directory is not a working directory!
  However, `/home` filesystem is [backed up](#backup). The section
  [Global `/home` Filesystem](permanent.md#global-home-filesystem) provides additional information.
* Use your `/project` directory for project-related data. This directory enables collaboration by
  sharing data with colleagues and project members. Please refer to the section
  [Global `/projects` Filesystem](permanent.md#global-projects-filesystem) for further information.
* Use [`workspaces`](workspaces.md) as a place for your *working data* (i.e. data sets).
  Recommendations of choosing the most suitable filesystem for your workspaces is presented on the
  page [Working Filesystems](working.md).
* Use the [Intermediate Archive](intermediate_archive.md) and the
  [Long-Term Archive](longterm_preservation.md) to store all kind of data that needs to be kept
  for a long time, e.g. result data.

### Backup

The backup is a crucial part of any project. Organize it at the beginning of the project. The
backup mechanism on ZIH systems covers **only** the filesystems `/home` and `/projects`. The section
[Backup](permanent.md#backup) holds additional information.

!!! warning

    If you accidentally delete your data in the "no backup" filesystems it **can not be restored**!

### Folder Structure and Organizing Data

Organizing of living data using the filesystem helps for consistency of the
project. We recommend following the rules for your work regarding:

* Organizing the data: Never change the original data; Automatize the organizing the data; Clearly
  separate intermediate and final output in the filenames; Carry identifier and original name
  along in your analysis pipeline; Make outputs clearly identifiable; Document your analysis
  steps.
* Naming Data: Keep short, but meaningful names; Keep standard file endings; File names
  don’t replace documentation and metadata; Use standards of your discipline; Make rules for your
  project, document and keep them (See the [README recommendations](#readme-recommendation) below)

This is the example of an organization (hierarchical) for the folder structure. Use it as a visual
illustration of the above:

![Organizing_Data-using_file_systems.png](misc/Organizing_Data-using_file_systems.png)
{: align="center"}

Keep in mind the [input-process-output pattern](https://en.wikipedia.org/wiki/IPO_model#Programming)
for the folder structure within your projects.

#### README Recommendation

In general, a [README](https://en.wikipedia.org/wiki/README) file provides a brief and general
information on the software or project. A `README` file is used to explain the purpose of the
project and the **structure** of the project in a short way. We recommend providing a `README` file
for entire project as well as for every important folder in the project.

Example of the structure for the README: Think first: What is calculated why? (Description); What is
expected? (software and version)

!!! example "README"

    ```Bash
    Title:
    User:
    Date:
    Description:
    Software:
    Version:
    Repo URL:
    ```

### Metadata

Another important aspect is the Metadata. It is sufficient to use
[Metadata](longterm_preservation.md#what-are-meta-data) for your HPC project. Metadata
standards, i.e.,
[Dublin core](http://dublincore.org/resources/metadata-basics/),
[OME](https://www.openmicroscopy.org/),
will help to do it easier.

### Data Hygiene

Don't forget about data hygiene: Classify your current data into critical (need it now), necessary
(need it later) or unnecessary (redundant, trivial or obsolete); Track and classify data throughout
its life cycle (from creation, storage and use to sharing, archiving and destruction); Erase the data
you don’t need throughout its life cycle.

### Access Rights

The concept of **permissions** and **ownership** is crucial in Linux. See the
[slides of HPC introduction](../misc/HPC-Introduction.pdf) for understanding of the main concept.
Standard Linux changing permission command (i.e `chmod`) valid for ZIH systems as well. The
**group** access level contains members of your project group. Be careful with 'write' permission
and never allow to change the original data.


# Permanent Filesystems

!!! hint

    Do not use permanent filesystems as work directories:

    - Even temporary files are kept in the snapshots and in the backup tapes over a long time,
    senselessly filling the disks,
    - By the sheer number and volume of work files, they may keep the backup from working efficiently.

| Filesystem Name   | Usable Directory  | Availability | Type     | Quota              |
|:------------------|:------------------|:-------------|:---------|:-------------------|
| Home              | `/home`           | global       | Lustre   | per user: 50 GB    |
| Projects          | `/projects`       | global       | NFS      | per project        |

## Global /home Filesystem

Each user has 50 GiB in a `/home` directory independent of the granted capacity for the project.
The home directory is mounted with read-write permissions on all nodes of the ZIH system.

Hints for the usage of the global home directory:

- If you need distinct `.bashrc` files for each machine, you should
  create separate files for them, named `.bashrc_<machine_name>`

If a user exceeds her/his quota (total size OR total number of files) she/he cannot
submit jobs into the batch system. Running jobs are not affected.

!!! note

     We have no feasible way to get the contribution of
     a single user to a project's disk usage.

Some applications and frameworks are known to store cache or temporary data at places where quota
applies. You can change the default places using environment variables. We suggest to put such data
in `/tmp` or workspaces.
We cannot list all applications that do this, but some known ones are

| Application      | Environment variable              |
|:-----------------|:----------------------------------|
| Singularity      | `SINGULARITY_CACHEDIR`            |
| pip              | `PIP_CACHE_DIR`                   |
| Hugging Face      | `HF_HOME` and `TRANSFORMERS_CACHE`|
| Torch Extensions | `TORCH_EXTENSIONS_DIR`            |

Python virtual environments and conda directories can grow quickly,
so they should also be placed inside workspaces.

## Global /projects Filesystem

For project data, we have a global project directory, that allows better collaboration between the
members of an HPC project.
Typically, all members of the project have read/write access to that directory.
It can only be written to on the login and export nodes.

!!! note

    On compute nodes, `/projects` is mounted as read-only, because it must not be used as
    work directory and heavy I/O.

## Backup

Just for the eventuality of a major filesystem crash, we keep tape-based backups of our
permanent filesystems for 180 days. Please send a
[ticket to the HPC support team](mailto:hpc-support@tu-dresden.de) in case you need backuped data.

## Quotas

The quotas of the permanent filesystem are meant to help users to keep only data that is necessary.
Especially in HPC, it happens that millions of temporary files are created within hours. This is the
main reason for performance degradation of the filesystem.

!!! note

    If a quota is exceeded - project or home - (total size OR total number of files)
    job submission is forbidden. Running jobs are not affected.

The following commands can be used for monitoring:

- `show_resources` shows your projects' usage of the filesystem.
- `quota -s -f /home` shows the user's usage of the filesystem.

In case a quota is above its limits:

- Remove core dumps and temporary data
- Talk with your colleagues to identify unused or unnecessarily stored data
- Check your workflow and use `/tmp` or the scratch filesystems for temporary files
- *Systematically* handle your important data:
    - For later use (weeks...months) at the ZIH systems, build and zip tar
      archives with meaningful names or IDs and store them, e.g., in a workspace in the
      [`walrus` filesystem](working.md) or an [archive](intermediate_archive.md)
    - Refer to the hints for [long-term preservation of research data](longterm_preservation.md)


# Working Filesystems

As soon as you have access to ZIH systems, you have to manage your data. Several filesystems are
available. Each filesystem serves for special purpose according to their respective capacity,
performance and permanence.

| Filesystem Type | Usable Directory  | Capacity | Availability       | Remarks                                                   |
|:----------------|:------------------|:---------|:-------------------|:----------------------------------------------------------|
| `Lustre`        | `/data/horse`     | 20 PB    | global             | Only accessible via [Workspaces](workspaces.md). **The(!)** working directory to meet almost all demands |
| `Lustre`        | `/data/walrus`    | 20 PB    | global             | Only accessible via [Workspaces](workspaces.md). For moderately low bandwidth, low IOPS. Mounted read-only on compute nodes. |
| `WEKAio`        | `/data/weasel`    | 1 PB     | global (w/o Power) | *Coming 2024!* For high IOPS                              |
| `ext4`          | `/tmp`            | 95 GB    | node local         | Systems: tbd. Is cleaned up after the job automatically.  |
| `WEKAio`        | `/data/cat`       | 1 PB     | only Capella  | For high IOPS. Only available on [`Capella`](../jobs_and_resources/hardware_overview.md#capella).                 |

## Recommendations for Filesystem Usage

To work as efficient as possible, consider the following points

- Save source code etc. in `/home` or `/projects/...`
- Store checkpoints and other temporary data in [workspaces](workspaces.md) on `horse`
- Compilation in `/dev/shm` or `/tmp`

Getting high I/O-bandwidth

- Use many clients
- Use many processes (writing in the same file at the same time is possible)
- Use large I/O transfer blocks
- Avoid reading many small files. Use data container e. g.
  [ratarmount](../software/utilities.md#direct-archive-access-without-extraction-using-ratarmount)
  to bundle small files into one

## Cheat Sheet for Debugging Filesystem Issues

Users can select from the following commands to get some idea about their data.

### General

For the first view, you can use the command `df`.

```console
marie@login$ df
```

Alternatively, you can use the command `findmnt`, which is also able to report space usage
by adding the parameter `-D`:

```console
marie@login$ findmnt -D
```

Optionally, you can use the parameter `-t` to specify the filesystem type or the parameter `-o` to
alter the output.

!!! important

    Do **not** use the `du`-command for this purpose. It is able to cause issues
    for other users, while reading data from the filesystem.


# Workspaces

Storage systems differ in terms of capacity, streaming bandwidth, IOPS rate, etc. Price and
efficiency don't allow to have it all in one. That is why fast parallel filesystems at ZIH have
restrictions with regards to **lifetime** and volume **[quota](permanent.md#quotas)**. The mechanism
of using _workspaces_ enables you to better manage your HPC data. It is common and used at a large
number of HPC centers.

!!! note

    A **workspace** is a directory, with an associated expiration date, created on behalf of a user
    in a certain filesystem.

Once the workspace has reached its expiration date, it gets moved to a hidden directory and enters a
grace period. Once the grace period ends, the workspace is deleted permanently. The maximum lifetime
of a workspace depends on the storage system. All workspaces can be extended a certain amount of
times.

!!! tip

    Use the faster filesystems if you need to write temporary data in your computations, and use
    the capacity oriented filesystems if you only need to read data for your computations. Please
    keep track of your data and move it to a capacity oriented filesystem after the end of your
    computations.

## Workspace Management

### Workspace Lifetimes

Since the workspace filesystems are intended for different use cases and thus differ in
performance, their granted timespans differ accordingly. The maximum lifetime and number of
renewals are provided in the following table.

| Filesystem (use with parameter `--filesystem <filesystem>`) | Max. Duration in Days | Extensions | Keeptime |
|:------------------------------------------------------------|---------------:|-----------:|---------:|
| `horse`                                                     | 100            | 10         | 30       |
| `walrus`                                                    | 100            | 10         | 60       |
| `cat`                                                    |             | 2          | 30       |
{: summary="Settings for Workspace Filesystems."}

!!! note

    Currently, not all filesystems are available on all of our five clusters. The page
    [Working Filesystems](working.md) provides the necessary information.

### List Available Filesystems

To list all available filesystems for using workspaces, you can either invoke `ws_list -l` or
`ws_find --list`. Since not all workspace filesystems are available on all HPC systems, the concrete
output differs depending on the system you are logged in. The page [Working Filesystems](working.md)
provides information which filesystem is available on which cluster.

=== "Barnard"

    ```console
    marie@login.barnard$ ws_list -l
    available filesystems:
    horse (default)
    walrus
    ```

=== "Alpha Centauri"

    ```console
    marie@login.alpha$ ws_list -l
    available filesystems:
    horse (default)
    walrus
    ```

=== "Capella"

    ```console
    marie@login.capella$ ws_list -l
    available filesystems:
    horse
    walrus
    cat  (default)
    ```

=== "Romeo"

    ```console
    marie@login.romeo$ ws_list -l
    available filesystems:
    horse (default)
    walrus
    ```

!!! note "Default filesystem"

    The output of the commands `ws_find --list` and `ws_list -l` will indicate the
    **default filesystem**. If you prefer another filesystem (cf. section
    [List Available Filesystems](#list-available-filesystems)), you have to explictly
    provide the option `--filesystem <filesystem>` to the workspace commands. If the default
    filesystems is the one you want to work with, you can omit this option.

### List Current Workspaces

The command `ws_list` lists all your currently active (,i.e, not expired) workspaces, e.g.

```console
marie@login$ ws_list
id: test-workspace
    workspace directory  : /data/horse/ws/marie-test-workspace
    remaining time       : 89 days 23 hours
    creation time        : Wed Dec  6 14:46:12 2023
    expiration date      : Tue Mar  5 14:46:12 2024
    filesystem name      : horse
    available extensions : 10
```

The output of `ws_list` can be customized via several options. The following switch tab provides a
overview of some of these options. All available options can be queried by `ws_list --help`.

=== "Certain filesystem"

    ```console
    marie@login$ ws_list --filesystem walrus
    id: marie-numbercrunch
        workspace directory  : /data/walrus/ws/marie-numbercrunch
        remaining time       : 89 days 23 hours
        creation time        : Wed Dec  6 14:49:55 2023
        expiration date      : Tue Mar  5 14:49:55 2024
        filesystem name      : walrus
        available extensions : 2
    ```

=== "Verbose output"

    ```console
    marie@login$ ws_list -v
    id: test-workspace
        workspace directory  : /data/horse/ws/marie-test-workspace
        remaining time       : 89 days 23 hours
        creation time        : Wed Dec  6 14:46:12 2023
        expiration date      : Tue Mar  5 14:46:12 2024
        filesystem name      : scratch
        available extensions : 10
        acctcode             : p_number_crunch
        reminder             : Tue Feb 27 14:46:12 2024
        mailaddress          : marie@tu-dresden.de
    ```

=== "Terse output"

    ```console
    marie@login$ ws_list -t
    id: test-workspace
        workspace directory  : /data/horse/ws/marie-test-workspace
        remaining time       : 89 days 23 hours
        available extensions : 10
    id: numbercrunch
        workspace directory  : /data/walrus/ws/marie-numbercrunch
        remaining time       : 89 days 23 hours
        available extensions : 2
    ```

=== "Show only names"

    ```console
    marie@login$ ws_list -s
    test-workspace
    numbercrunch
    ```

=== "Sort by remaining time"

    You can list your currently allocated workspace by remaining time. This is especially useful
    for housekeeping tasks, such as extending soon expiring workspaces if necessary.

    ```
    marie@login$ ws_list -R -t
    id: test-workspace
         workspace directory  : /data/horse/ws/marie-test-workspace
         remaining time       : 89 days 23 hours
         available extensions : 10
    id: marie-numbercrunch
        workspace directory  : /data/walrus/ws/marie-numbercrunch
        remaining time       : 89 days 23 hours
        available extensions : 2
    ```

### Allocate a Workspace

To allocate a workspace in one of the listed filesystems, use `ws_allocate`. It is necessary to
specify a unique name and the duration (in days) of the workspace.

```console
ws_allocate: [options] workspace_name duration

Options:
  -h [ --help]               produce help message
  -V [ --version ]           show version
  -d [ --duration ] arg (=1) duration in days
  -n [ --name ] arg          workspace name
  -F [ --filesystem ] arg    filesystem
  -r [ --reminder ] arg      reminder to be sent n days before expiration
  -m [ --mailaddress ] arg   mailaddress to send reminder to (works only with tu-dresden.de mails)
  -x [ --extension ]         extend workspace
  -u [ --username ] arg      username
  -g [ --group ]             group workspace
  -c [ --comment ] arg       comment
```

!!! Note "Name of a workspace"

    The workspace name should help you to remember the experiment and data stored here. It has to
    be unique on a certain filesystem. On the other hand it is possible to use the very same name
    for workspaces on different filesystems.

=== "Simple allocation"

    The simple way to allocate a workspace is calling `ws_allocate` command with two arguments,
    where the first specifies the workspace name and the second the duration. This allocates a
    workspace on the default filesystem with no e-mail reminder.

    ```console
    marie@login$ ws_allocate test-workspace 90
    Info: creating workspace.
    /data/horse/ws/marie-test-workspace
    remaining extensions  : 10
    remaining time in days: 90
    ```

=== "Specific filesystem"

    In order to allocate a workspace on a non-default filesystem, the option
    `--filesystem <filesystem>` is required.

    ```console
    marie@login$ ws_allocate --filesystem walrus test-workspace 99
    Info: creating workspace.
    /data/walrus/ws/marie-test-workspace
    remaining extensions  : 2
    remaining time in days: 99
    ```

=== "with e-mail reminder"

    This command will create a workspace with the name `test-workspace` on the `/horse` filesystem
    (default)
    with a duration of 99 days and send an e-mail reminder. The e-mail reminder will be sent every
    day starting 7 days prior to expiration. We strongly recommend setting this e-mail reminder.

    ```console
    marie@login$ ws_allocate --reminder 7 --mailaddress marie@tu-dresden.de test-workspace 99
    Info: creating workspace.
    /data/horse/ws/marie-test-workspace
    remaining extensions  : 10
    remaining time in days: 99
    ```

Please refer to the [section Cooperative Usage](#cooperative-usage-group-workspaces) for
group workspaces.

### Extension of a Workspace

The lifetime of a workspace is finite and different filesystems (storage systems) have different
maximum durations. The life time of a workspace can be adjusted multiple times, depending on the
filesystem. You can find the concrete values in the
[section settings for workspaces](#workspace-lifetimes).

Use the command `ws_extend [-F filesystem] workspace days` to extend your workspace:

```console
marie@login$ ws_extend -F scratch test-workspace 100
Info: extending workspace.
/data/horse/ws/marie-test-workspace
remaining extensions  : 1
remaining time in days: 100
```

E-mail reminder settings are retained. I.e., previously set e-mail alerts apply to the extended
workspace, too.

!!! attention

    With the `ws_extend` command, a new duration for the workspace is set. The new duration is not
    added to the remaining lifetime!

This means when you extend a workspace that expires in 90 days with the command

```console
marie@login$ ws_extend -F scratch test-workspace 40
```

it will now expire in 40 days, **not** in 130 days!

### Send Reminder for Workspace Expiration Date

We strongly recommend using one of the two provided ways to ensure that the expiration date of a
workspace is not forgotten.

#### Send Daily Reminder

An e-mail reminder can be set at workspace allocation using

```console
ws_allocate --reminder <N> --mailaddress <your.email>@tu-dresden.de [...]
```

This will send an e-mail every day starting `N` days prior to the expiration date.
See the [example above](#allocate-a-workspace) for reference.

If you missed setting an e-mail reminder at workspace allocation, you can add a reminder later, e.g.

```
# initial allocation
marie@login$ ws_allocate --name test-workspace --duration 17
[...]
# add e-mail reminder
marie@login$ ws_allocate --name test-workspace --duration 17 --reminder 7 --mailaddress <your.email>@tu-dresden.de
--extension
```

This will reallocate the workspace, which counts against your maximum number of reallocations (Note:
No data is deleted, but the database entry is modified).

#### Send Calendar Invitation

The command `ws_send_ical` sends you an ical event on the expiration date of a specified workspace.
This calendar invitation can be further managed according to your personal preferences.
The syntax is as follows:

```console
ws_send_ical [--filesystem <filesystem>] --mail <mail address> --workspace <workspace name>
```

E.g.

```console
ws_send_ical --filesystem horse --mail <your.email>@tu-dresden.de --workspace test-workspace
```

### Deletion of a Workspace

There is an [Expire process](#expire-process) for every workspace filesystem running on a daily
basis. These processes check the lifetime of all workspaces and move expired workspaces into the
grace period.

In addition to this automatic process, you also have the option of **explicitly releasing
workspaces** using the `ws_release` command. It is mandatory to specify the name of the
workspace and the filesystem in which it is allocated:

```console
marie@login$ ws_release --filesystem horse --name test-workspace
```

You can list your already released or expired workspaces using the `ws_restore --list` command.

```console
marie@login$ ws_restore --list
horse:
marie-test-workspace-1701873807
    unavailable since Wed Dec  6 15:43:27 2023
walrus:
marie-numbercrunch-1701873907
    unavailable since Wed Dec  6 15:45:07 2023
```

In this example, the user `marie` has two inactive, i.e., expired, workspaces namely
`test-workspace` in `horse`, as well as `numbercrunch` in the `walrus` filesystem. The command
`ws_restore --list` lists the name of the workspace and its expiration date. As you can see, the
expiration date in Unix timestamp format is added to the workspace name.

!!! hint "Deleting data in an expired workspace"

    If you are short on quota, you might want to delete data in expired workspaces since it counts
    to your quota. Expired workspaces are moved to a hidden directory named `.removed`. The access
    rights remain unchanged. I.e., you can delete the data inside the workspace directory but you
    must not delete the workspace directory itself!

!!! warning

    When you release a workspace **manually**, it will not receive a grace period and be
    **permanently deleted** the **next day**. The advantage of this design is that you can create
    and release workspaces inside jobs and not pollute the filesystem with data no one needs anymore
    in the hidden directories (when workspaces are in the grace period).

#### Expire Process

The clean up process of expired workspaces is automatically handled by a so-called expirer process.
It performs the following steps once per day and filesystem:

- Check for remaining life time of all workspaces.
    - If the workspaces expired, move it to a hidden directory so that it becomes inactive.
- Send reminder e-mails to users if the reminder functionality was configured for their particular
  workspaces.
- Scan through all workspaces in grace period.
    - If a workspace exceeded the grace period, the workspace and its data are permanently deleted.

### Restoring Expired Workspaces

At expiration time your workspace will be moved to a special, hidden directory. For a month,
you can still restore your data **into an existing workspace** using the command `ws_restore`.

The expired workspace has to be specified by its full name as listed by `ws_restore --list`,
including username prefix and timestamp suffix. Otherwise, it cannot be uniquely identified. The
target workspace, on the other hand, must be given with just its short name, as listed by `ws_list`,
without the username prefix.

Both workspaces must be on the **very same filesystem**. The data from the old workspace will be
moved into a directory in the new workspace with the name of the old one. This means a newly
allocated workspace works as well as a workspace that already contains data.

!!! note "Steps for restoring a workspace"

    1. Use `ws_restore --list` to list all your expired workspaces and get the correct identifier
       string for the expired workspace. The identifier string of an expired and an active workspace
       are different!
    1. (Optional) Allocate a new workspace on the very same filesystem using `ws_allocate`.
    1. Then, you can invoke `ws_restore <workspace_name> <target_name>` to restore the expired
       workspace into the active workspace.

??? example "Restore workspace `number_crunch` into new workspace `long_computations`"

    This example depictes the necessary steps to restore the expired workspace `number_crunch` into
    a newly allocated workspace named `long_computations.`

    **First step**: List expired workspaces and retrieve correct identifier for the expired workspace.
    In this example, `marie` has two expired workspaces, namely `test-workspace` and `number_crunch`
    both in the `horse` filesystem. The identifier for the restore command is
    `marie-number_crunch-1701873907`.

    ```console
    marie@login$ ws_restore --list
    horse:
    marie-test-workspace-1701873807
        unavailable since Wed Dec  6 15:43:27 2023
    marie-number_crunch-1701873907
        unavailable since Wed Dec  6 15:45:07 2023
    walrus:
    ```

    **Second step:** Allocate new workspace `long_compuations` on the very same filesystem. Please
    refer to the documentation of the [`ws_allocate` command](#allocate-a-workspace) for
    additional useful options.

    ```console
    marie@login$ ws_allocate --filesystem horse --name long_computations --duration 60
    ```

    **Third step:** Invoke the command `ws_restore`.

    ```console
    marie@login$ ws_restore --filesystem horse marie-number_crunch-1701873907 long_computations
    to verify that you are human, please type 'menunesarowo': menunesarowo
    you are human
    Info: restore successful, database entry removed.
    ```

## Linking Workspaces in HOME

It might be valuable to have links to personal workspaces within a certain directory, e.g., your
home directory. The command `ws_register DIR` will create and manage links to all personal
workspaces within in the directory `DIR`. Calling this command will do the following:

- The directory `DIR` will be created if necessary.
- Links to all personal workspaces will be managed:
    - Create links to all available workspaces if not already present.
    - Remove links to released workspaces.

!!! hint "Automatic update of links"

    An automatic update of the workspace links can be invoked by putting the command
    `ws_register DIR` in your personal `shell` configuration file (e.g., `.bashrc`).

    When the filesystems are slow or even down, the command `ws_register` in your `.bashrc` can hang
    preventing you from logging in to ZIH systems.
    In order to make it failsafe, we recommend using `ws_register` in combination with a timeout.

    ```bash
    ANSRED=$'\e[41;1m'
    ANSRESET=$'\e[0m'
    buf="$(timeout 10s ws_register $HOME/workspaces)"
    if test $? -eq 0; then
        echo "${buf}"
    else
        echo "${ANSRED} ws_register: timeout after 10 seconds.${ANSRESET}\n"
    fi
    ```

## How to use Workspaces

There are three typical options for the use of workspaces:

### Per-Job Storage

The idea of a "workspace per-job storage" addresses the need of a batch job for a directory for
temporary data which can be deleted afterwards. To help you to write your own
[(Slurm) job file](../jobs_and_resources/slurm.md#job-files), suited to your needs, we came up with
the following example (which works [for the program g16](../software/nanoscale_simulations.md)).

!!! hint

    Please do not blind copy the example, but rather take the essential idea and concept and adjust
    it to your needs and workflow, e.g.

    * adopt Slurm options for ressource specification,
    * insert the path to your input file,
    * specify what software you want to [load](../software/modules.md),
    * and call the actual software to do your computation.

!!! example "Using temporary workspaces for I/O intensive tasks"

    ```bash
    #!/bin/bash

    #SBATCH --time=48:00:00
    #SBATCH --nodes=1
    #SBATCH --ntasks=1
    ## The optional constraint for the filesystem feature depends
    ## on the filesystem on which you want to use a workspace.
    ## Documentation here https://compendium.hpc.tu-dresden.de/jobs_and_resources/slurm/#filesystem-features
    #SBATCH --constraint=local_disk
    #SBATCH --cpus-per-task=24

    # Load the software you need here
    module purge
    module load <modules>

    # The path to where your input file is located
    INPUTFILE="/path/to/my/inputfile.data"
    test ! -f "${INPUTFILE}" && echo "Error: Could not find the input file ${INPUTFILE}" && exit 1

    # The workspace where results from multiple expirements will be saved for later analysis
    RESULT_WSDIR="/path/to/workspace-experiments-results"
    test -z "${RESULT_WSDIR}" && echo "Error: Cannot find workspace ${RESULT_WSDIR}" && exit 1

    # Allocate workspace for this job. Adjust time span to time limit of the job (--duration).
    WSNAME=computation_$SLURM_JOB_ID
    export WSDDIR=$(ws_allocate --filesystem horse --name ${WSNAME} --duration 2)
    echo ${WSDIR}

    # Check allocation
    test -z "${WSDIR}" && echo "Error: Cannot allocate workspace ${WSDIR}" && exit 1

    # Change to workspace directory
    cd ${WSDIR}

    # Adjust the following line to invoke the program you want to run
    srun <application> < "${INPUTFILE}" > logfile.log

    # Move result and log files of interest to directory named 'results'. This directory and its
    # content will be saved in another storage location for later analysis. All files and
    # directories will be deleted right away at the end of this job file.
    mkdir results
    cp <results and log files> results/

    # Save result files in a general workspace (RESULT_WSDIR, s.a.) holding results from several
    # experiments.
    # Compress results with bzip2 (which includes CRC32 Checksums).
    bzip2 --compress --stdout -4 "${WSDIR}/results" > ${RESULT_WSDIR}/gaussian_job-${SLURM_JOB_ID}.bz2
    RETURN_CODE=$?
    COMPRESSION_SUCCESS="$(if test ${RETURN_CODE} -eq 0; then echo 'TRUE'; else echo 'FALSE'; fi)"

    # Clean up workspace
    if [ "TRUE" = ${COMPRESSION_SUCCESS} ]; then
        test -d ${WSDIR} && rm -rf ${WSDIR}/*
        # Reduces grace period to 1 day!
        ws_release -F horse ${WSNAME}
    else
        echo "Error with compression and writing of results"
        echo "Please check the folder \"${WSDIR}\" for any partial(?) results."
        exit 1
    fi
    ```

### Data for a Campaign

For a series of jobs or calculations that work on the same data, you should allocate a workspace
once, e.g., in `horse` for 100 days:

```console
marie@login$ ws_allocate --filesystem horse my_scratchdata 100
Info: creating workspace.
/data/horse/ws/marie-my_scratchdata
remaining extensions  : 10
remaining time in days: 99
```

You can grant your project group access rights:

```console
marie@login$ chmod g+wrx /data/horse/ws/marie-my_scratchdata
```

And verify it with:

```console
marie@login$ ls -la /data/horse/ws/marie-my_scratchdata
total 8
drwxrwx--- 2 marie    hpcsupport 4096 Jul 10 09:03 .
drwxr-xr-x 5 operator adm        4096 Jul 10 09:01 ..
```

### Mid-Term Storage

<!-- TODO: to be confirmed - is walrus really intended for this purpose? -->
For data that rarely changes but consumes a lot of space, the `walrus` filesystem can be used. Note
that this is mounted read-only on the compute nodes, so you cannot use it as a work directory for
your jobs!

```console
marie@login$ ws_allocate --filesystem walrus my_inputdata 100
/data/walrus/ws/marie-my_inputdata
remaining extensions  : 2
remaining time in days: 100
```

<!-- TODO to be confirmed for walrus / warm_archive replacement
!!!Attention

    The warm archive is not built for billions of files. There
    is a quota for 100.000 files per group. Please archive data.
-->

<!-- TODO command not found - not available yet for walrus?!
To see your active quota use

```console
marie@login$ qinfo quota /data/walrus/ws/
```

Note that the workspaces reside under the mountpoint `/warm_archive/ws/` and not `/warm_archive`
anymore.
-->

## Cooperative Usage (Group Workspaces)

When a workspace is created with the option `-g, --group`, it gets a group workspace that is visible
to others (if in the same group) via `ws_list -g`.

!!! hint "Choose group"

    If you are member of multiple groups, then the group workspace is visible for your primary
    group. You can list all groups you belong to via `groups`, and the first entry is your
    primary group.

    Nevertheless, you can create a group workspace for any of your groups following these two
    steps:

    1. Change to the desired group using `newgrp <other-group>`.
    1. Create the group workspace as usual, i.e., `ws_allocate --group [...]`

    The [page on Sharing Data](data_sharing.md) provides
    information on how to grant access to certain colleagues and whole project groups.

!!! Example "Allocate and list group workspaces"

    If Marie wants to share results and scripts in a workspace with all of her colleagues
    in the project `p_number_crunch`, she can allocate a so-called group workspace.

    ```console
    marie@login$ ws_allocate --group --name numbercrunch --duration 30
    Info: creating workspace.
    /data/horse/ws/marie-numbercrunch
    remaining extensions  : 10
    remaining time in days: 30
    ```

    This workspace directory is readable for the group, e.g.,

    ```console
    marie@login$ ls -ld /data/horse/ws/marie-numbercrunch
    drwxr-x--- 2 marie p_number_crunch 4096 Mar  2 15:24 /data/horse/ws/marie-numbercrunch
    ```

    All members of the project group `p_number_crunch` can now list this workspace using
    `ws_list -g` and access the data (read-only).

    ```console
    martin@login$ ws_list -g -t
    id: numbercrunch
         workspace directory  : /data/horse/ws/marie-numbercrunch
         remaining time       : 29 days 23 hours
         available extensions : 10
    ```

## FAQ and Troubleshooting

**Q**: I am getting the error `Error: could not create workspace directory!`

**A**: Please check the "locale" setting of your SSH client. Some clients (e.g. the one from Mac)
set values that are not valid on our ZIH systems. You should overwrite `LC_CTYPE` and set it to a
valid locale value like `export LC_CTYPE=de_DE.UTF-8`.

A list of valid locales can be retrieved via `locale -a`.

Please use `language_CountryCode.UTF-8` (or plain) settings. Avoid "iso" codepages!

Examples:

| Language | Code |
| -------- | ---- |
| Chinese - Simplified | zh_CN.UTF-8 |
| English | en_US.UTF-8 |
| French | fr_FR.UTF-8 |
| German | de_DE.UTF-8 |

----

**Q**: I am getting the error `Error: target workspace does not exist!` when trying to restore my
workspace.

**A**: The workspace you want to restore into is either not on the same filesystem or you used the
wrong name. Use only the short name that is listed after `id:` when using `ws_list`.
See section [restoring expired workspaces](#restoring-expired-workspaces).

----

**Q**: I forgot to specify an e-mail reminder when allocating my workspace. How can I add the
e-mail alert functionality to an existing workspace?

**A**: You can add the e-mail alert by "overwriting" the workspace settings via

```console
marie@login$ ws_allocate --extension --mailaddress <mail address> --reminder <days> \
             --name <workspace-name> --duration <duration> --filesystem <filesystem>
```

E.g.

```console
marie@login$ ws_allocate --extension --mailaddress <your.email>@tu-dresden.de --reminder 7 \
             --name numbercrunch --duration 20 --filesystem horse
```

This will lower the remaining extensions by one.


# Transfer Data Inside ZIH Systems with Datamover

With the **Datamover**, we provide special data transfer machines for transferring data between
the ZIH filesystems with best transfer speed. The Datamover machine is not accessible
through SSH as it is dedicated to data transfers. To move or copy files from one filesystem to
another, you have to use the following commands after logging in to any of the ZIH HPC systems:

- `dtcp`, `dtls`, `dtmv`, `dtrm`, `dtrsync`, `dttar`, and `dtwget`

These special commands submit a [batch job](../jobs_and_resources/slurm.md) to the data transfer
machines performing the selected command. Their syntax and behavior is the very same as the
well-known shell commands without the prefix *`dt`*, except for the following options.

| Additional Option   | Description                                                                   |
|---------------------|-------------------------------------------------------------------------------|
| `--account=ACCOUNT` | Assign data transfer job to specified account.                                |
| `--blocking       ` | Do not return until the data transfer job is complete. (default for `dtls`)   |
| `--time=TIME      ` | Job time limit (default: 18 h).                                               |

## Managing Transfer Jobs

There are the commands `dtinfo`, `dtqueue`, `dtq`, and `dtcancel` to manage your transfer commands
and jobs.

* `dtinfo` shows information about the nodes of the data transfer machine (like `sinfo`).
* `dtqueue` and `dtq` show all your data transfer jobs (like `squeue --me`).
* `dtcancel` signals data transfer jobs (like `scancel`).

To identify the mount points of the different filesystems on the data transfer machine, use
`dtinfo`. It shows an output like this:

| Directory on Datamover | Mounting Clusters |                       Directory on Cluster |
|:-----------            |:---------         |:--------                                   |
| `/home`              |         Alpha,Barnard,Capella,Julia,Power9,Romeo  |      `/home`         |
| `/projects`          |         Alpha,Barnard,Capella,Julia,Power9,Romeo  |      `/projects`     |
| `/data/horse`        |         Alpha,Barnard,Capella,Julia,Power9,Romeo  |      `/data/horse`   |
| `/data/walrus`       |         Alpha,Barnard,Capella,Julia,Power9        |      `/data/walrus`  |
| `/data/octopus`      |         Alpha,Barnard,Capella,Power9,Romeo        |      `/data/octopus` |
| `/data/cat`          |         Capella                                   |      `/data/cat`     |
| `/data/archiv`       |                                           |                      |

## Usage of Datamover

!!! example "Copy data from `/data/horse` to `/projects` filesystem."

    ```console
    marie@login$ dtcp -r /data/horse/ws/marie-workdata/results /projects/p_number_crunch/.
    ```

!!! example "Move data from `/data/horse` to `/data/walrus` filesystem."

    ```console
    marie@login$ dtmv /data/horse/ws/marie-workdata/results /data/walrus/ws/marie-archive/.
    ```

!!! example "Archive data from `/data/walrus` to `/archiv` filesystem."

    ```console
    marie@login$ dttar -czf /archiv/p_number_crunch/results.tgz /data/walrus/ws/marie-workdata/results
    ```

!!! warning

    Do not generate files in the `/archiv` filesystem much larger that 500 GB!

!!! note

    The `projects` filesystem is not writable from within batch jobs.
    However, you can store the data in the [`walrus` filesystem](../data_lifecycle/working.md)
    using the Datamover nodes via `dt*` commands.

## Transferring Files Between ZIH Systems and Group Drive

In order to let the datamover have access to your group drive, copy your public SSH key from ZIH
system to `login1.zih.tu-dresden.de`, first.

   ```console
   marie@login$ ssh-copy-id -i ~/.ssh/id_rsa.pub login1.zih.tu-dresden.de
   # Export the name of your group drive for reuse of example commands
   marie@login$ export GROUP_DRIVE_NAME=<my-drive-name>
   ```

!!! example "Copy data from your group drive to `/data/horse` filesystem."

    ```console
    marie@login$ dtrsync -av dgw.zih.tu-dresden.de:/glw/${GROUP_DRIVE_NAME}/inputfile /data/horse/ws/marie-workdata/.
    ```

!!! example "Copy data from `/data/horse` filesystem to your group drive."

    ```console
    marie@login$ dtrsync -av /data/horse/ws/marie-workdata/resultfile dgw.zih.tu-dresden.de:/glw/${GROUP_DRIVE_NAME}/.
    ```


# Transfer Data to/from ZIH Systems via Dataport Nodes

To copy large data to/from ZIH systems, the so-called **dataport nodes** should be used. While it is
possible to transfer small files directly via the login nodes, they are not intended to be used that
way. Furthermore, longer transfers will hit the CPU time limit on the login nodes, i.e. the process
get killed. The **dataport nodes** have a better uplink (10 GBit/s) allowing for higher bandwidth.
Note that you cannot log in via SSH to the dataport nodes, but only use
`scp`, `rsync` or `sftp` (incl. FTP-clients like e.g.
[FileZilla](https://filezilla-project.org/)) on them.

The dataport nodes are reachable under the hostnames

- `dataport1.hpc.tu-dresden.de` (IP: 141.30.73.4)
- `dataport2.hpc.tu-dresden.de` (IP: 141.30.73.5)

Through the usage of these dataport nodes, you can bring your data to ZIH HPC systems or get data
from there - they have access to the different HPC
[filesystems](../data_lifecycle/file_systems.md#recommendations-for-filesystem-usage).
Please keep in mind that the different filesystems differ in capacity, IO-performance, and intended
use cases. Choose the one that matches your needs.

The following directories are accessible:

- `/home`
- `/projects`
- `/data/horse`
- `/data/walrus`
- `/data/archiv`

## Access From Linux

There are at least three tools to exchange data between your local workstation and ZIH systems. They
are explained in the following section in more detail.

!!! important "Premise: SSH configuration"

    The following explanations require that you have already set up your
    [SSH configuration](../access/ssh_login.md#configuring-default-parameters-for-ssh).

### SCP

The tool [`scp`](https://www.man7.org/linux/man-pages/man1/scp.1.html)
(OpenSSH secure file copy) copies files between hosts on a network. To copy all files
in a directory, the option `-r` has to be specified.

??? example "Example: Copy a file from your workstation to ZIH systems"

    ```bash
    marie@local$ scp <file> dataport:<target-location>

    # Add -r to copy whole directory
    marie@local$ scp -r <directory> dataport:<target-location>
    ```

    For example, if you want to copy your data file `mydata.csv` to the directory `input` in your
    home directory, you would use the following:

    ```console
    marie@local$ scp mydata.csv dataport:input/
    ```

??? example "Example: Copy a file from ZIH systems to your workstation"

    ```bash
    marie@local$ scp dataport:<file> <target-location>

    # Add -r to copy whole directory
    marie@local$ scp -r dataport:<directory> <target-location>
    ```

    For example, if you have a directory named `output` in your home directory on ZIH systems and
    you want to copy it to the directory `/tmp` on your workstation, you would use the following:

    ```console
    marie@local$ scp -r dataport:output /tmp
    ```

### SFTP

The tool [`sftp`](https://man7.org/linux/man-pages/man1/sftp.1.html) (OpenSSH secure file transfer)
is a file transfer program, which performs all operations over an encrypted SSH transport. It may
use compression to increase performance.

`sftp` is basically a virtual command line, which you could access and exit as follows.

!!! warning "Note"
    It is important from which point in your directory tree you 'enter' `sftp`!
    The current working directory (double ckeck with `pwd`) will be the target folder on your local
    machine from/to which remote files from the ZIH system will be put/get by `sftp`.
    The local folder might also be changed during a session with special commands.
    During the `sftp` session, you can use regular commands like `ls`, `cd`, `pwd` etc.
    But if you wish to access your local workstation, these must be prefixed with the letter `l`
    (`l`ocal), e.g., `lls`, `lcd` or `lpwd`.

```console
# Enter virtual command line
marie@local$ sftp dataport
# Exit virtual command line
sftp> exit
# or
sftp> <Ctrl+D>
```

??? example "Example: Copy a file from your workstation to ZIH systems"

    ```console
    marie@local$ cd my/local/work
    marie@local$ sftp dataport
    # Copy file
    sftp> put <file>
    # Copy directory
    sftp> put -r <directory>
    ```

??? example "Example: Copy a file from ZIH systems to your local workstation"

    ```console
    marie@local$ sftp dataport
    # Copy file
    sftp> get <file>
    # change local (target) directory
    sftp> lcd /my/local/work
    # Copy directory
    sftp> get -r <directory>
    ```

### Rsync

[`Rsync`](https://man7.org/linux/man-pages/man1/rsync.1.html), is a fast and extraordinarily
versatile file copying tool. It can copy locally, to/from another host over any remote shell, or
to/from a remote `rsync` daemon. It is famous for its delta-transfer algorithm, which reduces the
amount of data sent over the network by sending only the differences between the source files and
the existing files in the destination.

Type following commands in the terminal when you are in the directory of
the local machine.

??? example "Example: Copy a file from your workstation to ZIH systems"

    ```console
    # Copy file
    marie@local$ rsync <file> dataport:<target-location>
    # Copy directory
    marie@local$ rsync -r <directory> dataport:<target-location>
    ```

??? example "Example: Copy a file from ZIH systems to your local workstation"

    ```console
    # Copy file
    marie@local$ rsync dataport:<file> <target-location>
    # Copy directory
    marie@local$ rsync -r dataport:<directory> <target-location>
    ```

## Access From Windows

### Command Line

Windows 10 (1809 and higher) comes with a
[built-in OpenSSH support](https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_overview)
including the above described <!--[SCP](#scp) and -->[SFTP](#sftp).

### GUI - Using WinSCP

First you have to install [WinSCP](http://winscp.net/eng/download.php).

Then you have to execute the WinSCP application and configure some
option as described below.

<!-- screenshots will have to be updated-->
![Login - WinSCP](misc/WinSCP_001_new.PNG)
{: align="center"}

![Save session as site](misc/WinSCP_002_new.PNG)
{: align="center"}

![Login - WinSCP click Login](misc/WinSCP_003_new.PNG)
{: align="center"}

![Enter password and click OK](misc/WinSCP_004_new.PNG)
{: align="center"}

After your connection succeeded, you can copy files from your local workstation to ZIH systems and
the other way around.

![WinSCP document explorer](misc/WinSCP_005_new.PNG)
{: align="center"}


# Transfer Data between ZIH Systems and Object Storage (S3)

Object Storage is an alternative to normal filesystem storage. It can be accessed via HTTPS and can
therefor be used where direct `scp` connections are prohibited, e.g. when one wants to copy data to
another data center. Access is provided on request via the corresponding
[self service page on object storage](https://selfservice.tu-dresden.de/services/objectstorage/).
The access key (`Zugriffsschlüssel`) and the secret key (`Geheimer Schlüssel`) are required later
when copying data to it.

Access to object storage is possible on ZIH systems via the module `rclone`:

```console
marie@login$ module load rclone
```

## Initial Configuration

Before you use `rclone` for the first time, you have to configure it. This is done interactively as
shown below. Replace `REPLACE_ME_WITH_ACCESS_KEY` and `REPLACE_ME_WITH_SECRET_KEY` with the values
from the self service portal:

```console
marie@login$ rclone config
2023/03/22 09:35:55 NOTICE: Config file "/home/marie/.config/rclone/rclone.conf" not found - using defaults
No remotes found, make a new one?
n) New remote
s) Set configuration password
q) Quit config
n/s/q> n

Enter name for new remote.
name> s3store

Option Storage.
Type of storage to configure.
Choose a number from below, or type in your own value.
 1 / 1Fichier
   \ (fichier)
 2 / Akamai NetStorage
   \ (netstorage)
 3 / Alias for an existing remote
   \ (alias)
 4 / Amazon Drive
   \ (amazon cloud drive)
 5 / Amazon S3 Compliant Storage Providers including AWS, Alibaba, Ceph, China Mobile, Cloudflare, ArvanCloud, Digital Ocean, Dreamhost, Huawei OBS, IBM COS, IDrive e2, IONOS Cloud, Lyve Cloud, Minio, Netease, RackCorp, Scaleway, SeaweedFS, StackPath, Storj, Tencent COS, Qiniu and Wasabi
   \ (s3)
 6 / Backblaze B2
   \ (b2)
 7 / Better checksums for other remotes
   \ (hasher)
 8 / Box
   \ (box)
 9 / Cache a remote
   \ (cache)
10 / Citrix Sharefile
   \ (sharefile)
11 / Combine several remotes into one
   \ (combine)
12 / Compress a remote
   \ (compress)
13 / Dropbox
   \ (dropbox)
14 / Encrypt/Decrypt a remote
   \ (crypt)
15 / Enterprise File Fabric
   \ (filefabric)
16 / FTP
   \ (ftp)
17 / Google Cloud Storage (this is not Google Drive)
   \ (google cloud storage)
18 / Google Drive
   \ (drive)
19 / Google Photos
   \ (google photos)
20 / HTTP
   \ (http)
21 / Hadoop distributed file system
   \ (hdfs)
22 / HiDrive
   \ (hidrive)
23 / In memory object storage system.
   \ (memory)
24 / Internet Archive
   \ (internetarchive)
25 / Jottacloud
   \ (jottacloud)
26 / Koofr, Digi Storage and other Koofr-compatible storage providers
   \ (koofr)
27 / Local Disk
   \ (local)
28 / Mail.ru Cloud
   \ (mailru)
29 / Mega
   \ (mega)
30 / Microsoft Azure Blob Storage
   \ (azureblob)
31 / Microsoft OneDrive
   \ (onedrive)
32 / OpenDrive
   \ (opendrive)
33 / OpenStack Swift (Rackspace Cloud Files, Memset Memstore, OVH)
   \ (swift)
34 / Oracle Cloud Infrastructure Object Storage
   \ (oracleobjectstorage)
35 / Pcloud
   \ (pcloud)
36 / Put.io
   \ (putio)
37 / QingCloud Object Storage
   \ (qingstor)
38 / SMB / CIFS
   \ (smb)
39 / SSH/SFTP
   \ (sftp)
40 / Sia Decentralized Cloud
   \ (sia)
41 / Storj Decentralized Cloud Storage
   \ (storj)
42 / Sugarsync
   \ (sugarsync)
43 / Transparently chunk/split large files
   \ (chunker)
44 / Union merges the contents of several upstream fs
   \ (union)
45 / Uptobox
   \ (uptobox)
46 / WebDAV
   \ (webdav)
47 / Yandex Disk
   \ (yandex)
48 / Zoho
   \ (zoho)
49 / premiumize.me
   \ (premiumizeme)
50 / seafile
   \ (seafile)
Storage> 5

Option provider.
Choose your S3 provider.
Choose a number from below, or type in your own value.
Press Enter to leave empty.
 1 / Amazon Web Services (AWS) S3
   \ (AWS)
 2 / Alibaba Cloud Object Storage System (OSS) formerly Aliyun
   \ (Alibaba)
 3 / Ceph Object Storage
   \ (Ceph)
 4 / China Mobile Ecloud Elastic Object Storage (EOS)
   \ (ChinaMobile)
 5 / Cloudflare R2 Storage
   \ (Cloudflare)
 6 / Arvan Cloud Object Storage (AOS)
   \ (ArvanCloud)
 7 / Digital Ocean Spaces
   \ (DigitalOcean)
 8 / Dreamhost DreamObjects
   \ (Dreamhost)
 9 / Huawei Object Storage Service
   \ (HuaweiOBS)
10 / IBM COS S3
   \ (IBMCOS)
11 / IDrive e2
   \ (IDrive)
12 / IONOS Cloud
   \ (IONOS)
13 / Seagate Lyve Cloud
   \ (LyveCloud)
14 / Minio Object Storage
   \ (Minio)
15 / Netease Object Storage (NOS)
   \ (Netease)
16 / RackCorp Object Storage
   \ (RackCorp)
17 / Scaleway Object Storage
   \ (Scaleway)
18 / SeaweedFS S3
   \ (SeaweedFS)
19 / StackPath Object Storage
   \ (StackPath)
20 / Storj (S3 Compatible Gateway)
   \ (Storj)
21 / Tencent Cloud Object Storage (COS)
   \ (TencentCOS)
22 / Wasabi Object Storage
   \ (Wasabi)
23 / Qiniu Object Storage (Kodo)
   \ (Qiniu)
24 / Any other S3 compatible provider
   \ (Other)
provider> 24

Option env_auth.
Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).
Only applies if access_key_id and secret_access_key is blank.
Choose a number from below, or type in your own boolean value (true or false).
Press Enter for the default (false).
 1 / Enter AWS credentials in the next step.
   \ (false)
 2 / Get AWS credentials from the environment (env vars or IAM).
   \ (true)
env_auth> 1

Option access_key_id.
AWS Access Key ID.
Leave blank for anonymous access or runtime credentials.
Enter a value. Press Enter to leave empty.
access_key_id> REPLACE_ME_WITH_ACCESS_KEY

Option secret_access_key.
AWS Secret Access Key (password).
Leave blank for anonymous access or runtime credentials.
Enter a value. Press Enter to leave empty.
secret_access_key> REPLACE_ME_WITH_SECRET_KEY

Option region.
Region to connect to.
Leave blank if you are using an S3 clone and you don't have a region.
Choose a number from below, or type in your own value.
Press Enter to leave empty.
   / Use this if unsure.
 1 | Will use v4 signatures and an empty region.
   \ ()
   / Use this only if v4 signatures don't work.
 2 | E.g. pre Jewel/v10 CEPH.
   \ (other-v2-signature)
region> 1

Option endpoint.
Endpoint for S3 API.
Required when using an S3 clone.
Enter a value. Press Enter to leave empty.
endpoint> s3.zih.tu-dresden.de

Option location_constraint.
Location constraint - must be set to match the Region.
Leave blank if not sure. Used when creating buckets only.
Enter a value. Press Enter to leave empty.
location_constraint> 

Option acl.
Canned ACL used when creating buckets and storing or copying objects.
This ACL is used for creating objects and if bucket_acl isn't set, for creating buckets too.
For more info visit https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl
Note that this ACL is applied when server-side copying objects as S3
doesn't copy the ACL from the source but rather writes a fresh one.
Choose a number from below, or type in your own value.
Press Enter to leave empty.
   / Owner gets FULL_CONTROL.
 1 | No one else has access rights (default).
   \ (private)
   / Owner gets FULL_CONTROL.
 2 | The AllUsers group gets READ access.
   \ (public-read)
   / Owner gets FULL_CONTROL.
 3 | The AllUsers group gets READ and WRITE access.
   | Granting this on a bucket is generally not recommended.
   \ (public-read-write)
   / Owner gets FULL_CONTROL.
 4 | The AuthenticatedUsers group gets READ access.
   \ (authenticated-read)
   / Object owner gets FULL_CONTROL.
 5 | Bucket owner gets READ access.
   | If you specify this canned ACL when creating a bucket, Amazon S3 ignores it.
   \ (bucket-owner-read)
   / Both the object owner and the bucket owner get FULL_CONTROL over the object.
 6 | If you specify this canned ACL when creating a bucket, Amazon S3 ignores it.
   \ (bucket-owner-full-control)
acl> 1

Edit advanced config?
y) Yes
n) No (default)
y/n> n
```

## Copying Data from/to Object Storage

The following commands show how to create a bucket `mystorage` in your part of the object store:

```console
marie@login$ module load rclone
marie@login$ rclone mkdir s3store:mystorage
```

After these commands, you can copy a file, e. g. `largedata.tar.gz`, to it in a separate job with
the help of the [Datamover](datamover.md). Adjust the parameters `time` and `account` as required:

```console
marie@login$ dtrclone --time=0:10:00 --account=p_number_crunch copy --s3-acl "public-read" largedata.tar.gz s3store:mystorage
```

!!! warning "Restricted access"

    If you want to restrict access to your data, replace the last command with:

    ```console
    marie@login$ dtrclone --time=0:10:00 --account=p_number_crunch copy largedata.tar.gz s3store:mystorage
    ```

    Then, it is not possible to access your data without providing your credentials.

For small files, you can also directly copy data:

```console
marie@login$ module load rclone
marie@login$ rclone copy --s3-acl "public-read" largedata.tar.gz s3store:mystorage
```

## Accessing the Object Storage

The following commands show different possibilities to access a file from object storage.

### Copying a File from Object Storage to ZIH systems

```console
marie@login$ dtrclone --time=0:10:00 --account=p_number_crunch copy s3store:mystorage/largedata.tar.gz .
```

### Copying a File from Object Storage to Your Workstation

The following command assumes you have installed the command `s3cmd`, please also see the
[s3cmd Installation Instructions](https://tu-dresden.de/zih/dienste/service-katalog/arbeitsumgebung/datenspeicher/objektspeicher-s3)

```console
marie@local$ s3cmd get s3://mystorage/largedata.tar.gz
```

### Accessing a Public-Readable File

It is possible to copy a public-readable file via `wget` or similar command line tools. Replace
`$USER` with your ZIH account.

```console
marie@somewhere$ wget https://s3.zih.tu-dresden.de/$USER:mystorage/largedata.tar.gz
```


# Data Transfer

## Data Transfer to/from ZIH Systems: Dataport Nodes

There are at least three tools for exchanging data between your local workstation and ZIH systems:
`scp`, `rsync`, and `sftp`. Please refer to the offline or online man pages of
[scp](https://www.man7.org/linux/man-pages/man1/scp.1.html),
[rsync](https://man7.org/linux/man-pages/man1/rsync.1.html), and
[sftp](https://man7.org/linux/man-pages/man1/sftp.1.html) for detailed information.

No matter what tool you prefer, it is crucial that the **dataport nodes** are used as preferred way
to copy data to/from ZIH systems. Please follow the link to the documentation on [dataport
nodes](dataport_nodes.md) for further reference and examples.

## Data Transfer Inside ZIH Systems: Datamover

The recommended way for data transfer inside ZIH Systems is the **Datamover**. It is a special data
transfer machine that provides the best transfer speed. To load, move, copy etc. files from one
filesystem to another filesystem, you have to use commands prefixed with `dt`: `dtcp`, `dtwget`,
`dtmv`, `dtrm`, `dtrsync`, `dttar`, `dtls`. These commands submit a job to the data transfer
machines that execute the selected command.  Please refer to the detailed documentation regarding
the [Datamover](datamover.md).


# GPU Cluster Alpha Centauri

## Overview

The multi-GPU cluster `Alpha Centauri` has been installed for AI-related computations (ScaDS.AI).

## Hardware Specification

The hardware specification is documented on the page
[HPC Resources](hardware_overview.md#alpha-centauri).

## Filesystems

Since 5th July 2024, `Alpha Centauri` is fully integrated in the InfiniBand infrastructure of
`Barnard`. With that, all [filesystems](../data_lifecycle/file_systems.md)
(`/home`, `/software`, `/data/horse`, `/data/walrus`, etc.) are available.

## Usage

!!! note

    The NVIDIA A100 GPUs may only be used with **CUDA 11** or later. Earlier versions do not
    recognize the new hardware properly. Make sure the software you are using is built with CUDA11.

There is a total of 48 physical cores in each node. SMT is also active, so in total, 96 logical
cores are available per node.
Each node on the cluster `Alpha` has 2x AMD EPYC CPUs, 8x NVIDIA
A100-SXM4 GPUs, 1 TB RAM and 3.5 TB local space (`/tmp`) on an NVMe device.

!!! note

    Multithreading is disabled per default in a job.
    See the [Slurm page](slurm.md) on how to enable it.

### Modules

The easiest way is using the [module system](../software/modules.md).
All software available from the module system has been specifically build for the cluster `Alpha`
i.e., with optimization for Zen2 microarchitecture and CUDA-support enabled.

To check the available modules for `Alpha`, use the command

```console
marie@login.alpha$ module spider <module_name>
```

??? example "Example: Searching and loading PyTorch"

    For example, to check which `PyTorch` versions are available you can invoke

    ```console
    marie@login.alpha$ module spider PyTorch
    -------------------------------------------------------------------------------------------------------------------------
      PyTorch:
    -------------------------------------------------------------------------------------------------------------------------
        Description:
          Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework
          that puts Python first.

         Versions:
            PyTorch/1.12.0
            PyTorch/1.12.1-CUDA-11.7.0
            PyTorch/1.12.1
    [...]
    ```

    Not all modules can be loaded directly. Most modules are build with a certain compiler or
    toolchain that need to be loaded beforehand. Luckely, the module system can tell us, what we
    need to do for a specific module or software version

    ```console
    marie@login.alpha$ module spider PyTorch/1.12.1-CUDA-11.7.0

    -------------------------------------------------------------------------------------------------------------------------
      PyTorch: PyTorch/1.12.1-CUDA-11.7.0
    -------------------------------------------------------------------------------------------------------------------------
        Description:
          Tensors and Dynamic neural networks in Python with strong GPU acceleration. PyTorch is a deep learning framework
          that puts Python first.


        You will need to load all module(s) on any one of the lines below before the "PyTorch/1.12.1" module is available to load.

          release/23.04  GCC/11.3.0  OpenMPI/4.1.4
    [...]
    ```

    Finaly, the commandline to load the `PyTorch/1.12.1-CUDA-11.7.0` module is

    ```console
    marie@login.alpha$ module load release/23.04  GCC/11.3.0  OpenMPI/4.1.4 PyTorch/1.12.1-CUDA-11.7.0
    Module GCC/11.3.0, OpenMPI/4.1.4, PyTorch/1.12.1-CUDA-11.7.0 and 64 dependencies loaded.
    ```

    Now, you can verify with the following command that the pytorch module is available

    ```console
    marie@login.alpha$ python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"
    1.12.1
    True
    ```

### Python Virtual Environments

[Virtual environments](../software/python_virtual_environments.md) allow you to install
additional Python packages and create an isolated runtime environment. We recommend using
`virtualenv` for this purpose.

!!! hint

    We recommend to use [workspaces](../data_lifecycle/workspaces.md) for your virtual environments.

??? example "Example: Creating a virtual environment and installing `torchvision` package"

    As first step, you should create a workspace

    ```console
    marie@login.alpha$ srun --nodes=1 --cpus-per-task=1 --gres=gpu:1 --time=01:00:00 --pty bash -l
    marie@alpha$ ws_allocate --name python_virtual_environment --duration 1
    Info: creating workspace.
    /horse/ws/marie-python_virtual_environment
    remaining extensions  : 2
    remaining time in days: 1
    ```

    Now, you can load the desired modules and create a virtual environment within the allocated
    workspace.

    ```
    marie@alpha$ module load release/23.04 GCCcore/11.3.0 GCC/11.3.0 OpenMPI/4.1.4 Python/3.10.4
    Module GCC/11.3.0, OpenMPI/4.1.4, Python/3.10.4 and 21 dependencies loaded.
    marie@alpha$ module load PyTorch/1.12.1-CUDA-11.7.0
    Module PyTorch/1.12.1-CUDA-11.7.0 and 42 dependencies loaded.
    marie@alpha$ which python
    /software/rome/r23.04/Python/3.10.4-GCCcore-11.3.0/bin/python
    marie@alpha$ pip list
    [...]
    marie@alpha$ virtualenv --system-site-packages /data/horse/ws/marie-python_virtual_environment/my-torch-env
    created virtual environment CPython3.8.6.final.0-64 in 42960ms
      creator CPython3Posix(dest=/horse/.global1/ws/marie-python_virtual_environment/my-torch-env, clear=False, global=True)
      seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=~/.local/share/virtualenv)
        added seed packages: pip==21.1.3, setuptools==57.2.0, wheel==0.36.2
      activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator
    marie@alpha$ source /data/horse/ws/marie-python_virtual_environment/my-torch-env/bin/activate
    (my-torch-env) marie@alpha$ pip install torchvision==0.13.1
    [...]
    Installing collected packages: torchvision
    Successfully installed torchvision-0.13.1
    [...]
    (my-torch-env) marie@alpha$ python -c "import torchvision; print(torchvision.__version__)"
    0.13.1+cu102
    (my-torch-env) marie@alpha$ deactivate
    ```

### JupyterHub

[JupyterHub](../access/jupyterhub.md) can be used to run Jupyter notebooks on Alpha Centauri
cluster. You can either use the
[standard profiles for Alpha](../access/jupyterhub.md#standard-profiles) or use the advanced form
and define the resources for your JupyterHub job. The "Alpha GPU (NVIDIA Ampere A100)" preset
is a good starting configuration.

### Containers

Singularity containers enable users to have full control of their software environment.
For more information, see the [Singularity container details](../software/containers.md).

Nvidia
[NGC](https://developer.nvidia.com/blog/how-to-run-ngc-deep-learning-containers-with-singularity/)
containers can be used as an effective solution for machine learning related tasks. (Downloading
containers requires registration). Nvidia-prepared containers with software solutions for specific
scientific problems can simplify the deployment of deep learning workloads on HPC. NGC containers
have shown consistent performance compared to directly run code.


# NVIDIA Arm HPC Developer Kit

As part of the ZIH systems, we provide a NVIDIA Arm HPC Developer Kit to allow for experimentation
with Arm based systems.

## Hardware

This Arm HPC Developer kit offers:

* GIGABYTE G242-P32, 2U server
* 1x Ampere Altra Q80-30 (Arm processor)
* 512G DDR4 memory (8x 64G)
* 6TB SAS/ SATA 3.5″
* 2x NVIDIA A100 GPU
* 2x NVIDIA BlueField-2 E-Series DPU: 200GbE/HDR single-port, both connected to the InfiniBand network

## Further Information

Further information about this new system can be found on the following websites:

* [NVIDIA product page](https://developer.nvidia.com/arm-hpc-devkit)
* [link collection curated by NVIDIA](https://github.com/arm-hpc-devkit/nvidia-arm-hpc-devkit-users-guide)

## Getting Access

To get access to the developer kit, write a mail to
[the hpcsupport team](mailto:hpc-support@tu-dresden.de)
with your ZIH login and a short description, what you want to use the developer kit for.

After you have gained access, you can log into the developer kit system via SSH from the login
nodes:

```console
marie@login$ ssh a1.misc.hpc.tu-dresden.de
```

## Running Applications

!!! warning "Not under Slurm control"

    In contrast to all other compute resources, the ARM HPC Developer Kit is **not** managed by the
    [Slurm batch system](../jobs_and_resources/slurm.md). To run your application just execute it.

    For long running applications, we recommend using a session manager, for example
    [tmux](../software/utilities.md#tmux).

!!! warning "No shared filesystem available"

    This is a test system. For this reason the shared filesystems (e.g. Lustre or BeeGFS) are not
    available.

The system supports the Arm v8.2+ architecture. Therefore, your application needs to be compiled
for the target architecture `aarch64` which is the 64-bit execution state of Arm v8. You can either
compile your application on the Developer Kit or cross compile for `aarch64` on another system.

### Cross compiling for the Arm Architecture

A compiler supporting the Arm architecture `aarch64` is required for cross compilation. You could
for example use the GCC compiler for `aarch64`. Most Linux distributions provide the compiler in
their package repositories, often the package is called `gcc-aarch64-linux-gnu`.

!!! note "No cross compiler available on ZIH systems"

    On the ZIH systems is no cross compiler available. If you can't cross compile on your own
    systems, compile your application on the Arm Developer Kit using the provided compiler, which
    already builds for the `aarch64` target.

To cross compile your application run the compiler for the `aarch64` architecture instead of the
compiler you normally use.

```console
# Instead of gcc
marie@local$ aarch64-linux-gnu-gcc -o application application.c

# When using make
marie@local$ make CC=aarch64-linux-gnu-gcc
```


# Binding and Distribution of Tasks

Slurm provides several binding strategies to place and bind the tasks and/or threads of your job
to cores, sockets and nodes.

!!! note

    Keep in mind that the distribution method might have a direct impact on the execution time of
    your application. The manipulation of the distribution can either speed up or slow down your
    application.

## General

To specify a pattern the commands `--cpu_bind=<cores|sockets>` and `--distribution=<block|cyclic>`
are needed. The option `cpu_bind` defines the resolution in which the tasks will be allocated. While
`--distribution` determinate the order in which the tasks will be allocated to the CPUs. Keep in
mind that the allocation pattern also depends on your specification.

!!! example "Explicitly specify binding and distribution"

    ```bash
    #!/bin/bash
    #SBATCH --nodes=2                        # request 2 nodes
    #SBATCH --cpus-per-task=4                # use 4 cores per task
    #SBATCH --tasks-per-node=4               # allocate 4 tasks per node - 2 per socket

    srun --ntasks 8 --cpus-per-task 4 --cpu_bind=cores --distribution=block:block ./application
    ```

In the following sections there are some selected examples of the combinations between `--cpu_bind`
and `--distribution` for different job types.

## OpenMP Strategies

The illustration below shows the default binding of a pure OpenMP job on a single node with 16 CPUs
on which 16 threads are allocated.

![OpenMP](misc/openmp.png)
{: align=center}

!!! example "Default binding and default distribution"

    ```bash
    #!/bin/bash
    #SBATCH --nodes=1
    #SBATCH --tasks-per-node=1
    #SBATCH --cpus-per-task=16

    export OMP_NUM_THREADS=16

    srun --ntasks 1 --cpus-per-task $OMP_NUM_THREADS ./application
    ```

## MPI Strategies

### Default Binding and Distribution Pattern

The default binding uses `--cpu_bind=cores` in combination with `--distribution=block:cyclic`. The
default (as well as `block:cyclic`) allocation method will fill up one node after another, while
filling socket one and two in alternation. Resulting in only even ranks on the first socket of each
node and odd on each second socket of each node.

![Default distribution](misc/mpi_default.png)
{: align="center"}

!!! example "Default binding and default distribution"

    ```bash
    #!/bin/bash
    #SBATCH --nodes=2
    #SBATCH --tasks-per-node=16
    #SBATCH --cpus-per-task=1

    srun --ntasks 32 ./application
    ```

### Core Bound

!!! note

    With this command the tasks will be bound to a core for the entire runtime of your
    application.

#### Distribution: block:block

This method allocates the tasks linearly to the cores.

![block:block distribution](misc/mpi_block_block.png)
{: align="center"}

!!! example "Binding to cores and block:block distribution"

    ```bash
    #!/bin/bash
    #SBATCH --nodes=2
    #SBATCH --tasks-per-node=16
    #SBATCH --cpus-per-task=1

    srun --ntasks 32 --cpu_bind=cores --distribution=block:block ./application
    ```

#### Distribution: cyclic:cyclic

`--distribution=cyclic:cyclic` will allocate your tasks to the cores in a
round robin approach. It starts with the first socket of the first node,
then the first socket of the second node until one task is placed on
every first socket of every node. After that it will place a task on
every second socket of every node and so on.

![cyclic:cyclic distribution](misc/mpi_cyclic_cyclic.png)
{: align="center"}

!!! example "Binding to cores and cyclic:cyclic distribution"

    ```bash
    #!/bin/bash
    #SBATCH --nodes=2
    #SBATCH --tasks-per-node=16
    #SBATCH --cpus-per-task=1

    srun --ntasks 32 --cpu_bind=cores --distribution=cyclic:cyclic ./application
    ```

#### Distribution: cyclic:block

The cyclic:block distribution will allocate the tasks of your job in
alternation on node level, starting with first node filling the sockets
linearly.

![cyclic:block distribution](misc/mpi_cyclic_block.png)
{: align="center"}

!!! example "Binding to cores and cyclic:block distribution"

    ```bash
    #!/bin/bash
    #SBATCH --nodes=2
    #SBATCH --tasks-per-node=16
    #SBATCH --cpus-per-task=1

    srun --ntasks 32 --cpu_bind=cores --distribution=cyclic:block ./application
    ```

### Socket Bound

The general distribution onto the nodes and sockets stays the same. The mayor difference between
socket- and CPU-bound lies within the ability of the OS to move tasks from one core to another
inside a socket while executing the application. These jumps can slow down the execution time of
your application.

#### Default Distribution

The default distribution uses `--cpu_bind=sockets` with `--distribution=block:cyclic`. The default
allocation method (as well as `block:cyclic`) will fill up one node after another, while filling
socket one and two in alternation. Resulting in only even ranks on the first socket of each node and
odd on each second socket of each node.

![Binding to sockets and block:cyclic distribution](misc/mpi_socket_block_cyclic.png)
{: align="center"}

!!! example "Binding to sockets and block:cyclic distribution"

    ```bash
    #!/bin/bash
    #SBATCH --nodes=2
    #SBATCH --tasks-per-node=16
    #SBATCH --cpus-per-task=1

    srun --ntasks 32 -cpu_bind=sockets ./application
    ```

#### Distribution: block:block

This method allocates the tasks linearly to the cores.

![Binding to sockets and block:block distribution](misc/mpi_socket_block_block.png)
{: align="center"}

!!! example "Binding to sockets and block:block distribution"

    ```bash
    #!/bin/bash
    #SBATCH --nodes=2
    #SBATCH --tasks-per-node=16
    #SBATCH --cpus-per-task=1

    srun --ntasks 32 --cpu_bind=sockets --distribution=block:block ./application
    ```

#### Distribution: block:cyclic

The `block:cyclic` distribution will allocate the tasks of your job in
alternation between the first node and the second node while filling the
sockets linearly.

![Binding to sockets and block:cyclic distribution](misc/mpi_socket_block_cyclic.png)
{: align="center"}

!!! example "Binding to sockets and block:cyclic distribution"

    ```bash
    #!/bin/bash
    #SBATCH --nodes=2
    #SBATCh --tasks-per-node=16
    #SBATCH --cpus-per-task=1

    srun --ntasks 32 --cpu_bind=sockets --distribution=block:cyclic ./application
    ```

## Hybrid Strategies

### Default Binding and Distribution Pattern

The default binding pattern of hybrid jobs will split the cores allocated to a rank between the
sockets of a node. The example shows that Rank 0 has 4 cores at its disposal. Two of them on first
socket inside the first node and two on the second socket inside the first node.

![sockets binding and block:block distribution](misc/hybrid.png)
{: align="center"}

!!! example "Binding to sockets and block:block distribution"

    ```bash
    #!/bin/bash
    #SBATCH --nodes=2
    #SBATCH --tasks-per-node=4
    #SBATCH --cpus-per-task=4

    export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
    srun --ntasks 8 --cpus-per-task $OMP_NUM_THREADS ./application
    ```

### Core Bound

#### Distribution: block:block

This method allocates the tasks linearly to the cores.

![Binding to cores and block:block distribution](misc/hybrid_cores_block_block.png)
{: align="center"}

!!! example "Binding to cores and block:block distribution"

    ```bash
    #!/bin/bash
    #SBATCH --nodes=2
    #SBATCH --tasks-per-node=4
    #SBATCH --cpus-per-task=4

    export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
    srun --ntasks 8 --cpus-per-task $OMP_NUM_THREADS --cpu_bind=cores --distribution=block:block ./application
    ```

#### Distribution: cyclic:block

The `cyclic:block` distribution will allocate the tasks of your job in alternation between the first
node and the second node while filling the sockets linearly.

![binding to cores and cyclic:block distribution](misc/hybrid_cores_cyclic_block.png)
{: align="center"}

!!! example "Binding to cores and cyclic:block distribution"

    ```bash
    #!/bin/bash
    #SBATCH --nodes=2
    #SBATCH --tasks-per-node=4
    #SBATCH --cpus-per-task=4

    export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
    srun --ntasks 8 --cpus-per-task $OMP_NUM_THREADS --cpu_bind=cores --distribution=cyclic:block ./application
    ```

## GPU

Currently with the Slurm version (20.11.9) used ZIH systems
it **is not possible** to bind tasks to GPUs. Is will be possible as soon as Slurm is updated at
least to version 21.08.0 (see [GRES/MIG documentation in Slurm 21.08.0](https://slurm.schedmd.com/archive/slurm-21.08.0/gres.html#MIG_Management)).


# GPU Cluster Capella

## Overview

The Lenovo multi-GPU cluster `Capella` has been installed by MEGWARE for
AI-related computations and traditional
HPC simulations. Capella is fully integrated into the ZIH HPC infrastructure.
Therefore, the usage should be similar to the other clusters.

In November 2024, Capella was ranked #51 in the [TOP500](https://top500.org/system/180298/),
which is #3 of German
systems, and #5 in the [GREEN500](https://top500.org/lists/green500/list/2024/11/) lists of the
world's fastest computers. Background information on how Capella reached these positions can be
found in this
[Golem article](https://www.golem.de/news/effiziente-grossrechner-wie-man-einen-supercomputer-in-die-green500-bekommt-2411-190925.html).

## Hardware Specifications

The hardware specification is documented on the page
[HPC Resources](hardware_overview.md#capella).

## Access and Login Nodes

You use `login[1-2].capella.hpc.tu-dresden.de` to access the cluster `Capella` from the campus
(or VPN) network.
In order to verify the SSH fingerprints of the login nodes, please refer to the page
[Key Fingerprints](../access/key_fingerprints.md#capella).

On the login nodes you have access to the same filesystems and the software stack
as on the compute node. GPUs are **not** available there.

In the subsections [Filesystems](#filesystems) and [Software and Modules](#software-and-modules) we
provide further information on these two topics.

## Filesystems

As with all other clusters, your `/home` directory is also available on `Capella`.
For reasons of convenience, the filesystems `horse` and `walrus` are also accessible.
Please note, that the filesystem `horse` **should not be used** as working
filesystem at the cluster `Capella` because we have something better.

### Cluster-Specific Filesystem `cat`

With `Capella` comes the new filesystem `cat` designed to meet the high I/O requirements of AI
and ML workflows. It is a WEKAio filesystem and mounted under `/data/cat`. It is **only available**
on the cluster `Capella` and the [Datamover nodes](../data_transfer/datamover.md).

The filesystem `cat` should be used as the
main working filesystem and has to be used with [workspaces](../data_lifecycle/file_systems.md).
Workspaces on the filesystem `cat` can only be created on the login and compute nodes, not on
the other clusters since `cat` is not available there.

`cat` has only limited capacity, hence workspace duration is significantly shorter than
in other filesystems. We recommend that you only store actively used data there.
To transfer input and result data from and to the filesystems `horse` and `walrus`, respectively,
you will need to use the [Datamover nodes](../data_transfer/datamover.md). Regardless of the
direction of transfer, you should pack your data into archives (,e.g., using `dttar` command)
for the transfer.

**Do not** invoke data transfer to the filesystems `horse` and `walrus` from login nodes.
Both login nodes are part of the cluster. Failures, reboots and other work
might affect your data transfer resulting in data corruption.

All other share [filesystems](../data_lifecycle/workspaces.md)
(`/home`, `/software`, `/data/horse`, `/data/walrus`, etc.) are also mounted.

## Software and Modules

The most straightforward method for utilizing the software is through the well-known
[module system](../software/modules.md).
All software available from the module system has been **specifically build** for the cluster
`Capella` i.e., with optimization for Zen4 (Genoa) microarchitecture and CUDA-support enabled.

### Python Virtual Environments

[Virtual environments](../software/python_virtual_environments.md) allow you to install
additional Python packages and create an isolated runtime environment. We recommend using
`venv` for this purpose.

!!! hint "Virtual environments in workspaces"

    We recommend to use [workspaces](../data_lifecycle/workspaces.md) for your virtual environments.

## Batch System

The batch system Slurm may be used as usual. Please refer to the page [Batch System Slurm](slurm.md)
for detailed information. In addition, the page [Job Examples with GPU](slurm_examples_with_gpu.md)
provides examples on GPU allocation with Slurm.

You can find out about upcoming reservations (,e.g., for acceptance benchmarks) via `sinfo -T`.
Acceptance has priority, so your reservation requests can currently not be considered.

!!! note "Slurm limits and job runtime"

    Although, each compute node is equipped with 64 CPU cores in total, only a **maximum of 56** can
    be requested via Slurm
    (cf. [Slurm Resource Limits Table](slurm_limits.md#slurm-resource-limits-table)).

    The **maximum runtime** of jobs and interactive sessions is currently 24 hours. However, to
    allow for greater fluctuation in testing, please make the jobs shorter if possible. You can use
    [Chain Jobs](slurm_examples.md#chain-jobs) to split a long running job exceeding the batch queues
    limits into parts and chain these parts. Applications with build-in check-point-restart
    functionality are very suitable for this approach! If your application provides
    check-point-restart, please use `/data/cat` for temporary data. Remove these data afterwards!

The partition `capella-interactive` can be used for your small tests and compilation of software.
In addition, JupyterHub instances that require low GPU utilization or only use GPUs for a short
period of time in their allocation are intended to use this partition.
You need to add `#SBATCH --partition=capella-interactive` to your job file and
`--partition=capella-interactive` to your `sbatch`, `srun` and `salloc` command line, respectively,
to address this partition.
The partition `capella-interactive` is configured to use [MIG](#virtual-gpus-mig) configuration of 1/7.

## Virtual GPUs-MIG

Starting with the Capella cluster, we introduce virtual GPUs. They are based on
[Nvidia's MIG technology](https://www.nvidia.com/de-de/technologies/multi-instance-gpu/).
From an application point of view, each virtual GPU looks like a normal physical GPU, but offers
only a fraction of the compute resources and the maximum allocatable memory on the device.
We also only account you a fraction of a full GPU hour.
By using virtual GPUs, we expect to improve overall system utilization for jobs that cannot take
advantage of a full H100 GPU.
In addition, we can provide you with more resources and therefore shorter waiting times.
We intend to use these partitions for all applications that cannot use a full H100 GPU, such as
Jupyter-Notebooks.
Users can check the usage of compute and memory usage of the GPU with the help of
[job monitoring system PIKA](../software/performance_engineering_overview.md#pika).
Since a GPU in the `Capella` cluster offers 3.2-3.5x more peak performance compared to an A100 GPU
in the cluster [`Alpha Centauri`](hardware_overview.md#alpha-centauri), a 1/7 shard of a GPU in
Capella is about half the performance of a GPU in `Alpha Centauri`.

At the moment we only have a partitioning of 7 in the `capella-interactive` partition,
but we are free to create more configurations in the future.
For this, users' demands and expected high utilization of the smaller GPUS are essential.

| Configuration Name      | Compute Resources   | Memory in GiB | Accounted GPU hour  |
| ------------------------| --------------------| ------------- |---------------------|
| `capella-interactive`   |  1 / 7              |  11           | 1/7 |


# Checkpoint/Restart

At some point, every HPC system fails, e.g., a compute node or the network might crash causing
running jobs to crash, too. In order to prevent starting your crashed experiments and simulations
from the very beginning, you should be familiar with the concept of checkpointing.

!!! note

    Checkpointing saves the state of a running process to a checkpointing image file. Using this
    file, the process can later be continued (restarted) from where it left off.

Another motivation is to use checkpoint/restart to split long running jobs into several shorter
ones. This might improve the overall job throughput, since shorter jobs can "fill holes" in the job
queue.
Here is an extreme example from literature for the waste of large computing resources due to missing
checkpoints:

!!! cite "Adams, D. The Hitchhikers Guide Through the Galaxy"

    Earth was a supercomputer constructed to find the question to the answer to the Life, the Universe,
    and Everything by a race of hyper-intelligent pan-dimensional beings. Unfortunately 10 million years
    later, and five minutes before the program had run to completion, the Earth was destroyed by
    Vogons.

If you wish to do checkpointing, your first step should always be to check if your application
already has such capabilities built-in, as that is the most stable and safe way of doing it.
Applications that are known to have some sort of **native checkpointing** include:

Abaqus, Amber, Gaussian, GROMACS, LAMMPS, NAMD, NWChem, Quantum Espresso, STAR-CCM+, VASP

In case your program does not natively support checkpointing, there are attempts at creating generic
checkpoint/restart solutions that should work application-agnostic. One such project which we
recommend is [Distributed Multi-Threaded Check-Pointing](http://dmtcp.sourceforge.net) (DMTCP).

DMTCP is available on ZIH systems after having loaded the `dmtcp` module

```console
marie@login$ module load DMTCP
```

While our batch system [Slurm](slurm.md) also provides a checkpointing interface to the user,
unfortunately, it does not yet support DMTCP at this time. However, there are ongoing efforts of
writing a Slurm plugin that hopefully will change this in the near future. We will update this
documentation as soon as it becomes available.

In order to help with setting up checkpointing for your jobs, we have written a few scripts that
make it easier to utilize DMTCP together with Slurm.

## Using w.r.t. Chain Jobs

For long-running jobs that you wish to split into multiple shorter jobs
([chain jobs](slurm_examples.md#chain-jobs)), thereby enabling the job scheduler to
fill the cluster much more efficiently and also providing some level of fault-tolerance, we have
written a script that automatically creates a number of jobs for your desired runtime and adds the
checkpoint/restart bits transparently to your batch script. You just have to specify the targeted
total runtime of your calculation and the interval in which you wish to do checkpoints. The latter
(plus the time it takes to write the checkpoint) will then be the runtime of the individual jobs.
This should be targeted at below 24 hours in order to be able to run on all
[partitions haswell64](../jobs_and_resources/slurm_limits.md#slurm-resource-limits-table). For
increased fault-tolerance, it can be chosen even shorter.

To use it, first add a `dmtcp_launch` before your application call in your batch script. In the case
of MPI applications, you have to add the parameters `--ib --rm` and put it between `srun` and your
application call, e.g.:

???+ example "Example my-dmtcp-script.sbatch"

    ```bash
    [...]

    srun dmtcp_launch --ib --rm ./my-mpi-application
    ```

!!! note

    We have successfully tested checkpointing MPI applications with
    the latest `Intel MPI` (module: intelmpi/2018.0.128). While it might
    work with other MPI libraries, too, we have no experience in this
    regard, so you should always try it out before using it for your
    productive jobs.

Then just substitute your usual `sbatch` call with `dmtcp_sbatch` and be sure to specify the `-t`
and `-i` parameters (don't forget you need to have loaded the `dmtcp` module).

```console
marie@login$ dmtcp_sbatch --time 2-00:00:00 --interval 28000,800 my-dmtcp-script.sbatch
```

With `-t, --time` you set the total runtime of your calculations. This will be replaced in the batch
script in order to shorten your individual jobs.

The parameter `-i, --interval` sets the time in seconds for your checkpoint intervals. It can
optionally include a timeout for writing out the checkpoint files, separated from the interval time
via comma (defaults to 10 minutes).

In the above example, there will be 6 jobs each running 8 hours, so
about 2 days in total.

!!! Hints

    - If you see your first job running into the time limit, that probably
    means the timeout for writing out checkpoint files does not suffice
    and should be increased. Our tests have shown that it takes
    approximately 5 minutes to write out the memory content of a fully
    utilized 64GB haswell node, so you should choose at least 10 minutes
    there (better err on the side of caution). Your mileage may vary,
    depending on how much memory your application uses. If your memory
    content is rather incompressible, it might be a good idea to disable
    the checkpoint file compression by setting: `export DMTCP_GZIP=0`
    - Note that all jobs the script deems necessary for your chosen
    time limit/interval values are submitted right when first calling the
    script. If your applications take considerably less time than what
    you specified, some of the individual jobs will be unnecessary. As
    soon as one job does not find a checkpoint to resume from, it will
    cancel all subsequent jobs for you.
    - See `dmtcp_sbatch -h` for a list of available parameters and more help

What happens in your work directory?

- The script will create subdirectories named `ckpt_<jobid>` for each
  individual job it puts into the queue
- It will also create modified versions of your batch script, one for
  the first job (`ckpt_launch.job`), one for the middle parts
  (`ckpt_rstr.job`) and one for the final job (`cpkt_rstr_last.job`)
- Inside the `ckpt_*` directories you will also find a file
  (`job_ids`) containing all job ids that are related to this job
  chain

If you wish to restart manually from one of your checkpoints (e.g., if something went wrong in your
later jobs or the jobs vanished from the queue for some reason), you have to call `dmtcp_sbatch`
with the `-r, --resume` parameter, specifying a `cpkt_` directory to resume from.  Then it will use
the same parameters as in the initial run of this job chain. If you wish to adjust the time limit,
for instance, because you realized that your original limit was too short, just use the `-t, --time`
parameter again on resume.

## Using DMTCP Manually

If for some reason our automatic chain job script is not suitable for your use case, you could also
just use DMTCP on its own. In the following we will give you step-by-step instructions on how to
checkpoint your job manually:

* Load the DMTCP module: `module load dmtcp`
* DMTCP usually runs an additional process that
manages the creation of checkpoints and such, the so-called `coordinator`. It must be started in
your batch script before the actual start of your application. To help you with this process, we
have created a bash function called `start_coordinator` that is available after sourcing
`$DMTCP_ROOT/bin/bash` in your script. The coordinator can take a handful of parameters, see `man
dmtcp_coordinator`. Via `-i` you can specify an interval (in seconds) in which checkpoint files are
to be created automatically. With `--exit-after-ckpt` the application will be terminated after the
first checkpoint has been created, which can be useful if you wish to implement some sort of job
chaining on your own.
* In front of your program call, you have to add the wrapper
script `dmtcp_launch`.  This will create a checkpoint automatically after 40 seconds and then
terminate your application and with it the job. If the job runs into its time limit (here: 60
seconds), the time to write out the checkpoint was probably not long enough. If all went well, you
should find `cpkt` files in your work directory together with a script called
`./dmtcp_restart_script.sh` that can be used to resume from the checkpoint.

???+ example

    ```bash
    #/bin/bash
    #SBATCH --time=00:01:00
    #SBATCH --cpus-per-task=8
    #SBATCH --mem-per-cpu=1500

    source $DMTCP_ROOT/bin/bash start_coordinator -i 40 --exit-after-ckpt

    dmtcp_launch ./my-application #for sequential/multithreaded applications
    #or: srun dmtcp_launch --ib --rm ./my-mpi-application #for MPI
    applications
    ```

* To restart your application, you need another batch file
(similar to the one above) where once again you first have to start the
DMTCP coordinator. The requested resources should match those of your
original job. If you do not wish to create another checkpoint in your
restarted run again, you can omit the `-i` and `--exit-after-ckpt`
parameters this time. Afterwards, the application must be run using the
restart script, specifying the host and port of the coordinator (they
have been exported by the `start_coordinator` function).

???+ example

    ```bash
    #/bin/bash
    #SBATCH --time=00:01:00
    #SBATCH --cpus-per-task=8
    #SBATCH --mem-per-cpu=1500

    source $DMTCP_ROOT/bin/bash start_coordinator -i 40 --exit-after-ckpt

    ./dmtcp_restart_script.sh -h $DMTCP_COORD_HOST -p
    $DMTCP_COORD_PORT
    ```

## Signal Handler

If for some reason your job is taking unexpectedly long and would be killed by Slurm
due to reaching its time limit, you can use `--signal=<sig_num>[@sig_time]` to make
Slurm sent your processes a Unix signal `sig_time` seconds before.
Your application should take care of this signal and can write some checkpoints
or output intermediate results and terminate gracefully.
`sig_num` can be any numeric signal number or name, e.g. `10` and `USR1`. You will find a
comprehensive list of Unix signals including documentation in the
[signal man page](https://man7.org/linux/man-pages/man7/signal.7.html).
`sig_time` has to be an integer value between 0 and 65535 representing seconds
Slurm sends the signal before the time limit is reached. Due to resolution effects
the signal may be sent up to 60 seconds earlier than specified.

The command line

```console
marie@login$ srun --ntasks=1 --time=00:05:00 --signal=USR1@120 ./signal-handler
```

makes Slurm send `./signal-handler` the signal `USR1` 120 seconds before
the time limit is reached. The following example provides a skeleton implementation of a
signal-aware application.

???+ example "Example signal-handler.c"

    ```C hl_lines="15"
    #include <stdio.h>
    #include <stdlib.h>
    #include <signal.h>

    void sigfunc(int sig) {
        if(sig == SIGUSR1) {
            printf("Allocation's time limit reached. Saving checkpoint and exit\n");
            exit(EXIT_SUCCESS);
        }

        return;
    }

    int main(void) {
       signal(SIGUSR1, sigfunc);
       printf("do number crunching\n");
       while(1) {
           ;
       }

       return EXIT_SUCCESS;
    }
    ```


# HPC Resources

HPC resources in ZIH systems comprise the *High Performance Computing and Storage Complex* and its
extension *High Performance Computing – Data Analytics*. In total it offers scientists
about 100,000 CPU cores and a peak performance of more than 1.5 quadrillion floating point
operations per second. The architecture specifically tailored to data-intensive computing, Big Data
analytics, and artificial intelligence methods with extensive capabilities for energy measurement
and performance monitoring provides ideal conditions to achieve the ambitious research goals of the
users and the ZIH.

## Architectural Design

Over the last decade we have been running our HPC system of high heterogeneity with a single
Slurm batch system. This made things very complicated, especially to inexperienced users. With
the replacement of the Taurus system by the cluster [Barnard](#barnard) in 2023 we have a new
architectural design comprising **six homogeneous clusters with their own Slurm instances and with
cluster specific login nodes** running on the same CPU. Job submission is possible only from
within the corresponding cluster (compute or login node).

All clusters are integrated to the new InfiniBand fabric and have the same access to
the shared filesystems. You find a comprehensive documentation on the available working and
permanent filesystems on the page [Filesystems](../data_lifecycle/file_systems.md).

![Architecture overview 2023](../jobs_and_resources/misc/architecture_2024.png)
{: align=center}

HPC resources at ZIH comprise a total of **six systems**:

| Name                                | Description           | Year of Installation | DNS |
| ----------------------------------- | ----------------------| -------------------- | --- |
| [`Capella`](#capella)               | GPU cluster           | 2024                 | `c[1-144].capella.hpc.tu-dresden.de` |
| [`Barnard`](#barnard)               | CPU cluster           | 2023                 | `n[1001-1630].barnard.hpc.tu-dresden.de` |
| [`Alpha Centauri`](#alpha-centauri) | GPU cluster           | 2021                 | `i[8001-8037].alpha.hpc.tu-dresden.de` |
| [`Julia`](#julia)                   | Single SMP system     | 2021                 | `julia.hpc.tu-dresden.de` |
| [`Romeo`](#romeo)                   | CPU cluster           | 2020                 | `i[7001-7186].romeo.hpc.tu-dresden.de` |
| [`Power9`](#power9)                 | IBM Power/GPU cluster | 2018                 | `ml[1-29].power9.hpc.tu-dresden.de` |

All clusters will run with their own [Slurm batch system](slurm.md) and job submission is possible
only from their respective login nodes.

## Login and Dataport Nodes

- Login-Nodes
    - Individual for each cluster. See the specifics in each cluster chapter.
- 2 Data-Transfer-Nodes
    - 2 servers without interactive login, only available via file transfer protocols
      (`rsync`, `ftp`)
    - `dataport[3-4].hpc.tu-dresden.de`
    - IPs: 141.30.73.\[4,5\]
    - Further information on the usage is documented on the site
      [Dataport Nodes](../data_transfer/dataport_nodes.md)

## Barnard

The cluster `Barnard` is a general purpose cluster by Bull. It is based on Intel Sapphire Rapids CPUs.

- 630 nodes, each with
    - 2 x Intel Xeon Platinum 8470 (52 cores) @ 2.00 GHz, Multithreading available
    - 512 GB RAM (8 x 32 GB DDR5-4800 MT/s per socket)
    - 12 nodes provide 1.8 TB local storage on NVMe device at `/tmp`
    - All other nodes are diskless and have no or very limited local storage (i.e. `/tmp`)
- Login nodes: `login[1-4].barnard.hpc.tu-dresden.de`
- Hostnames: `n[1001-1630].barnard.hpc.tu-dresden.de`
- Operating system: Red Hat Enterprise Linux 8.9

## Alpha Centauri

The cluster `Alpha Centauri` (short: `Alpha`) by NEC provides AMD Rome CPUs and NVIDIA A100 GPUs
and is designed for AI and ML tasks.

- 37 nodes, each with
    - 8 x NVIDIA A100-SXM4 Tensor Core-GPUs (40 GB HBM2)
    - 2 x AMD EPYC CPU 7352 (24 cores) @ 2.3 GHz, Multithreading available
    - 1 TB RAM (16 x 32 GB DDR4-2933 MT/s per socket)
    - 3.5 TB local storage on NVMe device at `/tmp`
- Login nodes: `login[1-2].alpha.hpc.tu-dresden.de`
- Hostnames: `i[8001-8037].alpha.hpc.tu-dresden.de`
- Operating system: Rocky Linux 8.9
- Further information on the usage is documented on the site [GPU Cluster Alpha Centauri](alpha_centauri.md)

## Capella

The cluster `Capella` by MEGWARE provides AMD Genoa CPUs and NVIDIA H100 GPUs
and is designed for AI and ML tasks.

- 144 nodes, each with
    - 4 x NVIDIA H100-SXM5 Tensor Core-GPUs (94 GB HBM2e)
    - 2 x AMD EPYC CPU 9334 (32 cores) @ 2.7 GHz, Multithreading disabled
    - 768 GB RAM (12 x 32 GB DDR5-4800 MT/s per socket)
    - 800 GB local storage on NVMe device at `/tmp`
- Login nodes: `login[1-2].capella.hpc.tu-dresden.de`
- Hostnames: `c[1-144].capella.hpc.tu-dresden.de`
- Operating system: Alma Linux 9.4
- Offers fractions of full GPUs via [Nvidia's MIG mechanism](capella.md#virtual-gpus-mig)
- Further information on the usage is documented on the site [GPU Cluster Capella](capella.md)

## Romeo

The cluster `Romeo` is a general purpose cluster by NEC based on AMD Rome CPUs.

- 188 nodes, each with
    - 2 x AMD EPYC CPU 7702 (64 cores) @ 2.0 GHz, Multithreading available
    - 512 GB RAM (8 x 32 GB DDR4-3200 MT/s per socket)
    - 200 GB local storage on SSD at `/tmp`
- Login nodes: `login[1-2].romeo.hpc.tu-dresden.de`
- Hostnames: `i[7001-7186].romeo.hpc.tu-dresden.de`
- Operating system: Rocky Linux 8.9
- Further information on the usage is documented on the site [CPU Cluster Romeo](romeo.md)

## Julia

The cluster `Julia` is a large SMP (shared memory parallel) system by HPE based on Superdome Flex
architecture.

- 1 node, with
    - 32 x Intel(R) Xeon(R) Platinum 8276M CPU @ 2.20 GHz (28 cores)
    - 47 TB RAM (12 x 128 GB DDR4-2933 MT/s per socket)
- Configured as one single node
- 48 TB RAM (usable: 47 TB - one TB is used for cache coherence protocols)
- 370 TB of fast NVME storage available at `/nvme/<projectname>`
- Login node: `julia.hpc.tu-dresden.de`
- Hostname: `julia.hpc.tu-dresden.de`
- Operating system: Rocky Linux 8.7
- Further information on the usage is documented on the site [SMP System Julia](julia.md)

## Power9

The cluster `Power9` by IBM is based on Power9 CPUs and provides NVIDIA V100 GPUs.
`Power9` is specifically designed for machine learning (ML) tasks.

- 32 nodes, each with
    - 2 x IBM Power9 CPU (2.80 GHz, 3.10 GHz boost, 22 cores)
    - 256 GB RAM (8 x 16 GB DDR4-2666 MT/s per socket)
    - 6 x NVIDIA V100-SXM2 GPUs (32 GB HBM2)
    - NVLINK bandwidth 150 GB/s between GPUs and host
- Login nodes: `login[1-2].power9.hpc.tu-dresden.de`
- Hostnames: `ml[1-29].power9.hpc.tu-dresden.de`
- Operating system: Alma Linux 8.7
- Further information on the usage is documented on the site [GPU Cluster Power9](power9.md)


# SMP Cluster Julia

## Overview

The HPE Superdome Flex is a large shared memory node. It is especially well suited for data
intensive application scenarios, for example to process extremely large data sets completely in main
memory or in very fast NVMe memory.

## Hardware Resources

The hardware specification is documented on the page
[HPC Resources](hardware_overview.md#julia).

!!! note

    `Julia` has been partitioned at the end of October 2024. A quarter of the hardware ressources
    (CPUs and memory) are now in exclusive operation for the
    [DZA](https://www.deutscheszentrumastrophysik.de/).

## Local Temporary on NVMe Storage

There are 370 TB of NVMe devices installed. For immediate access for all projects, a volume of 87 TB
of fast NVMe storage is available at `/nvme/1/<projectname>`. A quota of
100 GB per project on this NVMe storage is set.

With a more detailed proposal to [hpc-support@tu-dresden.de](mailto:hpc-support@tu-dresden.de)
on how this unique system (large shared memory + NVMe storage) can speed up their computations, a
project's quota can be increased or dedicated volumes of up to the full capacity can be set up.

## Hints for Usage

- Granularity should be a socket (28 cores)
- Can be used for OpenMP applications with large memory demands
- To use Open MPI it is necessary to export the following environment
  variables, so that Open MPI uses shared-memory instead of InfiniBand
  for message transport:

  ```
  export OMPI_MCA_pml=ob1
  export OMPI_MCA_mtl=^mxm
  ```

- Use `I_MPI_FABRICS=shm` so that Intel MPI doesn't even consider
  using InfiniBand devices itself, but only shared-memory instead


# Known Issues with MPI

This pages holds known issues observed with MPI and concrete MPI implementations.

## Open MPI

### Performance Loss with MPI-IO-Module OMPIO

Open MPI v4.1.x introduced a couple of major enhancements, e.g., the `OMPIO` module is now the
default module for MPI-IO on **all** filesystems incl. Lustre (cf.
[NEWS file in Open MPI source code](https://raw.githubusercontent.com/open-mpi/ompi/v4.1.x/NEWS)).
Prior to this, `ROMIO` was the default MPI-IO module for Lustre.

Colleagues of ZIH have found that some MPI-IO access patterns suffer a significant performance loss
using `OMPIO` as MPI-IO module with `OpenMPI/4.1.x` modules on ZIH systems. At the moment, the root
cause is unclear and needs further investigation.

**A workaround** for this performance loss is to use the "old", i.e., `ROMIO` MPI-IO-module. This
is achieved by setting the environment variable `OMPI_MCA_io` before executing the application as
follows

```console
marie@login$ export OMPI_MCA_io=^ompio
marie@login$ srun [...]
```

or setting the option as argument, in case you invoke `mpirun` directly

```console
marie@login$ mpirun --mca io ^ompio [...]
```

### Mpirun on clusters `alpha` and `power9`
<!-- laut max möglich dass es nach dem update von alpha und power9 das problem nicht mehr relevant ist.-->
Using `mpirun` on clusters `alpha` and `power` leads to wrong resource distribution when more than
one node is involved. This yields a strange distribution like e.g. `SLURM_NTASKS_PER_NODE=15,1`
even though `--tasks-per-node=8` was specified. Unless you really know what you're doing (e.g.
use rank pinning via perl script), avoid using mpirun.

Another issue arises when using the Intel toolchain: mpirun calls a different MPI and caused a
8-9x slowdown in the PALM app in comparison to using srun or the GCC-compiled version of the app
(which uses the correct MPI).

### R Parallel Library on Multiple Nodes

Using the R parallel library on MPI clusters has shown problems when using more than a few compute
nodes. The error messages indicate that there are buggy interactions of R/Rmpi/Open MPI and UCX.
Disabling UCX has solved these problems in our experiments.

We invoked the R script successfully with the following command:

```console
marie@login$ mpirun -mca btl_openib_allow_ib true --mca pml ^ucx --mca osc ^ucx -np 1 Rscript --vanilla the-script.R
```

where the arguments `-mca btl_openib_allow_ib true --mca pml ^ucx --mca osc ^ucx` disable usage of
UCX.

### MPI Function `MPI_Win_allocate`

The function `MPI_Win_allocate` is a one-sided MPI call that allocates memory and returns a window
object for RDMA operations (ref. [man page](https://www.open-mpi.org/doc/v3.0/man3/MPI_Win_allocate.3.php)).

> Using MPI_Win_allocate rather than separate MPI_Alloc_mem + MPI_Win_create may allow the MPI
> implementation to optimize the memory allocation. (Using advanced MPI)

It was observed for at least for the `OpenMPI/4.0.5` module that using `MPI_Win_Allocate` instead of
`MPI_Alloc_mem` in conjunction with `MPI_Win_create` leads to segmentation faults in the calling
application. To be precise, the segfaults occurred at partition `romeo` when about 200 GB per node
where allocated. In contrast, the segmentation faults vanished when the implementation was
refactored to call the `MPI_Alloc_mem` + `MPI_Win_create` functions.


# NVMe Storage

90 NVMe storage nodes, each with

-   8x Intel NVMe Datacenter SSD P4610, 3.2 TB
-   3.2 GB/s (8x 3.2 =25.6 GB/s)
-   2 InfiniBand EDR links, Mellanox MT27800, ConnectX-5, PCIe x16, 100
    Gbit/s
-   2 sockets Intel Xeon E5-2620 v4 (16 cores, 2.10GHz)
-   64 GB RAM

NVMe cards can saturate the HCAs

![Configuration](misc/nvme.png)
{: align=center}


# Introduction HPC Resources and Jobs

ZIH operates high performance computing (HPC) systems with about 100.000 cores, 900 GPUs, and a
flexible storage hierarchy with about 40 PB total capacity. The HPC system provides an optimal
research environment especially in the area of data analytics, artificial intelligence methods and
machine learning as well as for processing extremely large data sets. Moreover it is also a perfect
platform for highly scalable, data-intensive and compute-intensive applications and has extensive
capabilities for energy measurement and performance monitoring. Therefore provides ideal conditions
to achieve the ambitious research goals of the users and the ZIH.

The HPC system consists of [six clusters](hardware_overview.md)
with their own
[Slurm](slurm.md) instances and cluster specific
login nodes. The clusters share a number of different
[filesystems](../data_lifecycle/file_systems.md) which enable users to switch between the
components.

## Selection of Suitable Hardware

The six clusters
[`Barnard`](hardware_overview.md#barnard),
[`Alpha Centauri`](hardware_overview.md#alpha-centauri),
[`Capella`](hardware_overview.md#capella),
[`Romeo`](hardware_overview.md#romeo),
[`Power9`](hardware_overview.md#power9) and
[`Julia`](hardware_overview.md#julia)
differ, among others, in number of nodes, cores per node, and GPUs and memory. The particular
[characteristica](hardware_overview.md) qualify them for different applications.

### Which Cluster Do I Need?

The majority of the basic tasks can be executed on the conventional nodes like on `Barnard`. When
log in to ZIH systems, you are placed on a login node where you can execute short tests and compile
moderate projects. The login nodes cannot be used for real experiments and computations. Long and
extensive computational work and experiments have to be encapsulated into so called **jobs** and
scheduled to the compute nodes.

There is no such thing as free lunch at ZIH systems. Since compute nodes are operated in multi-user
node by default, jobs of several users can run at the same time at the very same node sharing
resources, like memory (but not CPU). On the other hand, a higher throughput can be achieved by
smaller jobs. Thus, restrictions w.r.t. [memory](#memory-limits) and
[runtime limits](#runtime-limits) have to be respected when submitting jobs.

The following questions may help to decide which cluster to use

- my application
    - is [interactive or a batch job](slurm.md)?
    - requires [parallelism](#parallel-jobs)?
    - requires [multithreading (SMT)](#multithreading)?
- Do I need [GPUs](#what-do-i-need-a-cpu-or-gpu)?
- How much [run time](#runtime-limits) do I need?
- How many [cores](#how-many-cores-do-i-need) do I need?
- How much [memory](#how-much-memory-do-i-need) do I need?
- Which [software](#available-software) is required?

<!-- cluster_overview_table -->
|Name|Description| DNS | Nodes | # Nodes | Cores per Node | Threads per Core | Memory per Node [in MB] | Memory per Core [in MB] | GPUs per Node
|---|---|----|:---|---:|---:|---:|---:|---:|---:|
|**Capella**<br>_2024_| GPU|`<node>.barnard.hpc.tu-dresden.de` |`c[1-144]` | 144 |64| 1 |768,000 | | 4 |
|**Barnard**<br>_2023_| CPU|`<node>.barnard.hpc.tu-dresden.de` |`n[1001-1630]` | 630 |104| 2 |515,000 |12,000 | 0 |
|**Alpha**<br>_2021_| GPU |`<node>.alpha.hpc.tu-dresden.de`|`i[8001-8037]` |  37 | 48 | 2 | 990,000 | 10,312|  8  |
|**Romeo**<br>_2020_| CPU |`<node>.romeo.hpc.tu-dresden.de`|`i[7001-7186]` | 186|128 | 2 | 505,000| 1,972 |  0  |
|**Julia**<br>_2021_| single SMP system |`julia.hpc.tu-dresden.de`| `julia` | 1 | 896 | 1 | 48,390,000 | 54,006 | - |
|**Power9**<br>_2018_|IBM Power/GPU system |`<node>.power9.hpc.tu-dresden.de`|`ml[1-29]` | 29 | 44 | 4 | 254,000 | 1,443 | 6 |
{: summary="cluster overview table" align="bottom"}

### Interactive or Batch Mode

**Interactive jobs:** An interactive job is the best choice for testing and development. See
 [interactive-jobs](slurm.md).
Slurm can forward your X11 credentials to the first node (or even all) for a job
with the `--x11` option. To use an interactive job you have to specify `-X` flag for the ssh login.

However, using `srun` directly on the Shell will lead to blocking and launch an interactive job.
Apart from short test runs, it is recommended to encapsulate your experiments and computational
tasks into batch jobs and submit them to the batch system. For that, you can conveniently put the
parameters directly into the job file which you can submit using `sbatch [options] <job file>`.

### Parallel Jobs

**MPI jobs:** For MPI jobs typically allocates one core per task. Several nodes could be allocated
if it is necessary. The batch system [Slurm](slurm.md) will automatically find suitable hardware.

**OpenMP jobs:** SMP-parallel applications can only run **within a node**, so it is necessary to
include the [batch system](slurm.md) options `--nodes=1` and `--tasks=1`. Using `--cpus-per-task=N`
Slurm will start one task and you will have `N` CPUs. The maximum number of processors for an
SMP-parallel program is 896 on cluster [`Julia`](julia.md) (be aware that the application has to be
developed with that large number of threads in mind).

Partitions with GPUs are best suited for **repetitive** and **highly-parallel** computing tasks. If
you have a task with potential [data parallelism](../software/gpu_programming.md) most likely that
you need the GPUs.  Beyond video rendering, GPUs excel in tasks such as machine learning, financial
simulations and risk modeling. Use the cluster `power` only if you need GPUs! Otherwise
using the x86-based partitions most likely would be more beneficial.

### Multithreading

Some cluster/nodes have Simultaneous Multithreading (SMT) enabled, e.g [`alpha`](slurm.md) You
request for this additional threads using the Slurm option `--hint=multithread` or by setting the
environment variable `SLURM_HINT=multithread`. Besides the usage of the threads to speed up the
computations, the memory of the other threads is allocated implicitly, too, and you will always get
`Memory per Core`*`number of threads` as memory pledge.

### What do I need, a CPU or GPU?

If an application is designed to run on GPUs this is normally announced unmistakable since the
efforts of adapting an existing software to make use of a GPU can be overwhelming.
And even if the software was listed in
[NVIDIA's list of GPU-Accelerated Applications](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/gpu-applications-catalog.pdf)
only certain parts of the computations may run on the GPU.

To answer the question: The easiest way is to compare a typical computation
on a normal node and on a GPU node. (Make sure to eliminate the influence of different
CPU types and different number of cores.) If the execution time with GPU is better
by a significant factor then this might be the obvious choice.

??? note "Difference in Architecture"

    The main difference between CPU and GPU architecture is that a CPU is designed to handle a wide
    range of tasks quickly, but are limited in the concurrency of tasks that can be running.
    While GPUs can process data much faster than a CPU due to massive parallelism
    (but the amount of data which
    a single GPU's core can handle is small), GPUs are not as versatile as CPUs.

### How much time do I need?

#### Runtime limits

!!! warning "Runtime limits on login nodes"

    There is a time limit of 600 seconds set for processes on login nodes. Each process running
    longer than this time limit is automatically killed. The login nodes are shared ressources
    between all users of ZIH system and thus, need to be available and cannot be used for productive
    runs.

    ```
    CPU time limit exceeded
    ```

    Please submit extensive application runs to the compute nodes using the [batch system](slurm.md).

!!! note "Runtime limits are enforced."

    A job is canceled as soon as it exceeds its requested limit. Currently, the maximum run time
    limit is 7 days.

Shorter jobs come with multiple advantages:

- lower risk of loss of computing time,
- shorter waiting time for scheduling,
- higher job fluctuation; thus, jobs with high priorities may start faster.

To bring down the percentage of long running jobs we restrict the number of cores with jobs longer
than 2 days to approximately 50% and with jobs longer than 24 to 75% of the total number of cores.
(These numbers are subject to change.) As best practice we advise a run time of about 8h.

!!! hint "Please always try to make a good estimation of your needed time limit."

    For this, you can use a command line like this to compare the requested timelimit with the
    elapsed time for your completed jobs that started after a given date:

    ```console
    marie@login$ sacct -X -S 2021-01-01 -E now --format=start,JobID,jobname,elapsed,timelimit -s COMPLETED
    ```

Instead of running one long job, you should split it up into a chain job. Even applications that are
not capable of checkpoint/restart can be adapted. Please refer to the section
[Checkpoint/Restart](../jobs_and_resources/checkpoint_restart.md) for further documentation.

### How many cores do I need?

ZIH systems are focused on data-intensive computing. They are meant to be used for highly
parallelized code. Please take that into account when migrating sequential code from a local machine
to our HPC systems. To estimate your execution time when executing your previously sequential
program in parallel, you can use [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl%27s_law).
Think in advance about the parallelization strategy for your project and how to effectively use HPC
resources.

However, this is highly depending on the used software, investigate if your application supports a
parallel execution.

### How much memory do I need?

#### Memory Limits

!!! note "Memory limits are enforced."

    Jobs which exceed their per-node memory limit are killed automatically by the batch system.

Memory requirements for your job can be specified via the `sbatch/srun` parameters:

`--mem-per-cpu=<MB>` or `--mem=<MB>` (which is "memory per node"). The **default limit** regardless
of the partition it runs on is quite low at **300 MB** per CPU. If you need more memory, you need
to request it.

ZIH systems comprise different sets of nodes with different amount of installed memory which affect
where your job may be run. To achieve the shortest possible waiting time for your jobs, you should
be aware of the limits shown in the
[Slurm resource limits table](../jobs_and_resources/slurm_limits.md#slurm-resource-limits-table).

Follow the page [Slurm](slurm.md) for comprehensive documentation using the batch system at
ZIH systems. There is also a page with extensive set of [Slurm examples](slurm_examples.md).

### Which software is required?

#### Available software

Pre-installed software on our HPC systems is managed via [modules](../software/modules.md).
However, there are many different variants of these modules available. Each cluster has its own set
of installed modules, depending on their purpose.

Specific modules can be found with:

```console
marie@login$ module spider <software_name>
```

## Processing of Data for Input and Output

Pre-processing and post-processing of the data is a crucial part for the majority of data-dependent
projects. The quality of this work influence on the computations. However, pre- and post-processing
in many cases can be done completely or partially on a local system and then transferred to ZIH
systems. Please use ZIH systems primarily for the computation-intensive tasks.

## Exclusive Reservation of Hardware

If you need for some special reasons, e.g., for benchmarking, a project or paper deadline, parts of
our machines exclusively, we offer the opportunity to request and reserve these parts for your
project.

Please send your request **7 working days** before the reservation should start (as that's our
maximum time limit for jobs and it is therefore not guaranteed that resources are available on
shorter notice) with the following information
[via e-mail to the HPC Support](mailto:support@example.com?subject=Exclusive%20Hardware%20Reservation%20Request&body=Dear%20HPC%20support%2C%0A%0AI%20have%20the%20following%20request%20for%20an%20exclusive%20hardware%20reservation%3A%0A%0AProject%3A%0AReservation%20owner%3A%0ACluster%3A%0ANumber%20of%20nodes%3A%0AStart%20time%3A%20YYYY-MM-DDTHH%3AMM%0AEnd%20time%3A%20YYYY-MM-DDTHH%3AMM%0AReason%3A)

- `Project:` *Which project will be credited for the reservation?*
- `Reservation owner:` *Who should be able to run jobs on the
  reservation? I.e., name of an individual user or a group of users
  within the specified project.*
- `Cluster:` *Which cluster should be used?*
- `Number of nodes:` *How many nodes do you need? (The number of GPUs will be scaled accordingly if
    you request on a GPU cluster.)*
- `Start time:` *Start time of the reservation in the form `YYYY-MM-DDTHH:MM`, e.g.,
   2020-05-21T09:00*
- `End time:` *End time of the reservation in the form `YYYY-MM-DDTHH:MM`*
- `Reason:` *Reason for the reservation.*

!!! warning

    Please note that your project CPU hour budget will be credited for the reserved hardware even if
    you don't use it.


# GPU Cluster Power9

## Overview

The multi-GPU cluster `Power9` was installed in 2018. Until the end of 2023, it was available as
partition `power` within the now decommissioned `Taurus` system. With the decommission of `Taurus`,
`Power9` has been re-engineered and is now a homogeneous, standalone cluster with own
[Slurm batch system](slurm.md) and own login nodes.

## Hardware Resources

The hardware specification of the cluster `Power9` is documented on the page
[HPC Resources](hardware_overview.md#power9).

We provide additional architectural information in the following.
The compute nodes of the cluster `Power9` are built on the base of
[Power9 architecture](https://www.ibm.com/it-infrastructure/power/power9) from IBM.
The system was created for AI challenges, analytics and working with data-intensive workloads and
accelerated databases.

The main feature of the nodes is the ability to work with the
[NVIDIA Tesla V100](https://www.nvidia.com/en-gb/data-center/tesla-v100/) GPU with **NV-Link**
support that allows a total bandwidth with up to 300 GB/s. Each node on the
cluster `Power9` has six Tesla V100 GPUs. You can find a detailed specification of the cluster in our
[Power9 documentation](../jobs_and_resources/hardware_overview.md#power9).

!!! note

    The cluster `Power9` is based on the PPC64 architecture, which means that the software built
    for x86_64 will not work on this cluster.

## Usage

### Containers

If you want to use containers on `Power9`, please refer to the page
[Singularity for Power9 Architecture](../software/singularity_power9.md).

### Power AI

There are tools provided by IBM, that work on cluster `Power9` and are related to AI tasks.
For more information see our [Power AI documentation](../software/power_ai.md).


# CPU Cluster Romeo

## Overview

The HPC system `Romeo` is a general purpose cluster based on AMD Rome CPUs. From 2019 till the end
of 2023, it was available as partition `romeo` within `Taurus`. With the decommission of `Taurus`,
`Romeo` has been re-engineered and is now a homogeneous, standalone cluster with own
[Slurm batch system](slurm.md) and own login nodes.

## Hardware Resources

The hardware specification is documented on the page
[HPC Resources](hardware_overview.md#romeo).

## Usage

There is a total of 128 physical cores in each node. SMT is also active, so in total, 256 logical
cores are available per node.

!!! note

    Multithreading is disabled per default in a job. To make use of it include the Slurm parameter
    `--hint=multithread` in your job script or command line, or set the environment variable
    `SLURM_HINT=multithread` before job submission.

Each node brings 512 GB of main memory, so you can request roughly 1972 MB per logical core (using
`--mem-per-cpu`). Note that you will always get the memory for the logical core sibling too, even if
you do not intend to use SMT.

!!! note

    If you are running a job here with only ONE process (maybe multiple cores), please explicitly
    set the option `-n 1`!

Be aware that software built with Intel compilers and `-x*` optimization flags will not run on those
AMD processors! That's why most older modules built with Intel toolchains are not available on
partition `romeo`.

We provide the script `ml_arch_avail` that can be used to check if a certain module is available on
`rome` architecture.

## Example, running CP2K on Rome

First, check what CP2K modules are available in general:
`module spider CP2K` or `module avail CP2K`.

You will see that there are several different CP2K versions avail, built with different toolchains.
Now let's assume you have to decided you want to run CP2K version 6 at least, so to check if those
modules are built for rome, use:

```console
marie@login$ ml_arch_avail CP2K/6
CP2K/6.1-foss-2019a: haswell, rome
CP2K/6.1-foss-2019a-spglib: haswell, rome
CP2K/6.1-intel-2018a: sandy, haswell
CP2K/6.1-intel-2018a-spglib: haswell
```

There you will see that only the modules built with toolchain `foss` are available on architecture
`rome`, not the ones built with `intel`. So you can load, e.g. `ml CP2K/6.1-foss-2019a`.

Then, when writing your batch script, you have to specify the partition `romeo`. Also, if e.g. you
wanted to use an entire ROME node (no SMT) and fill it with MPI ranks, it could look like this:

```bash
#!/bin/bash
#SBATCH --partition=romeo
#SBATCH --ntasks-per-node=128
#SBATCH --nodes=1
#SBATCH --mem-per-cpu=1972

srun cp2k.popt input.inp
```

## Using the Intel Toolchain on Rome

Currently, we have only newer toolchains starting at `intel/2019b` installed for the Rome nodes.
Even though they have AMD CPUs, you can still use the Intel compilers on there and they don't even
create bad-performing code. When using the Intel Math Kernel Library (MKL) up to version 2019,
though, you should set the following environment variable to make sure that AVX2 is used:

```bash
export MKL_DEBUG_CPU_TYPE=5
```

Without it, the MKL does a CPUID check and disables AVX2/FMA on non-Intel CPUs, leading to much
worse performance.

!!! note

    In version 2020 and above, Intel has removed this environment variable and added separate Zen codepaths
    to the library. However, they are still incomplete and do not cover every BLAS function. Also, the
    Intel AVX2 codepaths still seem to provide somewhat better performance, so a new workaround
    would be to overwrite the `mkl_serv_intel_cpu_true` symbol with a custom function:

```c
int mkl_serv_intel_cpu_true() {
    return 1;
}
```

and preloading this in a library:

```console
marie@login$ gcc -shared -fPIC -o libfakeintel.so fakeintel.c
marie@login$ export LD_PRELOAD=libfakeintel.so
```

As for compiler optimization flags, `-xHOST` does not seem to produce best-performing code in every
case on Rome. You might want to try `-mavx2 -fma` instead.

### Intel MPI

We have seen only half the theoretical peak bandwidth via InfiniBand between two nodes, whereas
Open MPI got close to the peak bandwidth, so you might want to avoid using Intel MPI on partition
`rome` if your application heavily relies on MPI communication until this issue is resolved.


---
search:
  boost: 2.0
---

# Batch System Slurm

ZIH uses the batch system Slurm for resource management and job scheduling. Compute nodes are not
accessed directly, but addressed through Slurm. You specify the needed resources
(cores, memory, GPU, time, ...) and Slurm will schedule your job for execution.

When logging in to ZIH systems, you are placed on a login node. There, you can manage your
[data life cycle](../data_lifecycle/overview.md),
setup experiments, and
edit and prepare jobs. The login nodes are not suited for computational work! From the login nodes,
you can interact with the batch system, e.g., submit and monitor your jobs.

??? note "Batch System"

    The batch system is the central organ of every HPC system users interact with its compute
    resources. The batch system finds an adequate compute system (partition) for your compute jobs.
    It organizes the queueing and messaging, if all resources are in use. If resources are available
    for your job, the batch system allocates and connects to these resources, transfers runtime
    environment, and starts the job.

    A workflow could look like this:

    ```mermaid
    sequenceDiagram
        user ->>+ login node: run programm
        login node ->> login node: kill after 5 min
        login node ->>- user: Killed!
        user ->> login node: salloc [...]
        login node ->> Slurm: Request resources
        Slurm ->> user: resources
        user ->>+ allocated resources: srun [options] [command]
        allocated resources ->> allocated resources: run command (on allocated nodes)
        allocated resources ->>- user: program finished
        user ->>+ allocated resources: srun [options] [further_command]
        allocated resources ->> allocated resources: run further command
        allocated resources ->>- user: program finished
        user ->>+ allocated resources: srun [options] [further_command]
        allocated resources ->> allocated resources: run further command
        Slurm ->> allocated resources: Job limit reached/exceeded
        allocated resources ->>- user: Job limit reached
    ```

??? note "Batch Job"

    At HPC systems, computational work and resource requirements are encapsulated into so-called
    jobs. In order to allow the batch system an efficient job placement it needs these
    specifications:

    * requirements: number of nodes and cores, memory per core, additional resources (GPU)
    * maximum run-time
    * HPC project for accounting
    * who gets an email on which occasion

    Moreover, the [runtime environment](../software/overview.md) as well as the executable and
    certain command-line arguments have to be specified to run the computational work.

This page provides a brief overview on

* [Slurm options](#options) to specify resource requirements,
* how to submit [interactive](#interactive-jobs) and [batch jobs](#batch-jobs),
* how to [write job files](#job-files),
* how to [manage and control your jobs](#manage-and-control-jobs).

If you are are already familiar with Slurm, you might be more interested in our collection of
[job examples](slurm_examples.md) and [job examples for GPU usage](slurm_examples_with_gpu.md).
There is also a ton of external resources regarding Slurm. We recommend these links for detailed
information:

- [slurm.schedmd.com](https://slurm.schedmd.com/) provides the official documentation comprising
   manual pages, tutorials, examples, etc.
- [Comparison with other batch systems](https://www.schedmd.com/slurmdocs/rosetta.html)

## Job Submission

There are three basic Slurm commands for job submission and execution:

1. `srun`: Run a parallel application (and, if necessary, allocate resources first).
1. `sbatch`: Submit a batch script to Slurm for later execution.
1. `salloc`: Obtain a Slurm job allocation (i.e., resources like CPUs, nodes and GPUs) for
interactive use. Release the allocation when finished.

Executing a program with `srun` directly on the shell will be blocking and launch an
[interactive job](#interactive-jobs). Apart from short test runs, it is recommended to submit your
jobs to Slurm for later execution by using [batch jobs](#batch-jobs). For that, you can conveniently
put the parameters in a [job file](#job-files), which you can submit using `sbatch
[options] <job file>`.

After submission, your job gets a unique job ID, which is stored in the environment variable
`SLURM_JOB_ID` at job runtime. The command `sbatch` outputs the job ID to stderr. Furthermore, you
can find it via `squeue --me`. The job ID allows you to
[manage and control](#manage-and-control-jobs) your jobs.

!!! warning "srun vs. mpirun"

    On ZIH systems, `srun` is used to run your parallel application. The use of `mpirun` is provenly
    broken on clusters `Power9` and `Alpha` for jobs requiring more than one node. Especially when
    using code from github projects, double-check its configuration by looking for a line like
    'submit command  mpirun -n $ranks ./app' and replace it with 'srun ./app'.

    Otherwise, this may lead to wrong resource distribution and thus job failure, or tremendous
    slowdowns of your application.

## Options

The following table contains the most important options for `srun`, `sbatch`, `salloc` to specify
resource requirements and control communication.

??? tip "Options Table (see `man sbatch` for all available options)"

    | Slurm Option               | Description |
    |:---------------------------|:------------|
    | `-n, --ntasks=<N>`         | Total number of (MPI) tasks (default: 1) |
    | `-N, --nodes=<N>`          | Number of compute nodes |
    | `--ntasks-per-node=<N>`    | Number of tasks per allocated node to start (default: 1) |
    | `-c, --cpus-per-task=<N>`  | Number of CPUs per task; needed for multithreaded (e.g. OpenMP) jobs; typically `N` should be equal to `OMP_NUM_THREADS` |
    | `--mem-per-cpu=<size>`     | Memory need per allocated CPU in MB |
    | `-t, --time=<HH:MM:SS>`    | Maximum runtime of the job |
    | `--mail-user=<your email>` | Get updates about the status of the jobs |
    | `--mail-type=ALL`          | For what type of events you want to get a mail; valid options: `ALL`, `BEGIN`, `END`, `FAIL`, `REQUEUE` |
    | `-J, --job-name=<name>`    | Name of the job shown in the queue and in mails (cut after 24 chars) |
    | `--no-requeue`             | Disable requeueing of the job in case of node failure (default: enabled) |
    | `--exclusive`              | Exclusive usage of compute nodes; you will be charged for all CPUs/cores on the node |
    | `-A, --account=<project>`  | Charge resources used by this job to the specified project |
    | `-o, --output=<filename>`  | File to save all normal output (stdout) (default: `slurm-%j.out`) |
    | `-e, --error=<filename>`   | File to save all error output (stderr)  (default: `slurm-%j.out`) |
    | `-a, --array=<arg>`        | Submit an array job ([examples](slurm_examples.md#array-jobs)) |
    | `-w <node1>,<node2>,...`   | Restrict job to run on specific nodes only |
    | `-x <node1>,<node2>,...`   | Exclude specific nodes from job |
    | `--switches=<count>[@max-time]` | Optimum switches and max time to wait for optimum |
    | `--signal=<sig_num>[@sig_time]` | Send signal `sig_num` to job `sig_time` before time limit (see [Checkoint/Restart page](checkpoint_restart.md#signal-handler)) |
    | `--test-only`              | Retrieve estimated start time of a job considering the job queue; does not actually submit the job nor run the application |

!!! note "Output and Error Files"

    When redirecting stderr and stderr into a file using `--output=<filename>` and
    `--stderr=<filename>`, make sure the target path is writeable on the
    compute nodes, i.e., it may not point to a read-only mounted
    [filesystem](../data_lifecycle/overview.md) like `/projects.`

!!! note "No free lunch"

    Runtime and memory limits are enforced. Please refer to the page
    [Slurm resource limits](slurm_limits.md) for a detailed overview.

### Host List

If you want to place your job onto specific nodes, use `-w, --nodelist=<host1,host2,..>` with a
list of hosts that will work for you.

### Number of Switches

You can fine tune your job by specifying the number of switches desired for the job allocation and
optionally the maximum time to wait for that number of switches. The corresponding option to
`sbatch` is `--switches=<count>[@max-time]`. The job remains pending until it either finds an
allocation with desired switch count or the time limit expires. Acceptable time formats include
"minutes", "minutes:seconds", "hours:minutes:seconds", "days-hours", "days-hours:minutes" and
"days-hours:minutes:seconds". For a detailed explanation, please refer to the
[sbatch online documentation](https://slurm.schedmd.com/sbatch.html#OPT_switches).

## Interactive Jobs

Interactive activities like editing, compiling, preparing experiments etc. are normally limited to
the login nodes. For longer interactive sessions, you can allocate resources on the compute node
with the command `salloc`. It takes the same options as `sbatch` to specify the required resources.

`salloc` returns a new shell on the node where you submitted the job. You need to use the command
`srun` in front of the following commands to have these commands executed on the allocated
resources. If you request for  more than one task, please be aware that `srun` will run the command
on each allocated task by default! To release the allocated resources, invoke the command `exit` or
`scancel <jobid>`.

!!! example "Example: Interactive allocation using `salloc`"

    The following code listing depicts the allocation of two nodes with two tasks on each node with a
    time limit of one hour on the cluster `Barnard` for interactive usage.

    ```console linenums="1"
    marie@login.barnard$ salloc --nodes=2 --ntasks-per-node=2 --time=01:00:00
    salloc: Pending job allocation 1234567
    salloc: job 1234567 queued and waiting for resources
    salloc: job 1234567 has been allocated resources
    salloc: Granted job allocation 1234567
    salloc: Waiting for resource configuration
    salloc: Nodes n[1184,1223] are ready for job
    [...]
    marie@login.barnard$ hostname
    login1
    marie@login.barnard$ srun hostname
    n1184
    n1184
    n1223
    n1223
    marie@login.barnard$ exit # ending the resource allocation
    ```

    After Slurm successfully allocated resources for the job, a new shell is created on the submit
    host (cf. lines 9-10).

    In order to use the allocated resources, you need to invoke your commands with `srun` (cf. lines
    11 ff).

The command `srun` also creates an allocation, if it is running outside any `sbatch` or `salloc`
allocation.

```console
marie@login$ srun --pty --ntasks=1 --cpus-per-task=4 --time=1:00:00 --mem-per-cpu=1700 bash -l
srun: job 13598400 queued and waiting for resources
srun: job 13598400 has been allocated resources
marie@compute$ # Now, you can start interactive work with e.g. 4 cores
```

Since Slurm 20.11 `--exclusive` is the default for `srun` as a step, that means you have to
use `--overlap`, if you want to run `srun` within a `srun` allocation.

```console
marie@login$ srun --pty bash -l
srun: job 27410688 queued and waiting for resources
srun: job 27410688 has been allocated resources
marie@compute$ srun --overlap hostname
taurusi6604.taurus.hrsk.tu-dresden.de
```

!!! note "Using `module` commands in interactive mode"

    The [module commands](../software/modules.md) are made available by sourcing the files
    `/etc/profile` and `~/.bashrc`. This is done automatically by passing the parameter `-l` to your
    shell, as shown in the example above. If you missed adding `-l` at submitting the interactive
    session, no worry, you can source this files also later on manually (`source /etc/profile`).

### Interactive X11/GUI Jobs

Slurm will forward your X11 credentials to the first (or even all) node for a job with the
(undocumented) `--x11` option.

```console
marie@login$ srun --ntasks=1 --pty --x11=first xeyes
```

!!! hint "X11 error"

    If you are getting the error:

    ```Bash
    srun: error: x11: unable to connect node taurusiXXXX
    ```

    that probably means you still have an old host key for the target node in your
    `~.ssh/known_hosts` file (e.g. from pre-SCS5). This can be solved either by removing the entry
    from your `known_hosts` or by simply deleting the `known_hosts` file altogether if you don't have
    important other entries in it.

## Batch Jobs

Working interactively using `srun` and `salloc` is a good starting point for testing and compiling.
But, as soon as you leave the testing stage, we highly recommend to use batch jobs.
Batch jobs are encapsulated within [job files](#job-files) and submitted to the batch system using
`sbatch` for later execution. A job file is basically a script holding the resource requirements,
environment settings and the commands for executing the application. Using batch jobs and job files
has multiple advantages*:

* You can reproduce your experiments and work, because all steps are saved in a file.
* You can easily share your settings and experimental setup with colleagues.

*) If job files are version controlled or environment `env` is saved along with Slurm output.

!!! hint "Syntax: Submitting a batch job"

    ```console
    marie@login$ sbatch [options] <job_file>
    ```

### Job Files

Job files have to be written with the following structure.

```bash
#!/bin/bash
# ^Batch script starts with shebang line

#SBATCH --ntasks=24                   # #SBATCH lines request resources and
#SBATCH --time=01:00:00               # specify Slurm options
#SBATCH --account=<KTR>               #
#SBATCH --job-name=fancyExp           # All #SBATCH lines have to follow uninterrupted
#SBATCH --output=simulation-%j.out    # after the shebang line
#SBATCH --error=simulation-%j.err     # Comments start with # and do not count as interruptions

module purge                          # Set up environment, e.g., clean/switch modules environment
module load <module1 module2>         # and load necessary modules

srun ./application [options]          # Execute parallel application with srun
```

The following two examples show the basic resource specifications for a pure OpenMP application and
a pure MPI application, respectively. Within the section [Job Examples](slurm_examples.md), we
provide a comprehensive collection of job examples.

??? example "Job file OpenMP"

    ```bash
    #!/bin/bash

    #SBATCH --nodes=1
    #SBATCH --tasks-per-node=1
    #SBATCH --cpus-per-task=64
    #SBATCH --time=01:00:00
    #SBATCH --account=<account>

    module purge
    module load <modules>

    export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
    srun ./path/to/openmp_application
    ```

    * Submisson: `marie@login$ sbatch batch_script.sh`
    * Run with fewer CPUs: `marie@login$ sbatch --cpus-per-task=14 batch_script.sh`

??? example "Job file MPI"

    ```bash
    #!/bin/bash

    #SBATCH --ntasks=64
    #SBATCH --time=01:00:00
    #SBATCH --account=<account>

    module purge
    module load <modules>

    srun ./path/to/mpi_application
    ```

    * Submisson: `marie@login$ sbatch batch_script.sh`
    * Run with fewer MPI tasks: `marie@login$ sbatch --ntasks=14 batch_script.sh`

## Using Simultaneous Multithreading (SMT)

Most modern architectures offer simultaneous multithreading (SMT), where physical cores of a CPU are
split into virtual cores (aka. threads). This technique allows to run two instruction streams per
physical core in parallel.

At ZIH systems, SMT is available at the partitions `rome` and `alpha`. It is deactivated by
default, because the environment variable `SLURM_HINT` is set to `nomultithread`.
If you wish to make use of the SMT cores, you need to explicitly activate it.
In principle, there are two different ways:

1. Change the value of the environment variable via `export SLURM_HINT=multithread` in your current
   shell and submit your job file, or invoke your `srun` or `salloc` command line.

1. Clear the environment variable via `unset SLURM_HINT` and provide the option `--hint=multithread`
   to `sbatch`, `srun` or `salloc` command line.

??? warning

     If you like to activate SMT via the directive
     ```
     #SBATCH --hint=multithread
     ```
     within your job file, you also have to clear the environment variable `SLURM_HINT` before
     submitting the job file. Otherwise, the environment varibale `SLURM_HINT` takes precedence.

## Heterogeneous Jobs

A heterogeneous job consists of several job components, all of which can have individual job
options. In particular, different components can use resources from different Slurm partitions.
One example for this setting is an MPI application consisting of a master process with a huge memory
footprint and worker processes requiring GPU support.

The `salloc`, `sbatch` and `srun` commands can all be used to submit heterogeneous jobs. Resource
specifications for each component of the heterogeneous job should be separated with ":" character.
Running a job step on a specific component is supported by the option `--het-group`.

```console
marie@login$ salloc --ntasks=1 --cpus-per-task=4 --partition <partition> --mem=200G : \
                    --ntasks=8 --cpus-per-task=1 --gres=gpu:8 --mem=80G --partition <partition>
[...]
marie@login$ srun ./my_application <args for master tasks> : ./my_application <args for worker tasks>
```

Heterogeneous jobs can also be defined in job files. There, it is required to separate multiple
components by a line containing the directive `#SBATCH hetjob`.

```bash
#!/bin/bash

#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --partition=<partition>
#SBATCH --mem=200G
#SBATCH hetjob # required to separate groups
#SBATCH --ntasks=8
#SBATCH --cpus-per-task=1
#SBATCH --gres=gpu:8
#SBATCH --mem=80G
#SBATCH --partition=<partition>

srun ./my_application <args for master tasks> : ./my_application <args for worker tasks>

# or as an alternative
srun ./my_application <args for master tasks> &
srun --het-group=1 ./my_application <args for worker tasks> &
wait
```

### Limitations

Due to the way scheduling algorithm works it is required that each component has to be allocated on
a different node. Furthermore, job arrays of heterogeneous jobs are not supported.

## Manage and Control Jobs

### Job and Slurm Monitoring

On the command line, use `squeue` to watch the scheduling queue.

!!! tip "Show your jobs"

    Invoke `squeue --me` to list only your jobs.

In its last column, the `squeue` command will also tell why a job is not running.
Possible reasons and their detailed descriptions are listed in the following table.
More information about job parameters can be obtained with `scontrol -d show
job <jobid>`.

??? tip "Reason Table"

    | Reason             | Long Description  |
    |:-------------------|:------------------|
    | `Dependency`         | This job is waiting for a dependent job to complete. |
    | `None`               | No reason is set for this job. |
    | `PartitionDown`      | The partition required by this job is in a down state. |
    | `PartitionNodeLimit` | The number of nodes required by this job is outside of its partitions current limits. Can also indicate that required nodes are down or drained. |
    | `PartitionTimeLimit` | The jobs time limit exceeds its partitions current time limit. |
    | `Priority`           | One or higher priority jobs exist for this partition. |
    | `Resources`          | The job is waiting for resources to become available. |
    | `NodeDown`           | A node required by the job is down. |
    | `BadConstraints`     | The jobs constraints can not be satisfied. |
    | `SystemFailure`      | Failure of the Slurm system, a filesystem, the network, etc. |
    | `JobLaunchFailure`   | The job could not be launched. This may be due to a filesystem problem, invalid program name, etc. |
    | `NonZeroExitCode`    | The job terminated with a non-zero exit code. |
    | `TimeLimit`          | The job exhausted its time limit. |
    | `InactiveLimit`      | The job reached the system inactive limit. |

For detailed information on why your submitted job has not started yet, you can use the command

```console
marie@login$ whypending <jobid>
```

### Editing Jobs

Jobs that have not yet started can be altered. By using `scontrol update timelimit=4:00:00
jobid=<jobid>`, it is for example possible to modify the maximum runtime. `scontrol` understands
many different options, please take a look at the
[scontrol documentation](https://slurm.schedmd.com/scontrol.html) for more details.

### Canceling Jobs

The command `scancel <jobid>` kills a single job and removes it from the queue. By using `scancel -u
<username>`, you can send a canceling signal to all of your jobs at once.

### Evaluating Jobs

The Slurm command `sacct` provides job statistics like memory usage, CPU time, energy usage etc.
as table-formatted output on the command line.

The job monitor [PIKA](../software/pika.md) provides web-based graphical performance statistics
at no extra cost.

!!! hint "Learn from old jobs"

    We highly encourage you to inspect your previous jobs in order to better
    estimate the requirements, e.g., runtime, for future jobs.
    With PIKA, it is e.g. easy to check whether a job is hanging, idling,
    or making good use of the resources.

??? tip "Using sacct (see also `man sacct`)"

    `sacct` outputs the following fields by default.

    ```console
    # show all own jobs contained in the accounting database
    marie@login$ sacct
        JobID    JobName  Partition    Account  AllocCPUS      State ExitCode
    ------------ ---------- ---------- ---------- ---------- ---------- --------
    [...]
    ```

    We'd like to point your attention to the following options to gain insight in your jobs.

    ??? example "Show specific job"

        ```console
        marie@login$ sacct --jobs=<JOBID>
        ```

    ??? example "Show all fields for a specific job"

        ```console
        marie@login$ sacct --jobs=<JOBID> --format=All
        ```

    ??? example "Show specific fields"

        ```console
        marie@login$ sacct --jobs=<JOBID> --format=JobName,MaxRSS,MaxVMSize,CPUTime,ConsumedEnergy
        ```

    The manual page (`man sacct`) and the [sacct online reference](https://slurm.schedmd.com/sacct.html)
    provide a comprehensive documentation regarding available fields and formats.

    !!! hint "Time span"

        By default, `sacct` only shows data of the last day. If you want to look further into the past
        without specifying an explicit job id, you need to provide a start date via the option
        `--starttime` (or short: `-S`). A certain end date is also possible via `--endtime` (or `-E`).

    ??? example "Show all jobs since the beginning of year 2021"

        ```console
        marie@login$ sacct --starttime 2021-01-01 [--endtime now]
        ```

## Jobs at Reservations

Within a reservation, you have privileged access to HPC resources.
How to ask for a reservation is described in the section
[reservations](overview.md#exclusive-reservation-of-hardware).
After we agreed with your requirements, we will send you an e-mail with your reservation name. Then,
you could see more information about your reservation with the following command:

```console
marie@login$ scontrol show res=<reservation name>
# e.g. scontrol show res=hpcsupport_123
```

If you want to use your reservation, you have to add the parameter
`--reservation=<reservation name>` either in your job script or to your `srun` or `salloc` command.

## Node-Local Storage in Jobs

For some workloads and applications, it is valuable to use node-local storage in order to reduce or
even completely omit usage of the [parallel filesystems](../data_lifecycle/working.md).

The availability and the capacity of local storage differ between the clusters, as depicted in the
following table.

| Cluster          | Number of  Nodes  | Local Storage         | Mountpoint | Request                                                            |
|------------------|-------------------|-----------------------|------------|--------------------------------------------------------------------|
| [`Alpha Centauri`](hardware_overview.md#alpha-centauri) | All compute nodes | 3.5 TB on NVMe device | `/tmp`     | Always present, no action needed                                   |
| [`Barnard`](hardware_overview.md#barnard)               | 12 nodes          | 1.8 TB on NVMe device | `/tmp`     | `--constraint=local_disk` option to `sbatch`, `salloc`, and `srun` |
| [`Capella`](hardware_overview.md#capella)               | All compute nodes | 800 GB                | `/tmp`     | Always present, no action needed                                   |
| [`Romeo`](hardware_overview.md#romeo)                   | All compute nodes | 200 GB                | `/tmp`     | Always present, no action needed                                   |

!!! hint "Clusters `Power9` and `Julia`"

    Node-local storage is not available on the two clusters [`Power9`](hardware_overview.md#power9)
    and [`Julia`](julia.md).

!!! hint Request nodes with local storage on `Barnard`

    Note that most nodes on `Barnard` don't have a local disk and space in `/tmp` is **very**
    limited. If you need a local disk request this with the
    [Slurm feature](slurm.md#node-local-storage-in-jobs) `--constraint=local_disk` to
    `sbatch`, `salloc`, and `srun`.


# Job Examples

## Parallel Jobs

For submitting parallel jobs, a few rules have to be understood and followed. In general, they
depend on the type of parallelization and architecture.

### OpenMP Jobs

An SMP-parallel job can only run within a node, so it is necessary to include the options `--node=1`
and `--ntasks=1`. The maximum number of processors for an SMP-parallel program is 896 on the cluster
[`Julia`](julia.md) as described in the
[section on memory limits](slurm_limits.md#slurm-resource-limits-table). Using the option
`--cpus-per-task=<N>` Slurm will start one task and you will have `N` CPUs available for your job.
An example job file would look like:

!!! example "Job file for OpenMP application"

    ```Bash
    #!/bin/bash
    #SBATCH --nodes=1
    #SBATCH --tasks-per-node=1
    #SBATCH --cpus-per-task=8
    #SBATCH --time=08:00:00
    #SBATCH --mail-type=start,end
    #SBATCH --mail-user=<your.email>@tu-dresden.de

    export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
    ./path/to/binary
    ```

### MPI Jobs

For MPI-parallel jobs one typically allocates one core per task that has to be started.

!!! warning "MPI libraries"

    There are different MPI libraries on ZIH systems for the different micro architectures. Thus,
    you have to compile the binaries specifically for the target architecture of the cluster of
    interest. Please refer to the sections [building software](../software/building_software.md) and
    [module environments](../software/modules.md#module-environments) for detailed information.

!!! example "Job file for MPI application"

    ```Bash
    #!/bin/bash
    #SBATCH --ntasks=864
    #SBATCH --time=08:00:00
    #SBATCH --job-name=Science1
    #SBATCH --mail-type=end
    #SBATCH --mail-user=<your.email>@tu-dresden.de

    srun ./path/to/binary
    ```

### Multiple Programs Running Simultaneously in a Job

In this short example, our goal is to run four instances of a program concurrently in a **single**
batch script. Of course, we could also start a batch script four times with `sbatch`, but this is
not what we want to do here. However, you can also find an example about
[how to run GPU programs simultaneously in a single job](slurm_examples_with_gpu.md#running-multiple-gpu-applications-simultaneously-in-a-batch-job)

!!! example " "

    ```Bash
    #!/bin/bash
    #SBATCH --ntasks=4
    #SBATCH --cpus-per-task=1
    #SBATCH --time=01:00:00
    #SBATCH --job-name=PseudoParallelJobs
    #SBATCH --mail-type=end
    #SBATCH --mail-user=<your.email>@tu-dresden.de

    # The following sleep command was reported to fix warnings/errors with srun by users (feel free to uncomment).
    #sleep 5
    srun --exclusive --ntasks=1 ./path/to/binary &

    #sleep 5
    srun --exclusive --ntasks=1 ./path/to/binary &

    #sleep 5
    srun --exclusive --ntasks=1 ./path/to/binary &

    #sleep 5
    srun --exclusive --ntasks=1 ./path/to/binary &

    echo "Waiting for parallel job steps to complete..."
    wait
    echo "All parallel job steps completed!"
    ```

### Request Resources for Parallel Make

From time to time, you want to build and compile software and applications on a compute node.
But, do you need to request tasks or CPUs from Slurm in order to provide resources for the parallel
`make` command?  The answer is "CPUs".

!!! example "Interactive allocation for parallel `make` command"

    ```console
    marie@login$ srun --ntasks=1 --cpus-per-task=16 --mem=16G --time=01:00:00 --pty bash --login
    [...]
    marie@compute$ # prepare the source code for building using configure, cmake or so
    marie@compute$ make -j 16
    ```

## Exclusive Jobs for Benchmarking

Jobs on ZIH systems run, by default, in shared-mode, meaning that multiple jobs (from different
users) can run at the same time on the same compute node. Sometimes, this behavior is not desired
(e.g. for benchmarking purposes). You can request for exclusive usage of resources using the Slurm
parameter `--exclusive`.

!!! note "Exclusive does not allocate all available resources"

    Setting `--exclusive` **only** makes sure that there will be **no other jobs running on your
    nodes**.  It does not, however, mean that you automatically get access to all the resources
    which the node might provide without explicitly requesting them.

    E.g. you still have to request for a GPU via the generic resources parameter (`gres`) on the GPU
    cluster. On the other hand, you also have to request all cores of a node if you need them.

CPU cores can either to be used for a task (`--ntasks`) or for multi-threading within the same task
(`--cpus-per-task`). Since those two options are semantically different (e.g., the former will
influence how many MPI processes will be spawned by `srun` whereas the latter does not), Slurm
cannot determine automatically which of the two you might want to use. Since we use cgroups for
separation of jobs, your job is not allowed to use more resources than requested.

Here is a short example to ensure that a benchmark is not spoiled by other jobs, even if it doesn't
use up all resources of the nodes:

!!! example "Job file with exclusive resources"

    ```Bash
    #!/bin/bash
    #SBATCH --nodes=2
    #SBATCH --ntasks-per-node=2
    #SBATCH --cpus-per-task=8
    #SBATCH --exclusive            # ensure that nobody spoils my measurement on 2 x 2 x 8 cores
    #SBATCH --time=00:10:00
    #SBATCH --job-name=benchmark
    #SBATCH --mail-type=start,end
    #SBATCH --mail-user=<your.email>@tu-dresden.de

    srun ./my_benchmark
    ```

## Array Jobs

Array jobs can be used to create a sequence of jobs that share the same executable and resource
requirements, but have different input files, to be submitted, controlled, and monitored as a single
unit. The option is `-a, --array=<indexes>` where the parameter `indexes` specifies the array
indices. The following specifications are possible

* comma separated list, e.g., `--array=0,1,2,17`,
* range based, e.g., `--array=0-42`,
* step based, e.g., `--array=0-15:4`,
* mix of comma separated and range base, e.g., `--array=0,1,2,16-42`.

A maximum number of simultaneously running tasks from the job array may be specified using the `%`
separator. The specification `--array=0-23%8` limits the number of simultaneously running tasks from
this job array to 8.

Within the job you can read the environment variables `SLURM_ARRAY_JOB_ID` and
`SLURM_ARRAY_TASK_ID` which is set to the first job ID of the array and set individually for each
step, respectively.

Within an array job, you can use `%a` and `%A` in addition to `%j` and `%N` to make the output file
name specific to the job:

* `%A` will be replaced by the value of `SLURM_ARRAY_JOB_ID`
* `%a` will be replaced by the value of `SLURM_ARRAY_TASK_ID`

!!! example "Job file using job arrays"

    ```Bash
    #!/bin/bash
    #SBATCH --array=0-9
    #SBATCH --output=arraytest-%A_%a.out
    #SBATCH --error=arraytest-%A_%a.err
    #SBATCH --ntasks=864
    #SBATCH --time=08:00:00
    #SBATCH --job-name=Science1
    #SBATCH --mail-type=end
    #SBATCH --mail-user=<your.email>@tu-dresden.de

    echo "Hi, I am step $SLURM_ARRAY_TASK_ID in this array job $SLURM_ARRAY_JOB_ID"
    ```

!!! note

    If you submit a large number of jobs doing heavy I/O in the Lustre filesystems you should limit
    the number of your simultaneously running job with a second parameter like:

    ```Bash
    #SBATCH --array=1-100000%100
    ```

Please read the Slurm documentation at https://slurm.schedmd.com/sbatch.html for further details.

## Chain Jobs

You can use chain jobs to **create dependencies between jobs**. This is often useful if a job
relies on the result of one or more preceding jobs. Chain jobs can also be used to split a long
running job exceeding the batch queues limits into parts and chain these parts. Slurm has an option
`-d, --dependency=<dependency_list>` that allows to specify that a job is only allowed to start if
another job finished.

In the following we provide two examples for scripts that submit chain jobs.

??? example "Scaling experiment using chain jobs"

    This scripts submits the very same job file `myjob.sh` four times, which will be executed one
    after each other. The number of tasks is increased from job to job making this submit script a
    good starting point for (strong) scaling experiments.

    ```Bash title="submit_scaling.sh"
    #!/bin/bash

    task_numbers="1 2 4 8"
    dependency=""
    job_file="myjob.sh"

    for tasks in ${task_numbers} ; do
        job_cmd="sbatch --ntasks=${tasks}"
        if [ -n "${dependency}" ] ; then
            job_cmd="${job_cmd} --dependency=afterany:${dependency}"
        fi
        job_cmd="${job_cmd} ${job_file}"
        echo -n "Running command: ${job_cmd}  "
        out="$(${job_cmd})"
        echo "Result: ${out}"
        dependency=$(echo "${out}" | awk '{print $4}')
    done
    ```

    The output looks like:
    ```console
    marie@login$ sh submit_scaling.sh
    Running command: sbatch --ntasks=1 myjob.sh  Result: Submitted batch job 2963822
    Running command: sbatch --ntasks=2 --dependency afterany:32963822 myjob.sh  Result: Submitted batch job 2963823
    Running command: sbatch --ntasks=4 --dependency afterany:32963823 myjob.sh  Result: Submitted batch job 2963824
    Running command: sbatch --ntasks=8 --dependency afterany:32963824 myjob.sh  Result: Submitted batch job 2963825
    ```

??? example "Example to submit job chain via script"

    This script submits three different job files, which will be executed one after each other. Of
    course, the dependency reasons can be adopted.

    ```bash title="submit_job_chain.sh"
    #!/bin/bash

    declare -a job_names=("jobfile_a.sh" "jobfile_b.sh" "jobfile_c.sh")
    dependency=""
    arraylength=${#job_names[@]}

    for (( i=0; i<arraylength; i++ )) ; do
      job_nr=$((i + 1))
      echo "Job ${job_nr}/${arraylength}: ${job_names[$i]}"
      if [ -n "${dependency}" ] ; then
          echo "Dependency: after job ${dependency}"
          dependency="--dependency=afterany:${dependency}"
      fi
      job="sbatch ${dependency} ${job_names[$i]}"
      out=$(${job})
      dependency=$(echo "${out}" | awk '{print $4}')
    done
    ```

    The output looks like:
    ```console
    marie@login$ sh submit_job_chains.sh
    Job 1/3: jobfile_a.sh
    Job 2/3: jobfile_b.sh
    Dependency: after job 2963708
    Job 3/3: jobfile_c.sh
    Dependency: after job 2963709
    ```

## Requesting GPUs

 Examples of jobs that require the use of GPUs can be found in the
 [Job Examples with GPU](slurm_examples_with_gpu.md) section.


# Job Examples with GPU

General information on how to request resources via the Slurm batch system can be found in the
[Job Examples](slurm_examples.md) section.

## Requesting GPUs

Slurm will allocate one or many GPUs for your job if requested.
Please note that GPUs are only available in the GPU clusters, like
[`Alpha`](hardware_overview.md#alpha-centauri), [`Capella`](hardware_overview.md#capella)
and [`Power9`](hardware_overview.md#power9).
The option for `sbatch/srun` in this case is `--gres=gpu:[NUM_PER_NODE]`,
where `NUM_PER_NODE` is the number of GPUs **per node** that will be used for the job.

!!! example "Job file to request a GPU"

    ```Bash
    #!/bin/bash
    #SBATCH --nodes=2              # request 2 nodes
    #SBATCH --mincpus=1            # allocate one task per node...
    #SBATCH --ntasks=2             # ...which means 2 tasks in total (see note below)
    #SBATCH --cpus-per-task=6      # use 6 threads per task
    #SBATCH --gres=gpu:1           # use 1 GPU per node (i.e. use one GPU per task)
    #SBATCH --time=01:00:00        # run for 1 hour
    #SBATCH --account=p_number_crunch      # account CPU time to project p_number_crunch

    srun ./your/cuda/application   # start you application (probably requires MPI to use both nodes)
    ```

!!! note

    Due to an unresolved issue concerning the Slurm job scheduling behavior, it is currently not
    practical to use `--ntasks-per-node` together with GPU jobs. If you want to use multiple nodes,
    please use the parameters `--ntasks` and `--mincpus` instead. The values of `mincpus`*`nodes`
    has to equal `ntasks` in this case.

### Limitations of GPU Job Allocations

The number of cores per node that are currently allowed to be allocated for GPU jobs is limited
depending on how many GPUs are being requested.
This is because we do not wish that GPUs become unusable due to all cores on a node being used by
a single job which does not, at the same time, request all GPUs.

E.g., if you specify `--gres=gpu:2`, your total number of cores per node (meaning:
`ntasks`*`cpus-per-task`) may not exceed 12 on [`Alpha`](alpha_centauri.md) or on
[`Capella`](capella.md).

Note that this also has implications for the use of the `--exclusive` parameter.
Since this sets the number of allocated cores to the maximum, you also **must** request all GPUs
otherwise your job will not start.
In the case of `--exclusive`, it won't be denied on submission,
because this is evaluated in a later scheduling step.
Jobs that directly request too many cores per GPU will be denied with the error message:

```console
Batch job submission failed: Requested node configuration is not available
```

Similar it is not allowed to start CPU-only jobs on the GPU cluster.
I.e. you must request at least one GPU there, or you will get this error message:

```console
srun: error: QOSMinGRES
srun: error: Unable to allocate resources: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)
```

### Running Multiple GPU Applications Simultaneously in a Batch Job

Our starting point is a (serial) program that needs a single GPU and four CPU cores to perform its
task (e.g. TensorFlow). The following batch script shows how to run such a job on any of
the GPU clusters `Power9`, `Alpha` or `Capella`.

!!! example

    ```bash
    #!/bin/bash
    #SBATCH --ntasks=1
    #SBATCH --cpus-per-task=4
    #SBATCH --gres=gpu:1
    #SBATCH --gpus-per-task=1
    #SBATCH --time=01:00:00
    #SBATCH --mem-per-cpu=1443

    srun some-gpu-application
    ```

When `srun` is used within a submission script, it inherits parameters from `sbatch`, including
`--ntasks=1`, `--cpus-per-task=4`, etc. So we actually implicitly run the following

```bash
srun --ntasks=1 --cpus-per-task=4 [...] some-gpu-application
```

Now, our goal is to run four instances of this program concurrently in a single batch script. Of
course we could also start the above script multiple times with `sbatch`, but this is not what we
want to do here.

#### Solution

In order to run multiple programs concurrently in a single batch script/allocation we have to do
three things:

1. Allocate enough resources to accommodate multiple instances of our program. This can be achieved
   with an appropriate batch script header (see below).
1. Start job steps with `srun` as background processes. This is achieved by adding an ampersand at
   the end of the `srun` command.
1. Make sure that each background process gets its private resources. We need to set the resource
   fraction needed for a single run in the corresponding `srun` command. The total aggregated
   resources of all job steps must fit in the allocation specified in the batch script header.
   Additionally, the option `--exclusive` is needed to make sure that each job step is provided with
   its private set of CPU and GPU resources.  The following example shows how four independent
   instances of the same program can be run concurrently from a single batch script. Each instance
   (task) is equipped with 4 CPUs (cores) and one GPU.

!!! example "Job file simultaneously executing four independent instances of the same program"

    ```Bash
    #!/bin/bash
    #SBATCH --ntasks=4
    #SBATCH --cpus-per-task=4
    #SBATCH --gres=gpu:4
    #SBATCH --gpus-per-task=1
    #SBATCH --time=01:00:00
    #SBATCH --mem-per-cpu=1443

    srun --exclusive --gres=gpu:1 --ntasks=1 --cpus-per-task=4 --gpus-per-task=1 --mem-per-cpu=1443 some-gpu-application &
    srun --exclusive --gres=gpu:1 --ntasks=1 --cpus-per-task=4 --gpus-per-task=1 --mem-per-cpu=1443 some-gpu-application &
    srun --exclusive --gres=gpu:1 --ntasks=1 --cpus-per-task=4 --gpus-per-task=1 --mem-per-cpu=1443 some-gpu-application &
    srun --exclusive --gres=gpu:1 --ntasks=1 --cpus-per-task=4 --gpus-per-task=1 --mem-per-cpu=1443 some-gpu-application &

    echo "Waiting for all job steps to complete..."
    wait
    echo "All jobs completed!"
    ```

In practice, it is possible to leave out resource options in `srun` that do not differ from the ones
inherited from the surrounding `sbatch` context. The following line would be sufficient to do the
job in this example:

```bash
srun --exclusive --gres=gpu:1 --ntasks=1 some-gpu-application &
```

Yet, it adds some extra safety to leave them in, enabling the Slurm batch system to complain if not
enough resources in total were specified in the header of the batch script.


# Slurm Job File Generator

This page provides a generator for Slurm job files covering

- basic Slurm options for resource specification and job management,
- data life cycle handling using workspaces,
- and a skeleton for setting up the computational environment using modules.

It is intended as a starting point for beginners and thus, does not cover all available Slurm
options.

If you are interested in providing this job file generator for your HPC users, you can find the
project at
[https://gitlab.hrz.tu-chemnitz.de/zih/hpcsupport/slurm-jobfile-generator](https://gitlab.hrz.tu-chemnitz.de/zih/hpcsupport/slurm-jobfile-generator).

<!--
This file is part of sgen software.
Slurm Jobfile Generator

Copyright (c) 2022,
    Technische Universitaet Dresden, Germany

sgen is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Foobar is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with sgen.  If not, see <http://www.gnu.org/licenses/>.
-->

<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0"> -->
    <title>Slurm-Job-Generator</title>
    <!-- <link type="text/css" href="../misc/style.css" rel="stylesheet"> -->
    <!-- <script src="jquery-3.6.0.min.js"> </script> -->
  </head>

  <body>
    <div class="header">
      <label class="header">TU Dresden Slurm Job Generator</label>
    </div>
    <button type="button" class="collapsible">General</button>
    <div class="content">
      <div class="row">
        <label class="cell-name">Job name (<tt>-J, --job-name</tt>) </label>
        <div class="cell-tooltip">
          <img id="job-name-info" class="info-img" src="../misc/info.png" title="help"
          alt="Information sign">
        </div>
        <input id="job-name" class="cell-input" type="text">
      </div>
      <div class="row">
        <label class="cell-name">Project (<tt>-A, --account</tt>)</label>
        <div class="cell-tooltip">
          <img id="account-info" class="info-img" src="../misc/info.png" title="help"
               alt="Information sign">
        </div>
        <input id="account" class="cell-input" type="text">
      </div>
      <div class="row">
        <label class="cell-name">Email (<tt>--mail-user, --mail-type</tt>)</label>
        <div class="cell-tooltip">
          <img id="mail-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
        </div>
        <div class="cell-input">
          <input id="mail" class="mail" type="mail">
          <input id="mail-all" type="checkbox">
          <label for="all">All</label>
          <input id="mail-begin" type="checkbox">
          <label for="begin">Begin</label>
          <input id="mail-end" type="checkbox">
          <label for="end">End</label>
          <input id="mail-fail" type="checkbox">
          <label for="fail">Fail</label>
        </div>
      </div>
    </div>

    <button type="button" class="collapsible">Resources</button>
    <div class="content">
      <div class="partition-input">
        <div class="row">
          <label class="cell-name">Time limit (<tt>-t, --time</tt>)</label>
          <div class="cell-tooltip">
            <img id="time-info" class="info-img" src="../misc/info.png" title="days-hours:minutes:seconds" alt="Information sign">
          </div>
          <input id="timelimit" class="cell-input" type="text" placeholder="00-00:00:00">
          <label id="time-text" class="limits cell-input"></label>
        </div>
        <div class="row">
          <label class="cell-name">Partition (<tt>-p, --partition</tt>)</label>
          <div class="cell-tooltip">
            <img id="partition-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <select id="partition" class="cell-input"></select>
        </div>
        <div class="row">
          <label class="cell-name">Nodes (<tt>-N, --nodes</tt>)</label>
          <div class="cell-tooltip">
            <img id="nodes-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <input id="nodes" class="cell-input" type="text">
          <label id="nodes-text" class="limits cell-input"></label>
        </div>
        <div class="row">
          <label class="cell-name">Tasks (<tt>-n, --ntasks</tt>)</label>
          <div class="cell-tooltip">
            <img id="tasks-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <input id="tasks" class="cell-input" type="text">
          <label id="tasks-text" class="limits cell-input"></label>
        </div>
        <div class="row">
          <label class="cell-name">Tasks per node (<tt>--tasks-per-node</tt>)</label>
          <div class="cell-tooltip">
            <img id="tasks/node-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <input id="tasks/node" class="cell-input" type="text">
          <label id="tasks/node-text" class="limits cell-input"></label>
        </div>
        <div class="row">
          <label class="cell-name">CPUs per task (<tt>-c, --cpus-per-task</tt>)</label>
          <div class="cell-tooltip">
            <img id="cpus-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <input id="cpus" class="cell-input" type="text">
          <label id="cpus-text" class="limits cell-input"></label>
        </div>
        <div id="div-thread" class="row">
          <span class="cell-name"></span>
          <div class="cell-tooltip">
            <img id="nomultithread-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <div class="cell-input">
            <input id="nomultithread" class="cell" type="checkbox">
            <label for="nomultithread">No Multithreading</label>
          </div>
        </div>
        <div id="div-gpu" class="row">
          <label class="cell-name">GPUs per node (<tt>--gpus-per-node</tt>)</label>
          <div class="cell-tooltip">
            <img id="gpus-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <input id="gpus" class="cell-input" type="text">
          <label id="gpus-text" class="limits cell-input"></label>
        </div>
        <div id="div-gpu/task" class="row">
          <label class="cell-name">GPUs per task (<tt>--gpus-per-task</tt>)</label>
          <div class="cell-tooltip">
            <img id="gpus/task-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <input id="gpus/task" class="cell-input" type="text">
          <label id="gpus/task-text" class="limits cell-input"></label>
        </div>
        <div class="row">
          <label class="cell-name">Memory per CPU (<tt>--mem-per-cpu</tt>)</label>
          <div class="cell-tooltip">
            <img id="mem-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <div class="cell-input">
            <input id="mem" type="text">
            <select id="byte">
              <option value="M" title="placeholder" selected="selected">MiB</option>
              <option value="G" title="placeholder">GiB</option>
            </select>
            <label id="mem-text" class="limits"></label>
          </div>
        </div>
        <div class="row">
          <label class="cell-name">Reservation (<tt>--reservation</tt>)</label>
          <div class="cell-tooltip">
            <img id="reservation-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <input id="reservation" class="cell-input" type="text" alt="Information sign">
        </div>
        <div class="row">
          <label class="cell-name">Exclusive (<tt>--exclusive</tt>)</label>
          <div class="cell-tooltip">
            <img id="exclusive-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <div class="cell-input">
            <input id="exclusive" type="checkbox">
          </div>
        </div>
      </div>
      <div class="partition-info">
        <pre id="info-panel-partition" class="info-pre"></pre>
      </div>
    </div>

    <button type="button" class="collapsible">Advanced</button>
    <div class="content">
      <div class="row">
        <label class="cell-name">Array (<tt>-a, --array</tt>)</label>
        <div class="cell-tooltip">
          <img id="array-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
        </div>
        <input id="array" class="cell-input" type="text" placeholder="1-5">
      </div>
      <div class="row">
        <label class="cell-name">Dependency (<tt>-d, --dependency</tt>)</label>
        <div class="cell-tooltip">
          <img id="dependency-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
        </div>
        <div class="cell-input">
          <select id="type-depend">
            <option value="none" title="placeholder" selected="selected"></option>
            <option value="after" title="placeholder">after</option>
            <option value="afterany" title="placeholder">afterany</option>
            <option value="afterburstbuffer" title="placeholder">afterburstbuffer</option>
            <option value="aftercorr" title="placeholder">aftercorr</option>
            <option value="afternotok" title="placeholder">afternotok</option>
            <option value="afterok" title="placeholder">afterok</option>
            <option value="singleton" title="placeholder">singleton</option>
          </select>
          <input id="jobid" class="hidden" type="text" placeholder="jobid">
        </div>
      </div>
    </div>

    <button type="button" class="collapsible">Log Files</button>
    <div class="content">
      <div class="row">
        <label class="cell-name">Single output file</label>
        <div class="cell-tooltip">
          <img id="one-output-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
        </div>
        <div class="cell-input">
          <input id="one-output" type="checkbox">
        </div>
      </div>
      <div class="row">
        <label class="cell-name">Output file (<tt>-o, --output</tt>) </label>
        <div class="cell-tooltip">
          <img id="output-file-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
        </div>
        <input id="output-file" class="cell-input" type="text" placeholder="slurm-%j.out">
      </div>
      <div id="err-div" class="row">
        <label class="cell-name">Error file (<tt>-e, --error</tt>) </label>
        <div class="cell-tooltip">
          <img id="error-file-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
        </div>
        <input id="error-file" class="cell-input" type="text" placeholder="slurm-%j.out">
      </div>
    </div>

    <button type="button" class="collapsible">Application</button>
    <div class="content">
      <div class="row">
        <label class="cell-name">Command (w. path and arguments) </label>
        <div class="cell-tooltip">
          <img id="executable-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
        </div>
        <input id="executable" class="cell-input executable" type="text" placeholder="./a.out">
      </div>
    </div>

    <button type="button" class="collapsible">Workspace</button>
    <div class="content">
      <div class="partition-input">
        <div class="row">
          <label class="cell-name">Allocate a workspace</label>
          <div class="cell-tooltip">
            <img id="ws-alloc-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <div class="cell-input">
            <input id="check-workspace" type="checkbox">
          </div>
        </div>
        <div class="row hidden">
          <label class="cell-name">Filesystem</label>
          <div class="cell-tooltip">
            <img id="ws-filesystem-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <select id="workspace-filesystem" class="cell-input"></select>
        </div>
        <div class="row hidden">
          <label class="cell-name">Name</label>
          <div class="cell-tooltip">
            <img id="ws-name-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <input id="ws-name" class="cell-input" type="text">
        </div>
        <div class="row hidden">
          <label class="cell-name">Duration</label>
          <div class="cell-tooltip">
            <img id="ws-duration-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <div class="cell-input">
            <input id="duration" type="text" min="1">
            <label id="duration-text" class="limits"></label>
          </div>
        </div>
        <div class="row hidden">
          <label class="cell-name">Release workspace after job</label>
          <div class="cell-tooltip">
            <img id="ws-delete-info" class="info-img" src="../misc/info.png" title="help" alt="Information sign">
          </div>
          <div class="cell-input">
            <input id="check-delete" type="checkbox">
          </div>
          <label for="check-delete">Delete after job</label>
        </div>
      </div>
      <div class="partition-info">
        <pre id="info-panel-ws" class="info-pre row hidden"></pre>
      </div>
    </div>

    <div class="output">
      <button id="generate-button" class="output" type="button">Generate</button>
      <div class="code">
        <label id="output-text" class="limits">Output requires update</label>
        <pre id="output"></pre>
      </div>
      <button id="copy-button" class="output" type="button">Copy to Clipboard</button>
      <button id="save-button" class="output" type="button">Save as File</button>
    </div>

    <script>
      // dictionary containing the limits for the different partitions
      const limitsPartition = {
        'alpha' : alpha = {
          'MaxTime': 'INFINITE',
          'DefaultTime': 480,
          'Sockets': 2,
          'CoresPerSocket': 24,
          'ThreadsPerCore': 2,
          'nodes': 37,
          'GPU': 8,
          'HTCores': 96,
          'Cores': 48,
          'MemoryPerNode': 990000,
          'MemoryPerCore': 10312
        },
        'haswell' : haswell = {
          'MaxTime': 'INFINITE',
          'DefaultTime': 480,
          'Sockets': 2,
          'CoresPerSocket': 12,
          'ThreadsPerCore': 1,
          'nodes': 609,
          'GPU': 0,
          'HTCores': 24,
          'Cores': 24,
          'MemoryPerNode': 61000,
          'MemoryPerCore': 2541
        },
        'haswell64' : haswell64 = {
          'MaxTime': 'INFINITE',
          'DefaultTime': 480,
          'Sockets': 2,
          'CoresPerSocket': 12,
          'ThreadsPerCore': 1,
          'nodes': 586,
          'GPU': 0,
          'HTCores': 24,
          'Cores': 24,
          'MemoryPerNode': 61000,
          'MemoryPerCore': 2541
        },
        'haswell256' : haswell256 = {
          'MaxTime': 'INFINITE',
          'DefaultTime': 480,
          'Sockets': 2,
          'CoresPerSocket': 12,
          'ThreadsPerCore': 1,
          'nodes': 18,
          'GPU': 0,
          'HTCores': 24,
          'Cores': 24,
          'MemoryPerNode': 254000,
          'MemoryPerCore': 10583
        },
        'interactive' : interactive = {
          'MaxTime': 480,
          'DefaultTime': 30,
          'Sockets': 2,
          'CoresPerSocket': 12,
          'ThreadsPerCore': 1,
          'nodes': 8,
          'GPU': 0,
          'HTCores': 24,
          'Cores': 24,
          'MemoryPerNode': 61000,
          'MemoryPerCore': 2541
        },
        'smp2' : smp2 = {
          'MaxTime': 'INFINITE',
          'DefaultTime': 480,
          'Sockets': 4,
          'CoresPerSocket': 14,
          'ThreadsPerCore': 1,
          'nodes': 5,
          'GPU': 0,
          'HTCores': 56,
          'Cores': 56,
          'MemoryPerNode': 2044000,
          'MemoryPerCore': 36500
        },
        'hpdlf' : hpdlf = {
          'MaxTime': 'INFINITE',
          'DefaultTime': 60,
          'Sockets': 2,
          'CoresPerSocket': 6,
          'ThreadsPerCore': 1,
          'nodes': 14,
          'GPU': 3,
          'HTCores': 12,
          'Cores': 12,
          'MemoryPerNode': 95000,
          'MemoryPerCore': 7916
        },
        'power9' : power9 = {
          'MaxTime': 'INFINITE',
          'DefaultTime': 60,
          'Sockets': 2,
          'CoresPerSocket': 22,
          'ThreadsPerCore': 4,
          'nodes': 30,
          'GPU': 6,
          'HTCores': 176,
          'Cores': 44,
          'MemoryPerNode': 254000,
          'MemoryPerCore': 1443
        },
        'romeo' : romeo = {
          'MaxTime': 'INFINITE',
          'DefaultTime': 480,
          'Sockets': 2,
          'CoresPerSocket': 64,
          'ThreadsPerCore': 2,
          'nodes': 190,
          'GPU': 0,
          'HTCores': 256,
          'Cores': 128,
          'MemoryPerNode': 505000,
          'MemoryPerCore': 1972
        },
        'romeo-interactive': {
          'MaxTime': 480,
          'DefaultTime': 10,
          'Sockets': 2,
          'CoresPerSocket': 64,
          'ThreadsPerCore': 2,
          'nodes': 2,
          'GPU': 0,
          'HTCores': 256,
          'Cores': 128,
          'MemoryPerNode': 505000,
          'MemoryPerCore': 1972
        },
        'julia' : julia = {
          'MaxTime': 'INFINITE',
          'DefaultTime': 480,
          'Sockets': 32,
          'CoresPerSocket': 28,
          'ThreadsPerCore': 1,
          'nodes': 1,
          'GPU': 0,
          'HTCores': 896,
          'Cores': 896,
          'MemoryPerNode': 48390000,
          'MemoryPerCore': 54006
        },
        'alpha' : alpha = {
          'MaxTime': 'INFINITE',
          'DefaultTime': 480,
          'Sockets': 2,
          'CoresPerSocket': 24,
          'ThreadsPerCore': 2,
          'nodes': 32,
          'GPU': 8,
          'HTCores': 96,
          'Cores': 48,
          'MemoryPerNode': 990000,
          'MemoryPerCore': 10312
        },
        'alpha-interactive': {
          'MaxTime': 'INFINITE',
          'DefaultTime': 480,
          'Sockets': 2,
          'CoresPerSocket': 24,
          'ThreadsPerCore': 2,
          'nodes': 2,
          'GPU': 8,
          'HTCores': 96,
          'Cores': 48,
          'MemoryPerNode': 990000,
          'MemoryPerCore': 10312
        }      };

      // dictionary containing the limits for the different workspaces
      const limitsWorkspace = {
        'scratch' : scratch = {
          'info' : '',
          'duration' : 100,
          'extensions' : 10
        },
        'warm_archive' : warm_archive = {
          'info' : '',
          'duration' : 365,
          'extensions' : 2
        },
        'ssd' : ssd = {
          'info' : '',
          'duration' : 30,
          'extensions' : 2
        },
        'beegfs' : beegfs = {
          'info' : '',
          'duration' : 30,
          'extensions' : 2
        }
      };

      // dictionary containing the texts and link for the info icons
      const info = {
        'job-name': {
          'text': 'Specify a name for the job allocation. The specified name will appear along with the job id number when querying running jobs on the system. (default: name of the job file)',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_job-name'
        },
        'account': {
          'text': 'Charge resources used by this job to specified project.',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_account'
        },
        'mail': {
          'text': 'Specify which user is send a email notification of state changes as defined by --mail-type.',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_mail-user'
        },
        'time': {
          'text': 'Set the total run time limit of the job allocation. When the time limit is reached, each task in each job step is sent SIGTERM followed by SIGKILL. The default time limit is the partition\'s default time limit. (currently only supports ddd-hh:mm:ss and hh:mm:ss)',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_time'
        },
        'partition': {
          'text': 'Request a specific partition for the resource allocation. If the job can use more than one partition, specify their names in a comma separate list and the one offering earliest initiation will be used with no regard given to the partition name ordering. (default: default paritition of the system)',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_partition'
        },
        'nodes': {
          'text': 'Request that number of nodes be allocated to this job.',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_nodes'
        },
        'tasks': {
          'text': 'Request that many MPI tasks (default: one task per node)',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_ntasks'
        },
        'tasks/node': {
          'text': 'Allocate that many tasks per node. If used with the --ntasks option, the --ntasks option will take precedence and the --ntasks-per-node will be treated as a maximum count of tasks per node. Meant to be used with the --nodes option.',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_ntasks-per-node'
        },
        'cpus': {
          'text': 'Request that number of processors per MPI task. This is needed for multithreaded (e.g. OpenMP) jobs; typically <N> should be equal to OMP_NUM_THREADS',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_cpus-per-task'
        },
        'nomultithread': {
          'text': '[don\'t] use extra threads with in-core multi-threading which can benefit communication intensive applications.',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_[no]multithread'
        },
        'gpus': {
          'text': 'help text for gpus',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_gpus'
        },
        'gpus/task': {
          'text': 'help text for gpus/task',
          'link': 'test'
        },
        'mem': {
          'text': 'Specify the real memory required per node.',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_output'
        },
        'reservation': {
          'text': 'Allocate resources for the job from the named reservation.',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_reservation'
        },
        'exclusive': {
          'text': 'The job allocation can not share nodes with other running job. Exclusive usage of compute nodes; you will be charged for all CPUs/cores on the node',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_exclusive'
        },
        'executable': {
          'text': 'help text for executable',
          'link': 'test'
        },
        'one-output': {
          'text': 'help text for one-output',
          'link': 'test'
        },
        'output-file': {
          'text': 'File to save all normal output (stdout) (default: slurm-%j.out)',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_output'
        },
        'error-file': {
          'text': 'File to save all error output (stderr) (default: slurm-%j.out)',
          'link': 'https://slurm.schedmd.com/sbatch.html#OPT_error'
        },
        'array': {
          'text': 'help text for array',
          'link': 'test'
        },
        'dependency': {
          'text': 'help text for dependency',
          'link': 'test'
        },
        'ws-alloc': {
          'text': 'help text for allocate workspace',
          'link': 'test'
        },
        'ws-filesystem': {
          'text': 'help text for filesystem',
          'link': 'test'
        },
        'ws-name': {
          'text': 'Valid characters for workspace names are only alphanumeric characters, -, ., and _. Workspace name has to start with an alphanumeric character.',
          'link': 'test'
        },
        'ws-duration': {
          'text': 'help text for duration',
          'link': 'test'
        },
        'ws-delete': {
          'text': 'help text for delete workspace',
          'link': 'test'
        }
      };

      // dictionary for the max values
      var maxValues = {
        'nodes' : 1,
        'tasks' : 1,
        'tasks/node' : 1,
        'cpus' : 1,
        'gpus' : 0,
        'gpus/task' : 0,
        'mem' : 1,
        'duration': 1
      };

      // dictionary for the min values
      var minValues = {
        'nodes' : 1,
        'tasks' : 1,
        'tasks/node' : 1,
        'cpus' : 1,
        'gpus' : 0,
        'gpus/task' : 0,
        'mem' : 1,
        'duration': 1
      };

      /**
       *  Function to generate the output text
       */
      var generateOutput = function() {
        let outputText = document.getElementById('output')
        outputText.innerText = '#!/bin/bash\n';
        if (document.getElementById('job-name').value !== '') {
          outputText.innerText += '\n#SBATCH --job-name=\"'
          + document.getElementById('job-name').value + '\"';
        }
        if (document.getElementById('account').value !== '') {
          outputText.innerText += '\n#SBATCH --account=\"'
          + document.getElementById('account').value + '\"';
        }
        if (document.getElementById('mail').value !== '') {
          outputText.innerText += '\n#SBATCH --mail-user='
          + document.getElementById('mail').value;
          if (document.getElementById('mail-all').checked === true) {
            outputText.innerText += '\n#SBATCH --mail-type=ALL';
          } else {
            let mailtype = ""
            let mailtype_wanted = false
            if (document.getElementById('mail-begin').checked === true) {
              mailtype_wanted = true
              mailtype = "BEGIN"
              // outputText.innerText += '\n#SBATCH --mail-type=BEGIN';
            }
            if (document.getElementById('mail-end').checked === true) {
              if (mailtype_wanted === true) {
                mailtype += ",END"
              } else {
                mailtype += "END"
              }
              mailtype_wanted = true
              // outputText.innerText += '\n#SBATCH --mail-type=END';
            }
            if (document.getElementById('mail-fail').checked === true) {
              if (mailtype_wanted === true) {
                mailtype += ",FAIL"
              } else {
                mailtype += "FAIL"
              }
              mailtype_wanted = true
              // outputText.innerText += '\n#SBATCH --mail-type=FAIL';
            }
            if (mailtype_wanted === true) {
              outputText.innerText += '\n#SBATCH --mail-type=' + mailtype
            }
          }
        }
        if (document.getElementById('timelimit').value !== '') {
          outputText.innerText += '\n#SBATCH --time='
          + document.getElementById('timelimit').value;
        } else {
          outputText.innerText += '\n#SBATCH --time='
          + limitsPartition[document.getElementById('partition').value]['DefaultTime'];
        }
        outputText.innerText += '\n#SBATCH --partition='
        + document.getElementById('partition').value;
        if (document.getElementById('nodes').value !== '') {
          outputText.innerText += '\n#SBATCH --nodes=' + document.getElementById('nodes').value;
        }
        if (document.getElementById('tasks').value !== '') {
          outputText.innerText += '\n#SBATCH --ntasks='
          + document.getElementById('tasks').value;
        }
        if (document.getElementById('tasks/node').value !== '' && document.getElementById('gpus').value === '') {
          outputText.innerText += '\n#SBATCH --ntasks-per-node='
          + document.getElementById('tasks/node').value;
        } else if (document.getElementById('tasks/node').value !== '') {
          outputText.innerText += '\n#SBATCH --mincpus='
          + document.getElementById('tasks/node').value;
        }
        if (document.getElementById('cpus').value !== '') {
          outputText.innerText += '\n#SBATCH --cpus-per-task='
          + document.getElementById('cpus').value;
        }
        if (document.getElementById('gpus').value !== '') {
          outputText.innerText += '\n#SBATCH --gres=gpu:'
          + document.getElementById('gpus').value;
        }
        if (document.getElementById('gpus/task').value !== '') {
          outputText.innerText += '\n#SBATCH --gpus-per-task='
          + document.getElementById('gpus/task').value;
        }
        if (document.getElementById('mem').value !== '') {
          outputText.innerText += '\n#SBATCH --mem-per-cpu='
          + document.getElementById('mem').value
          + document.getElementById('byte').value;
        }
        if (document.getElementById('reservation').value !== '') {
          outputText.innerText += '\n#SBATCH --reservation='
          + document.getElementById('reservation').value;
        }
        if (document.getElementById('exclusive').checked === true) {
          outputText.innerText += '\n#SBATCH --exclusive';
        }
        if (document.getElementById('nomultithread').checked === true) {
          outputText.innerText += '\n#SBATCH --hint=nomultithread';
        }
        if (document.getElementById('output-file').value !== '') {
          outputText.innerText += '\n#SBATCH --output='
          + document.getElementById('output-file').value;
        }
        if (document.getElementById('error-file').value !== '') {
          outputText.innerText += '\n#SBATCH --error='
          + document.getElementById('error-file').value;
        }
        if (document.getElementById('array').value !== '') {
          outputText.innerText += '\n#SBATCH --array='
          + document.getElementById('array').value;
        }
        if (document.getElementById('type-depend').value !== 'none') {
          outputText.innerText += '\n#SBATCH --dependency='
          + document.getElementById('type-depend').value;
          if (document.getElementById('type-depend').value !== 'singleton') {
            outputText.innerText += ':' + document.getElementById('jobid').value;
          }
        }

        outputText.innerText += '\n\n# Setup computational environment, i.e, load desired modules';
        outputText.innerText += '\n# module purge';
        outputText.innerText += '\n# module load <module name>';
        outputText.innerText += '\n\n';

        if (document.getElementById('check-workspace').checked) {
          outputText.innerText += '\n# Allocate workspace as working directory';
          outputText.innerText += '\nWSNAME='
          + document.getElementById('ws-name').value.trim() + '_${SLURM_JOB_ID}';
          outputText.innerText += '\nexport WSDIR=$(ws_allocate -F '
          + document.getElementById('workspace-filesystem').value
          + ' -n ${WSNAME}';
          if (document.getElementById('duration').value) {
            outputText.innerText += ' -d ' + document.getElementById('duration').value
          }
          outputText.innerText += ')';
          outputText.innerText += '\necho "Workspace: ${WSDIR}"';

          outputText.innerText += '\n# Check allocation';
          outputText.innerText += '\n[ -z "${WSDIR}" ] && echo "Error: Cannot allocate workspace {$WSNAME}" && exit 1';

          outputText.innerText += '\n\n# Change to workspace directory';
          outputText.innerText += '\ncd ${WSDIR}';
        }

        if (document.getElementById('check-workspace').checked && document.getElementById('check-delete').checked) {
          outputText.innerText += '\n\n# The workspace where results from multiple experiments will be saved for later analysis.';
          outputText.innerText += '\n# Remark: Make sure RESULT_WSDIR exists!';
          outputText.innerText += '\nRESULT_WSDIR="/path/to/workspace-experiments-results"'
          outputText.innerText += '\ntest -z "${RESULT_WSDIR}" && echo "Error: Cannot find workspace ${RESULT_WSDIR}" && exit 1';
        }

        if (document.getElementById('executable').value !== '') {
          outputText.innerText += '\n\n# Execute parallel application ';
          outputText.innerText += '\nsrun '
          + document.getElementById('executable').value;
        } else {
          outputText.innerText += '\n\n# Execute parallel application ';
          outputText.innerText += '\n# srun <application>';
        }

        if (document.getElementById('check-workspace').checked && document.getElementById('check-delete').checked) {
          outputText.innerText += '\n\n# Save your results to the general workspace RESULT_WSDIR (s.a.).';
          outputText.innerText += '\n# Compress results with bzip2 (which includes CRC32 checksums)';
          outputText.innerText += '\n# Remark: Assume all result and log files of interest are in the directory `results`.';
          outputText.innerText += '\nbzip2 --compress --stdout -4 "${WSDIR}/results" > ${RESULT_WSDIR}/${SLURM_JOB_ID}.bz2';
          outputText.innerText += '\nRETURN_CODE=$?';
          outputText.innerText += '\nCOMPRESSION_SUCCESS="$(if test ${RETURN_CODE} -eq 0; then echo'
                                  +' \'TRUE\'; else echo \'FALSE\'; fi)"';

          outputText.innerText += '\n\n# Clean up workspace';
          outputText.innerText += '\nif [ "TRUE" = ${COMPRESSION_SUCCESS} ]; then';
          outputText.innerText += '\n    if [ -d ${WSDIR} ] && rm -rf ${WSDIR}/*';
          outputText.innerText += '\n    # Reduce grace period to 1 day';
          outputText.innerText += '\n    ws_release -F '
                                  + document.getElementById('workspace-filesystem').value
                                  + ' ${WSNAME}';
          outputText.innerText += '\nelse'
          outputText.innerText += '\n    echo "Error with compression and writing of results"'
          outputText.innerText += '\n    echo "Please check the folder \"${WSDIR}\" for any'
                                  + ' partial(?) results.n';
          outputText.innerText += '\n    exit 1';
          outputText.innerText += '\nfi';
        }
      }

      function validateTimelimit() {
        // walltime: Check if value is set
        let elem = document.getElementById('timelimit')
        elem.style.backgroundColor = '';
        let walltime = elem.value.trim()
        if (walltime) {
          // minutes, minutes:seconds, hours:minutes:seconds
          let re_minutes = /^([0-9]+:)?[0-9]+(:[0-9]+)?$/;
          // days-hours, days-hours:minutes and days-hours:minutes:seconds
          let re_days = /^[0-9]+-[0-9]+(:[0-9]+){0,2}$/;
          if (!re_minutes.test(walltime) &&
              !re_days.test(walltime)) {
            elem.style.backgroundColor = 'rgb(255, 121, 121)';
            return false;
          }
        }
        return true;
      }

      const validateEmailString = (email) => {
        return String(email)
          .toLowerCase()
          .match(
            /^(([^<>()[\]\\.,;:\s@"]+(\.[^<>()[\]\\.,;:\s@"]+)*)|(".+"))@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\])|(([a-zA-Z\-0-9]+\.)+[a-zA-Z]{2,}))$/
          );
      };

      function validateEmail() {
        let elem = document.getElementById('mail')
        elem.style.backgroundColor = '';
        let email = elem.value.trim()
        if (document.getElementById('mail-all').checked === true
            || document.getElementById('mail-begin').checked === true
            || document.getElementById('mail-end').checked === true
            || document.getElementById('mail-fail').checked === true) {
          // if checked type but empty email or invalid email
          if (! email || !validateEmailString(email)) {
            elem.style.backgroundColor = 'rgb(255, 121, 121)';
            return false;
          }
        } else {
          // If email is given but no Type is selected
          if (email) {
            elem.style.backgroundColor = 'rgb(255, 121, 121)';
            return false;
          }
        }
        return true;
      }

      function validateArray() {
        // walltime: Check if value is set
        let elem = document.getElementById('array')
        elem.style.backgroundColor = '';
        let array = elem.value.trim()
        let reArray = /^[0-9]+(-[0-9]+)?(,[0-9]+(-[0-9]+)?)*(:[0-9]+)?(%[0-9]+)?$/;
        // If array is filled, check validity
        if (array) {
          if (!reArray.test(array)) {
            elem.style.backgroundColor = 'rgb(255, 121, 121)';
            return false;
          }
        }
        return true;
      }

      function validateWorkspace() {
        // Workspace name can consist of alphanumeric characters, -, ., and _.
        // Has to start with alphanumerical character.
        let elem = document.getElementById('ws-name')
        elem.style.backgroundColor = '';
        let ws_name = elem.value.trim()
        let re_allowed = /^[a-zA-Z0-9][a-zA-Z0-9\-\.\_]*$/;
        if (!re_allowed.test(ws_name)) {
          elem.style.backgroundColor = 'rgb(255, 121, 121)';
          return false;
        }
        return true;
      }

      function validateNumericField(field) {
        // remove all leading zeros
        let elem = document.getElementById(field);
        let elem_label = document.getElementById(field + '-text');
        elem.style.backgroundColor = '';
        elem_label.style.display = 'none';
        let val_str = elem.value.trim();
        if (val_str) {
          // Check if value is not a number
          if (isNaN(val_str)) {
            elem.style.backgroundColor = 'rgb(255, 121, 121)'
            return false;
          }
          // Ensure it is an integer
          let val_num = Number(val_str);
          if (!Number.isInteger(val_num)) {
            elem.style.backgroundColor = 'rgb(255, 121, 121)'
            return false;
          }
          let min = minValues[field];
          let max = maxValues[field];
          if (val_num < min || val_num > max) {
            elem.style.backgroundColor = 'rgb(255, 121, 121)';
            elem_label.style.display = 'inline';
            return false;
          }
        }
        // If value is empty or only spaces, its ok
        return true;
      }

      function validateNumericFieldFactory(field) {
        return function() {
          validateNumericField(field);
        }
      }

      /**
       * Proof if all values are correct, if yes it prints the output, else it highlights incorrect values
       */
      var validateAllFields = function() {
        let allFieldsValid = true;
        if (!validateTimelimit()) {
          allFieldsValid = false;
        }
        if (!validateEmail()) {
          allFieldsValid = false;
        }
        if (!validateArray()) {
          allFieldsValid = false;
        }

        // Build list of numerical fields
        let fields = ['nodes', 'tasks', 'tasks/node', 'cpus', 'gpus', 'gpus/task', 'mem'];
        for (let index = 0; index < fields.length; index++) {
          if (!validateNumericField(fields[index])) {
            allFieldsValid = false;
          }
        }
        //fields.forEach(field => validateNumericField(field))

        if (document.getElementById('check-workspace').checked === true) {
          if (!validateNumericField('duration')) {
            allFieldsValid = false;
          }
          if (!validateWorkspace()) {
            allFieldsValid = false;
          }
        }

        if (allFieldsValid === true) {
          document.getElementById('output-text').style.display = 'none';
          generateOutput();
        } else {
          document.getElementById('output-text').style.display = 'block';
        }
      }

      // copy to clipboard
      document.getElementById('copy-button').addEventListener('click', function () {
        let code = document.getElementById('output');

        // select the text
        let range = document.createRange();
        range.selectNodeContents(code);
        let selection = window.getSelection();
        selection.removeAllRanges();
        selection.addRange(range);

        // copy to clipboard
        navigator.clipboard.writeText(code.innerText);

        // remove selection
        selection.removeAllRanges();
      });

      // save as file
      document.getElementById('save-button').addEventListener('click', function () {
        // create file informations
        var file = new Blob([document.getElementById('output').innerText], {type: 'text/plain'});
        // create url and link
        var url = URL.createObjectURL(file);
        var a = document.createElement('a');
        a.href = url;
        if (document.getElementById('job-name').value !== '') {
          a.download = document.getElementById('job-name').value + ".sh"
        }
        else {
          a.download = 'sbatchfile.sh';
        }
        document.body.appendChild(a);
        // activate the link
        a.click();
        // remove link after some time
        setTimeout(function() {
            document.body.removeChild(a);
            window.URL.revokeObjectURL(url);
        }, 0);
      });

      // remove the error field, if checkbox is active
      document.getElementById('one-output').addEventListener('change', function () {
        if (document.getElementById('one-output').checked) {
          document.getElementById('err-div').style.display = 'none';
          document.getElementById('err-div').value = '';
        } else {
          document.getElementById('err-div').style.display = '';
        }
      });

      // show workspace div, if checkbox is active
      document.getElementById('check-workspace').addEventListener('change', function () {
        let hidden = document.getElementsByClassName('row hidden');
        [].forEach.call(hidden, function (row) {
          if (document.getElementById('check-workspace').checked === true) {
            row.style.cssText = 'display:flex !important';
          } else {
            row.style.cssText = 'display:none !important';
          }
        })
      });

      /**
       * Function to fill the information panel about the currently selected partition
       */
      var fillPartitionInfo = function() {
        let panelText = document.getElementById('info-panel-partition');
        let partitionLimits = limitsPartition[document.getElementById('partition').value];

        panelText.innerText = "Partition " + document.getElementById('partition').value;
        if (partitionLimits['Description']) {
          panelText.innerText += ": " + partitionLimits['Description'];
        }
        panelText.innerText += '\nNodes: ' + partitionLimits['nodes'];
        panelText.innerText += '\nCores per node: ' + partitionLimits['Cores'];
        panelText.innerText += '\nHyper threading: ' + partitionLimits['HTCores'];
        panelText.innerText += '\nRAM per core: ' + partitionLimits['MemoryPerCore'] + ' MB';
        panelText.innerText += '\nRAM per node: ' + partitionLimits['MemoryPerNode'] + ' MB';
        panelText.innerText += '\nGPUs per node: ' + partitionLimits['GPU'];
      }

      /**
       * Function to fill the information panel about the currently selected partition
       */
      var fillWorkspaceInfo = function() {
        let panelText = document.getElementById('info-panel-ws');
        let workspaceLimits = limitsWorkspace[document.getElementById('workspace-filesystem').value];

        panelText.innerText = workspaceLimits['info'];
        panelText.innerText += '\nDuration: ' + workspaceLimits['duration'];
        panelText.innerText += '\nExtensions: ' + workspaceLimits['extensions'];
      }

      /**
       * Function to fill the tooltip about the partitions
       */
      var fillTooltips = function() {
        for (const [key, value] of Object.entries(limitsPartition)) {
          let panelText = document.getElementById(key);
          let partitionLimits = limitsPartition[key];

          panelText.title = partitionLimits['Description'];
          panelText.title += '\nNodes: ' + partitionLimits['nodes'];
          panelText.title += '\nCores per node: ' + partitionLimits['Cores'];
          panelText.title += '\nHyper threading: ' + partitionLimits['HTCores'];
          panelText.title += '\nRAM per core: ' + partitionLimits['MemoryPerCore'] + ' MB';
          panelText.title += '\nRAM per node: ' + partitionLimits['MemoryPerNode'] + ' MB';
          panelText.title += '\nGPUs per node: ' + partitionLimits['GPU'];
        }
      }

      /**
       * Function to fill the tooltip about the workspaces
       */
       var fillTooltipsWorkspace = function() {
        for (const [key, value] of Object.entries(limitsWorkspace)) {
          let panelText = document.getElementById(key);
          let partitionLimits = limitsWorkspace[key];

          panelText.title = partitionLimits['info'];
          panelText.title += '\nDuration: ' + partitionLimits['duration'];
          panelText.title += '\nExtensions: ' + partitionLimits['extensions'];
        }
      }

      /**
       * Function to fill the tooltip about limits of the field
       *
       * @param {string} field The id for the field to set the tooltip
       */
      var setTooltips = function(field) {
        let panelText = document.getElementById(field);

        panelText.title = 'Limits by current setting:';
        panelText.title += '\nmin: ' + minValues[field];
        panelText.title += '\nmax: ' + maxValues[field];
        panelText.title += '\nEmpty the field if unneeded';

        // set limit labels
        let limitText = document.getElementById(field + '-text');
        limitText.innerText = 'min: ' + minValues[field];
        limitText.innerText += ' max: ' + maxValues[field];
      }

      /**
       * Get the value of a field, or its maximum
       */
      var getValue = function(field, type) {
        let number = 0;
        // get field value
        rawNodes = document.getElementById(field).value;
        // set to maximum if value is undefined or out of range
        if (
          rawNodes !== ''
          && Number(rawNodes) >= minValues[field]
          && Number(rawNodes) <= maxValues[field]
        ) {
          number = Number(document.getElementById(field).value);
        } else {
          number = type === 'min' ? minValues[field] : maxValues[field];
        }
        return number;
      }

      /**
       * Set the limits for the nodes field
       */
       var setLimitNodes = function() {
        // get partition limits from dictionary
        let partitionLimits = limitsPartition[document.getElementById('partition').value];

        // set max for nodes
        maxValues['nodes'] = partitionLimits['nodes'];

        // set min for nodes
        if (
          document.getElementById('tasks').value !== ''
          && document.getElementById('tasks/node').value !== ''
          ) {
          let tasks = getValue('tasks', 'min');
          let tasksPerNode = getValue('tasks/node', 'min');
          minValues['nodes'] = Math.ceil(tasks / tasksPerNode);
        } else {
          minValues['nodes'] = 1;
        }

        // set min for nodes
        setTooltips('nodes');
      }

      /**
       * Set the limits for the tasks field
       */
      var setLimitTasks = function() {
        // get partition limits from dictionary
        let partitionLimits = limitsPartition[document.getElementById('partition').value];

        // set max
        let nodes = getValue('nodes', 'max');
        let taskPerNode = getValue('tasks/node', 'max');
        // multithreading
        let limit = document.getElementById('nomultithread').checked === true ? 'Cores' : 'HTCores';
        maxValues['tasks'] = nodes * partitionLimits[limit];
        // limit if nodes and tasks/node
        if (
          document.getElementById('tasks/node').value !== ''
          && document.getElementById('nodes').value !== ''
        ) {
          maxValues['tasks'] = taskPerNode * nodes;
        } else {
          minValues['tasks/node'] = 1;
        }

        // set min
        nodes = getValue('nodes', 'min');
        minValues['tasks'] = nodes;
        setTooltips('tasks');
      }

      /**
       * Set the limits for tasks per node field
       */
      var setLimitTasksPerNode = function() {
        // get partition limits from dictionary
        let partitionLimits = limitsPartition[document.getElementById('partition').value];

        // set max
        let nodes = getValue('nodes', 'max');
        let tasks = getValue('tasks', 'max');

        // multithreading
        let limit = document.getElementById('nomultithread').checked === true ? 'Cores' : 'HTCores';
        maxValues['tasks/node'] = partitionLimits[limit];
        //set min
        nodes = getValue('nodes', 'min');
        tasks = getValue('tasks', 'min');
        if (
          document.getElementById('tasks').value !== ''
          && document.getElementById('nodes').value !== ''
        ) {
          minValues['tasks/node'] = Math.ceil(tasks / nodes);
        } else {
          minValues['tasks/node'] = 1;
        }

        // set min and max for --mincpus if gpus are allocated
        if (
          document.getElementById('gpus').value !== ''
          && document.getElementById('nodes').value !== ''
          && document.getElementById('tasks').value !== ''
        ) {
          minValues['tasks/node'] = Math.ceil(tasks / nodes);
          nodes = getValue('nodes', 'max');
          tasks = getValue('tasks', 'max');
          maxValues['tasks/node'] = Math.floor(tasks / nodes);
        }
        // set tooltips
        setTooltips('tasks/node');
      }

      /**
       * Set the limits for cpu per task field
       */
       var setLimitCpu = function() {
        // get partition limits from dictionary
        let partitionLimits = limitsPartition[document.getElementById('partition').value];

        // set max
        // multithreading
        let limit = document.getElementById('nomultithread').checked === true ? 'Cores' : 'HTCores';
        let nodes = getValue('nodes', 'max');
        let tasks = getValue('tasks', 'max');
        let tasksPerNode = getValue('tasks/node', 'max');
        let maxValue = [partitionLimits[limit]];
        maxValue.push(Math.floor(partitionLimits[limit] / Math.ceil(tasks / nodes)));
        maxValue.push(Math.floor(partitionLimits[limit] / tasksPerNode));
        maxValues['cpus'] = Math.min.apply(null, maxValue);
        // set max for cpus if gpus are set
        if (
          document.getElementById('gpus').value !== ''
          && document.getElementById('gpus').value !== '0'
          && document.getElementById('exclusive').checked !== true
        ) {
          let gpus = getValue('gpus', 'max');
          maxValues[field] = Math.floor(gpus / partitionLimits['GPU'] * partitionLimits[limit]);
        }
        //set min
        minValues['cpus'] = 1;
        // set tooltips
        setTooltips('cpus');
      }

      /**
       * Set the limits for gpu per node field
       */
       var setLimitGpu = function() {
        // get partition limits from dictionary
        let partitionLimits = limitsPartition[document.getElementById('partition').value];

        // set max for gpus
        maxValues['gpus'] = partitionLimits['GPU'];
        setTooltips('gpus');
        //set min
        if (document.getElementById('gpus/task').value !== '') {
          let tasks = getValue('tasks', 'min');
          let nodes = getValue('nodes', 'min');
          let gpusPerTask = getValue('gpus/task', 'min');
          minValues['gpus'] = Math.ceil(tasks / nodes * gpusPerTask);
        } else {
          minValues['gpus'] = 0;
        }
        // set tooltips
        setTooltips('gpus');
      }

      /**
       * Set the limits for the gpus per task field
       */
      var setLimitGpuPerTask = function() {
        // get partition limits from dictionary
        let partitionLimits = limitsPartition[document.getElementById('partition').value];

        // get value from base
        let tasks = getValue('tasks', 'max');
        let tasksNode = getValue('tasks/node', 'max');
        let nodes = getValue('nodes', 'max');

        // set new max
        let maxValue = [partitionLimits['GPU']];
        if (tasks !== '') {
          maxValue.push(Math.floor(partitionLimits['GPU'] / Math.ceil(Number(tasks) / Number(nodes))));
        }
        if (tasksNode !== '') {
          maxValue.push(Math.floor(partitionLimits['GPU'] / Number(document.getElementById('tasks/node').value)));
        }
        maxValues['gpus/task'] = Math.min.apply(null, maxValue);
        // set tooltips for new limits
        setTooltips('gpus/task');
      }

      /**
       * Update the max for memory per cpu values
       */
      var setLimitMem = function() {
        // get partition limits from dictionary
        let partitionLimits = limitsPartition[document.getElementById('partition').value];
        maxValues['mem'] = partitionLimits['MemoryPerCore'];
        if (document.getElementById('nomultithread').checked === true) {
          maxValues['mem'] *= partitionLimits['ThreadsPerCore'];
        }
        if (document.getElementById('byte').value === 'G') {
          maxValues['mem'] = Math.floor(maxValues['mem'] / 1024);
        }
        setTooltips('mem');
      }

      /**
       * Set the limits for the duration field
       */
      var setLimitDuration = function() {
        // get partition limits from dictionary
        let workspaceLimits = limitsWorkspace[document.getElementById('workspace-filesystem').value];
        // set new max
        maxValues['duration'] = workspaceLimits['duration'];

        //set min
        // get days and hours from walltime
        let reArray = /^(([0-9]{1,3})-)?([0-9]{2}):([0-9]{2}):([0-9]{2})$/;
        let match = reArray.exec(document.getElementById('timelimit').value);
        if (match === null) {
          setTooltips('duration');
          return;
        }
        // if days are defined or not
        if (match[2]) {
          minValues['duration'] = Number(match[2]);
        }
        minValues['duration'] += Math.ceil(Number(match[3]) / 24);
        if ((Number(match[4]) !== 0 || Number(match[5]) !== 0) && Number(match[3]) % 24 === 0) {
          minValues['duration'] += 1;
        }
        // set tooltips for new limits
        setTooltips('duration');
      }

      /**
       * Update the value und max for CPU and GPU values
       */
      var LimitChange = function() {
        setLimitNodes();
        setLimitTasks();
        setLimitTasksPerNode();
        setLimitCpu();
        if (document.getElementById('div-gpu').style.display !== 'none') {
          setLimitGpu();
          setLimitGpuPerTask();
        }
        setLimitMem();
        setLimitDuration();
      }

      /**
       * Function to trigger updates, if parttion was changed
       */
      var partitionLimitChange = function() {
        // get partition limits from dictionary
        let partitionLimits = limitsPartition[document.getElementById('partition').value];
        // hide the GPU field, if partition do not have GPUs
        if (partitionLimits['GPU'] === 0) {
          document.getElementById('div-gpu').style.display = 'none';
          document.getElementById('div-gpu/task').style.display = 'none';
          document.getElementById('gpus').value = '';
          document.getElementById('gpus/task').value = '';
        } else {
          document.getElementById('div-gpu').style.display = '';
          document.getElementById('div-gpu/task').style.display = '';
        }
        // hide the multithreading field if it isnt supported
        if (partitionLimits['ThreadsPerCore'] === 1) {
          document.getElementById('div-thread').style.display = 'none';
          document.getElementById('nomultithread').checked = false;
        } else {
          document.getElementById('div-thread').style.display = '';
        }
        // update other values
        LimitChange();
      }

      // set up event listeners, if field change
      document.getElementById('partition').addEventListener('change', partitionLimitChange);
      document.getElementById('partition').addEventListener('change', fillPartitionInfo);
      document.getElementById('nodes').addEventListener('change', LimitChange);
      document.getElementById('tasks').addEventListener('change', LimitChange);
      document.getElementById('tasks/node').addEventListener('change', LimitChange);
      document.getElementById('cpus').addEventListener('change', LimitChange);
      document.getElementById('gpus').addEventListener('change', LimitChange);
      document.getElementById('gpus/task').addEventListener('change', LimitChange);
      document.getElementById('mem').addEventListener('change', LimitChange);
      document.getElementById('byte').addEventListener('change', setLimitMem);
      document.getElementById('exclusive').addEventListener('change', LimitChange);
      document.getElementById('nomultithread').addEventListener('change', LimitChange);
      document.getElementById('workspace-filesystem').addEventListener('change', setLimitDuration);
      document.getElementById('workspace-filesystem').addEventListener('change', fillWorkspaceInfo);
      // Set up field validator events
      document.getElementById('nodes').addEventListener('change', validateNumericFieldFactory('nodes'));
      document.getElementById('tasks').addEventListener('change', validateNumericFieldFactory('tasks'));
      document.getElementById('tasks/node').addEventListener('change', validateNumericFieldFactory('tasks/node'));
      document.getElementById('cpus').addEventListener('change', validateNumericFieldFactory('cpus'));
      document.getElementById('gpus').addEventListener('change', validateNumericFieldFactory('gpus'));
      document.getElementById('gpus/task').addEventListener('change', validateNumericFieldFactory('gpus/task'));
      document.getElementById('mem').addEventListener('change', validateNumericFieldFactory('mem'));
      document.getElementById('array').addEventListener('change', validateArray);
      document.getElementById('ws-name').addEventListener('change', validateWorkspace);
      document.getElementById('timelimit').addEventListener('change', validateTimelimit);
      // We do not validate Email on change since the validateEmail checks if mail is correct + a box is selected.
      // In the user workflow, this would trigger a red field when they enter a correct Email, but have not selected a checkbox
      // To avoid confusion, we only check on generate.

      // hide jobid field if unneeded
      document.getElementById('type-depend').addEventListener('change', function() {
        if (
          document.getElementById('type-depend').value === 'none' ||
          document.getElementById('type-depend').value === 'singleton'
        ) {
          document.getElementById('jobid').style.cssText = 'display:none !important';
        } else {
          document.getElementById('jobid').style.cssText = 'display:inline !important';
        }
      });

      document.getElementById('generate-button').addEventListener('click', validateAllFields);

      // initialize webpage

      // set up the collapsible fields
      let colls = document.getElementsByClassName('collapsible');
      [].forEach.call(colls, function (coll) {
        // add event listeners
        coll.addEventListener('click', function() {
          this.classList.toggle('active');
          let content = this.nextElementSibling;
          if (content.style.display === 'block') {
            content.style.display = 'none';
          } else {
            content.style.display = 'block';
          }
        });

        // extended at beginning
        let content = coll.nextElementSibling;
        content.style.display = 'block';
        coll.classList.toggle('active');
      })

      // set up of partition options
      let select = document.getElementById('partition');

      for (const [key, value] of Object.entries(limitsPartition)) {
        let option = document.createElement('option');
        option.id = key;
        option.value = key;
        option.innerText = key;
        if (key === 'haswell64') {
          option.selected = 'selected';
        }
        select.appendChild(option);
      }

      // set up of workspace options
      select = document.getElementById('workspace-filesystem');

      for (const [key, value] of Object.entries(limitsWorkspace)) {
        let option = document.createElement('option');
        option.id = key;
        option.value = key;
        option.innerText = key;
        if (key === 'scratch') {
          option.selected = 'selected';
        }
        select.appendChild(option);
      }

      // set up info texts
      for (const [key, value] of Object.entries(info)) {
        document.getElementById(key + '-info').title = value['text'];
      }

      // initialize UI
      partitionLimitChange();
      fillPartitionInfo();
      fillWorkspaceInfo();
      fillTooltips();
      fillTooltipsWorkspace();
    </script>
  </body>
</html>


# Slurm Resource Limits

There is no such thing as free lunch at ZIH systems. Since compute nodes are operated in multi-user
node by default, jobs of several users can run at the same time at the very same node sharing
resources, like memory (but not CPU). On the other hand, a higher throughput can be achieved by
smaller jobs. Thus, restrictions w.r.t. [memory](#memory-limits) and
[runtime limits](#runtime-limits) have to be respected when submitting jobs.

## Runtime Limits

!!! warning "Runtime limits on login nodes"

    There is a time limit of 600 seconds set for processes on login nodes. Each process running
    longer than this time limit is automatically killed. The login nodes are shared ressources
    between all users of ZIH system and thus, need to be available and cannot be used for productive
    runs.

    ```
    CPU time limit exceeded
    ```

    Please submit extensive application runs to the compute nodes using the [batch system](slurm.md).

!!! note "Runtime limits are enforced."

    A job is canceled as soon as it exceeds its requested limit. Currently, the maximum run time
    limit is 7 days.

Shorter jobs come with multiple advantages:

- lower risk of loss of computing time,
- shorter waiting time for scheduling,
- higher job fluctuation; thus, jobs with high priorities may start faster.

To bring down the percentage of long running jobs we restrict the number of cores with jobs longer
than 2 days to approximately 50% and with jobs longer than 24 to 75% of the total number of cores.
(These numbers are subject to change.) As best practice we advise a run time of about 8h.

!!! hint "Please always try to make a good estimation of your needed time limit."

    For this, you can use a command line like this to compare the requested timelimit with the
    elapsed time for your completed jobs that started after a given date:

    ```console
    marie@login$ sacct -X -S 2021-01-01 -E now --format=start,JobID,jobname,elapsed,timelimit -s COMPLETED
    ```

Instead of running one long job, you should split it up into a chain job. Even applications that are
not capable of checkpoint/restart can be adapted. Please refer to the section
[Checkpoint/Restart](../jobs_and_resources/checkpoint_restart.md) for further documentation.

## Memory Limits

!!! note "Memory limits are enforced."

    Jobs which exceed their per-node memory limit are killed automatically by the batch system.

Memory requirements for your job can be specified via the `sbatch/srun` parameters:

`--mem-per-cpu=<MB>` or `--mem=<MB>` (which is "memory per node"). The **default limit** regardless
of the partition it runs on is quite low at **300 MB** per CPU. If you need more memory, you need
to request it.

ZIH systems comprise different sets of nodes with different amount of installed memory which affect
where your job may be run. To achieve the shortest possible waiting time for your jobs, you should
be aware of the limits shown in the
[Slurm resource limits table](#slurm-resource-limits-table).

## Slurm Resource Limits Table

The physical installed memory might differ from the amount available for Slurm jobs. One reason are
so-called diskless compute nodes, i.e., nodes without additional local drive. At these nodes, the
operating system and other components reside in the main memory, lowering the available memory for
jobs. The reserved amount of memory for the system operation might vary slightly over time. The
following table depicts the resource limits for [all our HPC systems](hardware_overview.md).

| HPC System | Nodes | # Nodes | Cores per Node | Threads per Core | Memory per Node [in MB] | Memory per (SMT) Core [in MB] | GPUs per Node | Cores per GPU | Job Max Time [in days] |
|:-----------|:------|--------:|---------------:|-----------------:|------------------------:|------------------------------:|--------------:|--------------:|-------------:|
| [`Capella`](hardware_overview.md#capella)               | `c[1-144].capella`     | 144 | 56  | 1 | 768,000    | 13,438 | 4 | 14 | 1 |
| [`Barnard`](hardware_overview.md#barnard)               | `n[1001-1630].barnard` | 630 | 104 | 2 | 515,000    | 4,951  | - | - | unlimited |
| [`Alpha Centauri`](hardware_overview.md#alpha-centauri) | `i[8001-8037].alpha`   | 37  | 48  | 2 | 990,000    | 10,312 | 8 | 6 | unlimited |
| [`Julia`](hardware_overview.md#julia)                   | `julia`                | 1   | 896 | 1 | 48,390,000 | 54,006 | - | - | unlimited |
| [`Romeo`](hardware_overview.md#romeo)                   | `i[7001-7186].romeo`   | 186 | 128 | 2 | 505,000    | 1,972  | - | - | unlimited |
| [`Power9`](hardware_overview.md#power9)                 | `ml[1-29].power9`      | 29  | 44  | 4 | 254,000    | 1,443  | 6 | - | unlimited |
{: summary="Slurm resource limits table" align="bottom"}

All HPC systems have Simultaneous Multithreading (SMT) enabled. You request for this
additional threads using the Slurm option `--hint=multithread` or by setting the environment
variable `SLURM_HINT=multithread`. Besides the usage of the threads to speed up the computations,
the memory of the other threads is allocated implicitly, too, and you will always get
`Memory per Core`*`number of threads` as memory pledge.


# Quick Start

This page will give new users guidance through the steps needed to submit a High Performance
Computing (HPC) job:

* Applying for the ZIH HPC login
* Accessing the ZIH HPC systems
* Transferring code/data to ZIH HPC systems
* Accessing software
* Running a parallel HPC job

## Introductory Instructions

The ZIH HPC systems are Linux systems (as most HPC systems). Basic Linux knowledge will
be needed. Being familiar with this [collection](https://hpc-wiki.info/hpc/Shell)
of the most important Linux commands is helpful.

To work on the ZIH HPC systems and to follow the instructions on this page as well as other
compendium pages, it is important to be familiar with the
[basic terminology](https://hpc-wiki.info/hpc/HPC-Dictionary) in HPC such as
[SSH](https://hpc-wiki.info/hpc/SSH), [cluster](https://hpc-wiki.info/hpc/HPC-Dictionary#Cluster),
[login node](https://hpc-wiki.info/hpc/HPC-Dictionary#Login_Node),
[compute node](https://hpc-wiki.info/hpc/HPC-Dictionary#Backend_Node),
[local and shared filesystem](https://hpc-wiki.info/hpc/HPC-Dictionary#File_System),
[command line (CLI) or shell](https://hpc-wiki.info/hpc/Shell).

If you are new to HPC, we recommend visiting the introductory article about HPC at
[https://hpc-wiki.info/hpc/Getting_Started](https://hpc-wiki.info/hpc/Getting_Started).

Throughout the compendium, `marie@login` is used as an indication of working on the ZIH HPC command
line and `marie@local` as working on your local machine's command line. `marie` stands-in for your
username.

## Obtaining Access

A ZIH HPC login is needed to use the systems. It is different from the ZIH login (which
members of the TU Dresden have), but has the same credentials. Apply for it via the
[HPC login application form](https://selfservice.zih.tu-dresden.de/index.php/hpclogin/noLogin).

Since HPC is structured in projects, there are two possibilities to work on the ZIH HPC systems:

* Creating a
  [new project](https://tu-dresden.de/zih/hochleistungsrechnen/zugang/projektantrag#section-3)
* Joining an existing project: e.g. new researchers in an existing project, students in projects for
teaching purposes. The details will be provided to you by the project administrator.

A HPC project on the ZIH HPC systems includes: a project directory, project group, project members
(at least admin and manager), and resource quotas for compute time (CPU/GPU hours) and storage.

It is essential to grant appropriate file permissions so that newly added users can access a
project appropriately.

## Accessing ZIH HPC Systems

ZIH provides five homogeneous compute systems, called clusters. These can only be accessed
within the TU Dresden campus networks. Access from outside is possible by establishing a
[VPN connection](https://tu-dresden.de/zih/dienste/service-katalog/arbeitsumgebung/zugang_datennetz/vpn#section-4).
Each of these clusters can be accessed in the three ways described below, depending on the user's
needs and previous knowledge:

* [JupyterHub](../access/jupyterhub.md): browser based connection, easiest way for beginners
* [SSH connection](../access/ssh_login.md) (command line/terminal/console): "classical" connection,
command line knowledge is required
* [Desktop Visualization](../access/desktop_cloud_visualization.md),
  [Graphical User Interfaces (GUIs)](../access/graphical_applications_with_webvnc.md) and similar:
  e.g. commercial software such as Ansys, LS-DYNA (are not covered here).

Next, the mentioned access methods are described step by step.

### JupyterHub

1. Access JupyterHub at
[https://jupyterhub.hpc.tu-dresden.de](https://jupyterhub.hpc.tu-dresden.de).
1. Log in with your ZIH-login credentials.
1. Choose a profile (system and resources):
![Simple form](misc/jupyterhub-spawner-options.jpg)
{: align="center"}
1. You will see the following - wait until resources are allocated:
![Spawning](misc/jupyterhub-spawning.jpg)
1. Once JupyterLab is loaded, you will see all available kernels on the system you chose,
categorized by `Notebook`, `Console` and `Other`, with which you can start right away. If you miss
software packages that you wish to use, you can
[build your own environment and jupyter kernel](../access/jupyterhub_custom_environments.md).
Note that you will now be working in your home directory as opposed to a specific workspace
(see [Data Management and Data Transfer](#data-transfer-and-data-management)
section below for more details).

!!! caution "Stopping your Jupyter session"

    Once you are done, you have to explicitly stop the Jupyter session by clicking
    `File` &#8594 `Hub Control Panel` &#8594 `Stop My Server`.
    Otherwise it will run for the amount of hours you chose with the spawning profile.
    Logging out will not stop your server.

Explore the [JupyterHub](../access/jupyterhub.md) page for more information.

### SSH Connection (Command Line)

The more "classical" way to work with HPC is based on the command line. After following
the instructions below, you will be on one of the login nodes.
This is the starting point for many tasks such as launching jobs and doing data management.

!!! hint "Using SSH key pair"

    We recommend to create an SSH key pair by following the
    [instructions here](../access/ssh_login.md#before-your-first-connection).
    Using an SSH key pair is beneficial for security reasons, although it is not necessary to work
    with ZIH HPC systems.

=== "Windows 10 and higher/Mac/Linux users"

    Windows users might need to install [Windows Terminal](https://www.microsoft.com/en-us/p/windows-terminal/9n0dx20hk701?activetab=pivot:overviewtab).

    1. Open a terminal/shell/console and type in
    ```console
    marie@local$ ssh marie@login2.barnard.hpc.tu-dresden.de
    ```

    1. After typing in your password, you end up seeing something like the following image.

    ![Successful SSH login](misc/ssh-success-login.png)
    {: align="center"}

=== "Users of older versions of Windows"

    Install and set up [MobaXTerm](../access/ssh_mobaxterm.md) or [PuTTY](../access/ssh_putty.md).

For more information explore the [access compendium page](../access/ssh_login.md).
[Configuring default parameters](../access/ssh_login.md#configuring-default-parameters-for-ssh)
makes connecting more comfortable.

## Data Transfer and Data Management

First, it is shown how to create a workspace, then how to transfer data within and to/from the ZIH
HPC system. Also keep in mind to set the file permissions when collaborating with other researchers.

### Create a Workspace

There are different places for storing your data on ZIH HPC systems, called [Filesystems](../data_lifecycle/file_systems.md).
You need to create a [workspace](../data_lifecycle/workspaces.md) for your data on one of these
(see example below).

The filesystems have different [properties](../data_lifecycle/file_systems.md) (available space,
storage time limit, permission rights). Therefore, choose the one that fits your project best.
To start we recommend the Lustre filesystem **horse**.

!!! example "Creating a workspace on Lustre filesystem horse"

    The following command creates a workspace

    ```console
    marie@login$ ws_allocate -F horse -r 7 -m marie@tu-dresden.de -n number_crunch -d 90
    Info: creating workspace.
    /data/horse/ws/marie-number_crunch
    remaining extensions  : 10
    remaining time in days: 90
    ```

    To explain:

    - `ws_allocate` - command to allocate
    - `-F horse` - on the horse filesystem
    - `-r 7 -m marie@tu-dresden.de` - send a reminder to `marie@tu-dresden.de` 7 days before expiration
    - `-n number_crunch` - workspace name
    - `-d 90` - a life time of 90 days

    The path to this workspace is `/data/horse/ws/marie-number_crunch`. You will need it when
    transferring data or running jobs.

Find more [information on workspaces in the compendium](../data_lifecycle/workspaces.md).

### Transferring Data *Within* ZIH HPC Systems

The approach depends on the data volume: up to 100 MB or above.

???+ example "`cp`/`mv` for small data (up to 100 MB)"

    Use the command `cp` to copy the file `example.R` from your ZIH home directory to a workspace:

     ```console
     marie@login$ cp /home/marie/example.R /data/horse/ws/marie-number_crunch
     ```

    Analogously use command `mv` to move a file.

    Find more examples for the `cp` command on [bropages.org](http://bropages.org/cp) or use
    manual pages with `man cp`.

???+ example "`dtcp`/`dtmv` for medium to large data (above 100 MB)"

    Use the command `dtcp` to copy the directory `/walrus/ws/large-dataset` from one
    filesystem location to another:

    ```console
    marie@login$ dtcp -r /walrus/ws/large-dataset /data/horse/ws/marie-number_crunch/data
    ```
    Analogously use the command `dtmv` to move a file or folder.

    More details on the [datamover](../data_transfer/datamover.md) are available in the data
    transfer section.

### Transferring Data *To/From* ZIH HPC Systems

???+ example "`scp` for transferring data to ZIH HPC systems"

    Copy the file `example.R` from your local machine to a workspace on the ZIH systems:

    ```console
    marie@local$ scp /home/marie/Documents/example.R marie@dataport1.hpc.tu-dresden.de:/data/horse/ws/marie-number_crunch/
    Password:
    example.R                                                     100%  312    32.2KB/s   00:00
    ```

    Note, the target path contains `dataport1.hpc.tu-dresden.de`, which is one of the
    so called [dataport nodes](../data_transfer/dataport_nodes.md) that allows for data transfer
    from/to the outside.

???+ example "`scp` to transfer data from ZIH HPC systems to local machine"

    Copy the file `results.csv` from a workspace on the ZIH HPC systems to your local machine:

    ```console
    marie@local$ scp marie@dataport1.hpc.tu-dresden.de:/data/horse/ws/marie-number_crunch/results.csv /home/marie/Documents/
    ```

    Feel free to explore further [examples](http://bropages.org/scp) of the `scp` command
    and possibilities of the [dataport nodes](../data_transfer/dataport_nodes.md).

!!! caution "Terabytes of data"

    If you are planning to move terabytes or even more from an outside machine into ZIH systems,
    please contact the ZIH [HPC support](mailto:hpc-support@tu-dresden.de) in advance.

### Permission Rights

Whenever working within a collaborative environment, take care of the file permissions.
Esp. after creating and transferring data, file permission configuration might be necessary.

**By default, workspaces are accessible only for the user who created the workspace.**
Files created by a user in the project directory have read-only access for other group members
by default. Therefore, the correct file permissions must be configured (using `chmod`
and `chgrp`) for all files in the project home and the workspaces that should be fully
accessible (read, write, execute) to your collaborator group.
Please refer to an [overview on users and permissions](https://hpc-wiki.info/hpc/Introduction_to_Linux_in_HPC/Users_and_permissions)
in Linux.

??? example "Checking and changing file permissions"

    The following example checks for file permissions (`ls -la`) of the file dataset.csv and adds
    permissions for write access for the group (`chmod g+w`).

    ```console
    marie@login$ ls -la /data/horse/ws/marie-training-data/dataset.csv # list file permissions
    -rw-r--r-- 1 marie p_number_crunch 0 12. Jan 15:11 /data/horse/ws/marie-training-data/dataset.csv

    marie@login$ chmod g+w /data/horse/ws/marie-training-data/dataset.csv # add write permissions

    marie@login$ ls -la /data/horse/ws/marie-training-data/dataset.csv # list file permissions again
    -rw-rw-r-- 1 marie p_number_crunch 0 12. Jan 15:11 /data/horse/ws/marie-training-data/dataset.csv
    ```

??? hint "GUI-based data management"

    - Transferring data and managing file permissions for smaller amounts of data can be handled
    by SSH clients.
    - More so for Linux-based systems, `sshfs` (a command-line tool for safely mounting a remote
    folder from a server to a local machine) can be used to mount user home, project home or
    workspaces within the local folder structure.
    Data can be transferred directly with drag and drop in your local file explorer.
    Moreover, this approach makes it possible to edit files with your common editors and tools on
    the local machine.
    - Windows users can use [SFTP Drive](https://www.nsoftware.com/sftp/drive/) utility, to mount
    remote filesystems as Windows drives.

## Software Environment

The [software](../software/overview.md) on the ZIH HPC systems is not installed system-wide,
but is provided within so-called [modules](../software/modules.md).
In order to use specific software you need to "load" the respective module.
This modifies the current environment (so only for the current user in the current session)
such that the software becomes available.

!!! note

    Different clusters (HPC systems) have different software or might have different versions of
    the same available software. See [software](../software/overview.md) for more details.

Use the command `module spider <software>` to check all available versions of a software that is
available on the one specific system you are currently on:

```console
marie@login$ module spider Python
--------------------------------------------------------------------------------------------------------------------------------
  Python:
--------------------------------------------------------------------------------------------------------------------------------
    Description:
      Python is a programming language that lets you work more quickly and integrate your systems more effectively.

     Versions:
        Python/2.7.14-foss-2018a
        [...]
        Python/3.8.6
        Python/3.9.5-bare
        Python/3.9.5
     Other possible modules matches:
        Biopython  Boost.Python  GitPython  IPython  PythonAnaconda  flatbuffers-python  netcdf4-python  protobuf-python  python

--------------------------------------------------------------------------------------------------------------------------------
  To find other possible module matches execute:

      $ module -r spider '.*Python.*'

--------------------------------------------------------------------------------------------------------------------------------
  For detailed information about a specific "Python" package (including how to load the modules) use the module's full name.
  Note that names that have a trailing (E) are extensions provided by other modules.
  For example:

     $ module spider Python/3.9.5
--------------------------------------------------------------------------------------------------------------------------------
```

We now see the list of available Python versions.

- To get information on a specific module, use `module spider <software>/<version>`:

```console  hl_lines="9 10 11"
marie@login$ module spider Python/3.9.5
--------------------------------------------------------------------------------------------------------------------------------
  Python: Python/3.9.5
--------------------------------------------------------------------------------------------------------------------------------
    Description:
      Python is a programming language that lets you work more quickly and integrate your systems more effectively.


    You will need to load all module(s) on any one of the lines below before the "Python/3.10.4" module is available to load.

      GCCcore/11.3.0

    This module provides the following extensions:

      alabaster/0.7.12 (E), appdirs/1.4.4 (E), asn1crypto/1.4.0 (E), atomicwrites/1.4.0 (E), attrs/21.2.0 (E), Babel/2.9.1 (E), bcrypt/3.2.0 (E), bitstring/3.1.7 (E), blist/1.3.6 (E), CacheControl/0.12.6 (E), cachy/0.3.0 (E), certifi/2020.12.5 (E), cffi/1.14.5 (E), chardet/4.0.0 (E), cleo/0.8.1 (E), click/7.1.2 (E), clikit/0.6.2 (E), colorama/
      [...]

    Help:
      Description
      ===========
      Python is a programming language that lets you work more quickly and integrate your systems
       more effectively.


      More information
      ================
       - Homepage: https://python.org/


      Included extensions
      ===================
      alabaster-0.7.12, appdirs-1.4.4, asn1crypto-1.4.0, atomicwrites-1.4.0,
      attrs-21.2.0, Babel-2.9.1, bcrypt-3.2.0, bitstring-3.1.7, blist-1.3.6,
      [...]
```

In some cases it is required to load additional modules before loading the desired software.
In the example above, it is `GCCcore/11.3.0`.

- Load prerequisites and the desired software:

```console
marie@login$ module load GCCcore/11.3.0  # load prerequisites

Module GCCcore/11.3.0 loaded.

marie@login$ module load Python/3.9.5   # load desired version of software
Module Python/3.9.5 and 11 dependencies loaded.
```

For additional information refer to the detailed documentation on [modules](../software/modules.md).

!!! hint "Special hints on different software"

    See also the section "Applications and Software" for more information on e.g.
    [Python](../software/data_analytics_with_python.md),
    [R](../software/data_analytics_with_r.md),
    [Mathematica/MatLab](../software/mathematics.md), etc.

!!! hint "Tip for Python packages"

    The use of [Virtual Environments](../software/python_virtual_environments.md)
    (best in [workspaces](../data_lifecycle/workspaces.md)) is required if you want to install
    additional Python packages with `pip`.
    When using `pip install` without an activated virtual environment you will get this error:
    > ERROR: Could not find an activated virtualenv (required).

    Please check the module system, even for specific Python packages,
    e.g. `numpy`, `tensorflow` or `pytorch`.
    Those modules may provide much better performance than the packages found on PyPi
    (installed via `pip`) which have to work on any system while our installation is optimized for
    each ZIH system to make the best use of the specific CPUs and GPUs found here.
    However the Python package ecosystem (like others) is very heterogeneous and dynamic,
    with daily updates.
    The central update cycle for software on ZIH HPC systems is approximately every six months.
    So the software installed as modules might be a bit older.

!!! warning

    When explicitely loading multiple modules you need to make sure that they are compatible.
    So try to stick to modules using the same toolchain.
    See the [Toolchains section](../software/modules.md#toolchains) for more information.

## Running a Program/Job

At HPC systems, computational work and resource requirements are encapsulated into so-called jobs.
Since all computational resources are shared with other users, these resources need to be
allocated. For managing these allocations a so-called job scheduler or a batch system is used -
on ZIH systems this is [Slurm](https://slurm.schedmd.com/quickstart.html).
It is possible to run a job [interactively](../jobs_and_resources/slurm.md#interactive-jobs)
(real time execution) or to submit it as a [batch job](../jobs_and_resources/slurm.md#batch-jobs)
(scheduled execution).

For beginners, we highly advise to run the job interactively. To do so, use the `srun` command
on any of the ZIH HPC clusters (systems).
For this `srun` command, it is possible to define options like the number of tasks (`--ntasks`),
number of CPUs per task (`--cpus-per-task`),
the amount of time you would like to keep this interactive session open (`--time`), memory per
CPU (`--mem-per-cpu`) and many others.
See [Slurm documentation](../jobs_and_resources/slurm.md#interactive-jobs) for more details.

```console
marie@login$ srun --ntasks=1 --cpus-per-task=4 --time=1:00:00 --mem-per-cpu=1700 --pty bash -l #allocate 4 cores for the interactive job
marie@compute$ module load Python #load necessary packages
marie@compute$ cd /data/horse/ws/marie-number_crunch/ #go to your created workspace
marie@compute$ python test.py #execute your file
Hello, World!
```

For more information, follow the [interactive jobs](../jobs_and_resources/slurm.md#interactive-jobs)
or the [batch job](../jobs_and_resources/slurm.md#batch-jobs) documentation.


# Big Data Analytics

[Apache Spark](https://spark.apache.org/), [Apache Flink](https://flink.apache.org/)
and [Apache Hadoop](https://hadoop.apache.org/) are frameworks for processing and integrating
Big Data.
These frameworks are also offered as software [modules](modules.md).
You can check module versions and availability with the command

=== "Spark"
    ```console
    marie@login$ module avail Spark
    ```
=== "Flink"
    ```console
    marie@login$ module avail Flink
    ```

**Prerequisites:** To work with the frameworks, you need [access](../access/ssh_login.md) to ZIH
systems and basic knowledge about data analysis and the batch system
[Slurm](../jobs_and_resources/slurm.md).

The usage of Big Data frameworks is different from other modules due to their master-worker
approach. That means, before an application can be started, one has to do additional steps.
In the following, we assume that a Spark application should be started and give alternative
commands for Flink where applicable.

The steps are:

1. Load the Spark software module
1. Configure the Spark cluster
1. Start a Spark cluster
1. Start the Spark application

Apache Spark can be used in [interactive](#interactive-jobs) and [batch](#batch-jobs) jobs as well
as via [Jupyter notebooks](#jupyter-notebook). All three ways are outlined in the following.

## Interactive Jobs

### Default Configuration

The Spark and Flink modules are available in the `power` environment.
Thus, Spark and Flink can be executed using different CPU architectures, e.g., Power.

Let us assume that two nodes should be used for the computation. Use a `srun` command similar to
the following to start an interactive session. The following code
snippet shows a job submission with an allocation of two nodes with 60000 MB main
memory exclusively for one hour:

```console
marie@login.power$ srun --nodes=2 --mem=60000M --exclusive --time=01:00:00 --pty bash -l
```

Once you have the shell, load desired Big Data framework using the command

=== "Spark"
    ```console
    marie@compute$ module load Spark
    ```
=== "Flink"
    ```console
    marie@compute$ module load Flink
    ```

Before the application can be started, the cluster with the allocated nodes needs to be set up. To
do this, configure the cluster first using the configuration template at `$SPARK_HOME/conf` for
Spark or `$FLINK_ROOT_DIR/conf` for Flink:

=== "Spark"
    ```console
    marie@compute$ source framework-configure.sh spark $SPARK_HOME/conf
    ```
=== "Flink"
    ```console
    marie@compute$ source framework-configure.sh flink $FLINK_ROOT_DIR/conf
    ```

This places the configuration in a directory called `cluster-conf-<JOB_ID>` in your home directory,
where `<JOB_ID>` stands for the id of the Slurm job. After that, you can start in
the usual way:

=== "Spark"
    ```console
    marie@compute$ start-all.sh
    ```
=== "Flink"
    ```console
    marie@compute$ start-cluster.sh
    ```

The necessary background processes should now be set up and you can start your application, e. g.:

=== "Spark"
    ```console
    marie@compute$ spark-submit --class org.apache.spark.examples.SparkPi \
    $SPARK_HOME/examples/jars/spark-examples_2.12-3.0.1.jar 1000
    ```
=== "Flink"
    ```console
    marie@compute$ flink run $FLINK_ROOT_DIR/examples/batch/KMeans.jar
    ```

!!! warning

    Do not delete the directory `cluster-conf-<JOB_ID>` while the job is still
    running. This leads to errors.

### Custom Configuration

The script `framework-configure.sh` is used to derive a configuration from a template. It takes two
parameters:

- The framework to set up (parameter `spark` for Spark, `flink` for Flink, and `hadoop` for Hadoop)
- A configuration template

Thus, you can modify the configuration by replacing the default configuration template with a
customized one. This way, your custom configuration template is reusable for different jobs. You
can start with a copy of the default configuration ahead of your interactive session:

=== "Spark"
    ```console
    marie@login.power$ cp -r $SPARK_HOME/conf my-config-template
    ```
=== "Flink"
    ```console
    marie@login.power$ cp -r $FLINK_ROOT_DIR/conf my-config-template
    ```

After you have changed `my-config-template`, you can use your new template in an interactive job
with:

=== "Spark"
    ```console
    marie@compute$ source framework-configure.sh spark my-config-template
    ```
=== "Flink"
    ```console
    marie@compute$ source framework-configure.sh flink my-config-template
    ```

### Using Hadoop Distributed Filesystem (HDFS)

If you want to use Spark and HDFS together (or in general more than one framework), a scheme
similar to the following can be used:

=== "Spark"
    ```console
    marie@compute$ module load Hadoop
    marie@compute$ module load Spark
    marie@compute$ source framework-configure.sh hadoop $HADOOP_ROOT_DIR/etc/hadoop
    marie@compute$ source framework-configure.sh spark $SPARK_HOME/conf
    marie@compute$ start-dfs.sh
    marie@compute$ start-all.sh
    ```
=== "Flink"
    ```console
    marie@compute$ module load Hadoop
    marie@compute$ module load Flink
    marie@compute$ source framework-configure.sh hadoop $HADOOP_ROOT_DIR/etc/hadoop
    marie@compute$ source framework-configure.sh flink $FLINK_ROOT_DIR/conf
    marie@compute$ start-dfs.sh
    marie@compute$ start-cluster.sh
    ```

## Batch Jobs

Using `srun` directly on the shell blocks the shell and launches an interactive job. Apart from
short test runs, it is **recommended to launch your jobs in the background using batch jobs**. For
that, you can conveniently put the parameters directly into the job file and submit it via
`sbatch [options] <job file>`.

Please use a [batch job](../jobs_and_resources/slurm.md) with a configuration, similar to the
example below:

??? example "example-starting-script.sbatch"
    === "Spark"
        ```bash
        #!/bin/bash -l
        #SBATCH --time=01:00:00
        #SBATCH --nodes=2
        #SBATCH --exclusive
        #SBATCH --mem=60000M
        #SBATCH --job-name="example-spark"

        module load Spark/3.0.1-Hadoop-2.7-Java-1.8-Python-3.7.4-GCCcore-8.3.0

        function myExitHandler () {
            stop-all.sh
        }

        #configuration
        . framework-configure.sh spark $SPARK_HOME/conf

        #register cleanup hook in case something goes wrong
        trap myExitHandler EXIT

        start-all.sh

        spark-submit --class org.apache.spark.examples.SparkPi $SPARK_HOME/examples/jars/spark-examples_2.12-3.0.1.jar 1000

        stop-all.sh

        exit 0
        ```
    === "Flink"
        ```bash
        #!/bin/bash -l
        #SBATCH --time=01:00:00
        #SBATCH --nodes=2
        #SBATCH --exclusive
        #SBATCH --mem=60000M
        #SBATCH --job-name="example-flink"

        module load Flink/1.12.3-Java-1.8.0_161-OpenJDK-Python-3.7.4-GCCcore-8.3.0

        function myExitHandler () {
            stop-cluster.sh
        }

        #configuration
        . framework-configure.sh flink $FLINK_ROOT_DIR/conf

        #register cleanup hook in case something goes wrong
        trap myExitHandler EXIT

        #start the cluster
        start-cluster.sh

        #run your application
        flink run $FLINK_ROOT_DIR/examples/batch/KMeans.jar

        #stop the cluster
        stop-cluster.sh

        exit 0
        ```

## Jupyter Notebook

You can run Jupyter notebooks with Spark and Flink on the ZIH systems in a similar way as described
on the [JupyterHub](../access/jupyterhub.md) page.

### Spawning a Notebook

Go to [https://jupyterhub.hpc.tu-dresden.de](https://jupyterhub.hpc.tu-dresden.de).
In the tab "Advanced", go to the field "Preload modules" and select the following Spark or Flink
module:

=== "Spark"
    ```
    Spark/3.0.1-Hadoop-2.7-Java-1.8-Python-3.7.4-GCCcore-8.3.0
    ```
=== "Flink"
    ```
    Flink/1.12.3-Java-1.8.0_161-OpenJDK-Python-3.7.4-GCCcore-8.3.0
    ```

When your Jupyter instance is started, you can set up Spark/Flink. Since the setup in the notebook
requires more steps than in an interactive session, we have created example notebooks that you can
use as a starting point for convenience: [SparkExample.ipynb](misc/SparkExample.ipynb),
[FlinkExample.ipynb](misc/FlinkExample.ipynb)

!!! warning

    The notebooks only work with the Spark or Flink module mentioned above. When using other
    Spark/Flink modules, it is possible that you have to do additional or other steps in order to
    make Spark/Flink running.

!!! note

    You could work with simple examples in your home directory, but, according to the
    [storage concept](../data_lifecycle/overview.md), **please use
    [workspaces](../data_lifecycle/workspaces.md) for your study and work projects**. For this
    reason, you have to use advanced options of Jupyterhub and put "/" in "Workspace scope" field.

## FAQ

Q: Command `source framework-configure.sh hadoop $HADOOP_ROOT_DIR/etc/hadoop` gives the output:
`bash: framework-configure.sh: No such file or directory`. How can this be resolved?

A: Please try to re-submit or re-run the job and if that doesn't help re-login to the ZIH system.

Q: There are a lot of errors and warnings during the set up of the session

A: Please check the work capability on a simple example as shown in this documentation.

!!! help

    If you have questions or need advice, please use the contact form on
    [https://scads.ai/about-us/](https://scads.ai/about-us/) or contact the HPC support.


# Building Software

While it is possible to do short compilations on the login nodes, it is generally considered good
practice to use a job for that, especially when using many parallel make processes. Since 2016,
the `/projects` filesystem is mounted read-only on all compute
nodes in order to prevent users from doing large I/O there (which is what the `/data/horse` is for).
In consequence, you cannot compile in `/projects` within a job. If you wish to install
software for your project group anyway, you can use a build directory in the `/data/horse` filesystem
instead.

Every sane build system should allow you to keep your source code tree and your build directory
separate, some even demand them to be different directories. Plus, you can set your installation
prefix (the target directory) back to your `/projects` folder and do the "make install" step on the
login nodes.

For instance, when using CMake and keeping your source in `/projects`, you could do the following:

```console
# save path to your source directory:
marie@login$ export SRCDIR=/projects/p_number_crunch/mysource

# create a build directory in /data/horse:
marie@login$ mkdir /data/horse/p_number_crunch/mysoftware_build

# change to build directory within /data/horse:
marie@login$ cd /data/horse/p_number_crunch/mysoftware_build

# create Makefiles:
marie@login$ cmake -DCMAKE_INSTALL_PREFIX=/projects/p_number_crunch/mysoftware $SRCDIR

# build in a job:
marie@login$ srun --mem-per-cpu=1500 --cpus-per-task=12 --pty make -j 12

# do the install step on the login node again:
marie@login$ make install
```

As a bonus, your compilation should also be faster in the parallel `/data/horse` filesystem than it
would be in the comparatively slow NFS-based `/projects` filesystem.


# Computational Fluid Dynamics (CFD)

The following CFD applications are available on our system:

|               | **Module** |
|---------------|------------|
| **OpenFOAM**  | openfoam   |
| **CFX**       | ansys      |
| **Fluent**    | ansys      |
| **ICEM CFD**  | ansys      |
| **STAR-CCM+** | star       |

## OpenFOAM

The OpenFOAM (Open Field Operation and Manipulation) CFD Toolbox can simulate anything from complex
fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics,
electromagnetics and the pricing of financial options. OpenFOAM is developed primarily by
[OpenCFD Ltd](https://www.openfoam.com) and is freely available and open-source,
licensed under the GNU General Public License.

The command `module spider OpenFOAM` provides the list of installed OpenFOAM versions. In order to
use OpenFOAM, it is mandatory to set the environment by sourcing the `bashrc` (for users running
bash or ksh) or `cshrc` (for users running tcsh or csh) provided by OpenFOAM:

```console
marie@login$ module load OpenFOAM/VERSION
marie@login$ source $FOAM_BASH
marie@login$ # source $FOAM_CSH
```

???+ example "Example for OpenFOAM job script:"
    ```bash
    #!/bin/bash
    #SBATCH --time=12:00:00     # walltime
    #SBATCH --ntasks=60         # number of processor cores (i.e. tasks)
    #SBATCH --mem-per-cpu=500M  # memory per CPU core
    #SBATCH --job-name="Test"   # job name
    #SBATCH --mail-user=marie@tu-dresden.de  # email address (only tu-dresden)
    #SBATCH --mail-type=ALL

    OUTFILE="Output"
    module load OpenFOAM
    source $FOAM_BASH
    cd /horse/ws/marie-example-workspace  # work directory using workspace
    srun pimpleFoam -parallel > "$OUTFILE"
    ```

## Ansys CFX

Ansys CFX is a powerful finite-volume-based program package for modeling general fluid flow in
complex geometries. The main components of the CFX package are the flow solver cfx5solve, the
geometry and mesh generator cfx5pre, and the post-processor cfx5post.

???+ example "Example for CFX job script:"
    ```bash
    #!/bin/bash
    #SBATCH --time=12:00                                       # walltime
    #SBATCH --ntasks=4                                         # number of processor cores (i.e. tasks)
    #SBATCH --mem-per-cpu=1900M                                # memory per CPU core
    #SBATCH --mail-user=marie@tu-dresden.de                    # email address (only tu-dresden)
    #SBATCH --mail-type=ALL

    module load ANSYS
    cd /horse/ws/marie-example-workspace                       # work directory using workspace
    cfx-parallel.sh -double -def StaticMixer.def
    ```

## Ansys Fluent

???+ example "Fluent needs the host names and can be run in parallel like this:"
    ```bash
    #!/bin/bash
    #SBATCH --time=12:00                        # walltime
    #SBATCH --ntasks=4                          # number of processor cores (i.e. tasks)
    #SBATCH --mem-per-cpu=1900M                 # memory per CPU core
    #SBATCH --mail-user=marie@tu-dresden.de     # email address (only tu-dresden)
    #SBATCH --mail-type=ALL

    module purge
    module load release/23.10
    module load ANSYS/2023R1
    fluent 2ddp -t$SLURM_NTASKS -g -mpi=openmpi -pinfiniband -cnf=$(/software/util/slurm/bin/create_rankfile -f CCM) -i input.jou
    ```

To use fluent interactively, please try:

```console
marie@login$ module load ANSYS/19.2
marie@login$ srun --nodes=1 --cpus-per-task=4 --time=1:00:00 --pty --x11=first bash
marie@compute$ fluent &
```

## STAR-CCM+

!!! note
    You have to use your own license in order to run STAR-CCM+ on ZIH systems, so you have to specify
    the parameters `-licpath` and `-podkey`, see the example below.

Our installation provides a script `create_rankfile -f CCM` that generates a host list from the
Slurm job environment that can be passed to `starccm+`, enabling it to run across multiple nodes.

???+ example
    ```bash
    #!/bin/bash
    #SBATCH --time=12:00                        # walltime
    #SBATCH --ntasks=32                         # number of processor cores (i.e. tasks)
    #SBATCH --mem-per-cpu=2500M                 # memory per CPU core
    #SBATCH --mail-user=marie@tu-dresden.de     # email address (only tu-dresden)
    #SBATCH --mail-type=ALL

    module load STAR-CCM+

    LICPATH="port@host"
    PODKEY="your podkey"
    INPUT_FILE="your_simulation.sim"
    starccm+ -collab -rsh ssh -cpubind off -np $SLURM_NTASKS -on $(/sw/taurus/tools/slurmtools/default/bin/create_rankfile -f CCM) -batch -power -licpath $LICPATH -podkey $PODKEY $INPUT_FILE
    ```

!!! note
    The software path of the script `create_rankfile -f CCM` is different on the
    [new HPC system Barnard](../jobs_and_resources/hardware_overview.md#barnard).

???+ example
    ```bash
    #!/bin/bash
    #SBATCH --time=12:00                        # walltime
    #SBATCH --ntasks=32                         # number of processor cores (i.e. tasks)
    #SBATCH --mem-per-cpu=2500M                 # memory per CPU core
    #SBATCH --mail-user=marie@tu-dresden.de     # email address (only tu-dresden)
    #SBATCH --mail-type=ALL

    module load STAR-CCM+

    LICPATH="port@host"
    PODKEY="your podkey"
    INPUT_FILE="your_simulation.sim"
    starccm+ -collab -rsh ssh -cpubind off -np $SLURM_NTASKS -on $(/software/util/slurm/bin/create_rankfile -f CCM) -batch -power -licpath $LICPATH -podkey $PODKEY $INPUT_FILE
    ```


# CI/CD on HPC

We provide a **GitLab Runner** that allows you to run a GitLab pipeline on the ZIH systems. With
that you can continuously build, test, and benchmark your HPC software in the target environment.

## Requirements

- You (and ideally every involved developer) need an [HPC-Login](../application/overview.md).
- You manage your source code in a repository at the [TU Chemnitz GitLab instance](https://gitlab.hrz.tu-chemnitz.de)

## Setup process

1. Open your repository in the browser.

2. Hover *Settings* and then click on *CI/CD*

    ![Hover *Settings* and then click on *CI/CD*](misc/menu12_en.png)
    { align=center }

3. *Expand* the *Runners* section

    ![*Expand* the *Runners* section](misc/menu3_en.png)
    { align=center }

4. Copy the *registration token*

    ![Copy the *registration token*](misc/menu4_en.png)
    { align=center }

5. Now, you can request the registration of your repository with the
   [HPC-Support](../support/support.md). In the ticket, you need to add the URL of the GitLab
   repository and the registration token.

!!! warning

    At the moment, only repositories hosted at the TU Chemnitz GitLab are supported.

## GitLab pipelines

As the ZIH provides the CI/CD as an GitLab runner, you can run any pipeline already working on other
runners with the CI/CD at the ZIH systems. This also means, to configure the actual steps performed
once your pipeline runs, you need to define the `.gitlab-ci.yml` file in the root of your
repository. There is a [comprehensive
documentation](https://gitlab.hrz.tu-chemnitz.de/help/ci/index.md) and a [reference for the
`.gitlab-ci.yml` file](https://gitlab.hrz.tu-chemnitz.de/help/ci/yaml/index) available at every
GitLab instance. There's also a [quick start
guide](https://gitlab.hrz.tu-chemnitz.de/help/ci/quick_start/index.md).

The main difference to other GitLab runner is that every pipeline jobs will be scheduled as an
individual HPC job on the ZIH systems. Therefore, an important aspect is the possibility to set
Slurm parameters. While scheduling jobs allows to run code directly on the target system, it also
means that a single pipeline has to wait for resource allocation. Hence, you want to restrict,
which commits will run the complete pipeline, or which commits only run a part of the pipeline.

### Passing Slurm parameters

You can pass Slurm parameters via the [`variables`
keyword](https://gitlab.hrz.tu-chemnitz.de/help/ci/yaml/index#variables), either globally for the
whole yaml file, or on a per-job base.

Use the variable `SCHEDULER_PARAMETERS` and define the same parameters you would use for [`srun` or
`sbatch`](../jobs_and_resources/slurm.md).

!!! warning

    The parameters `--job-name`, `--output`, and `--wait` are handled by the GitLab runner and must
    not be used. If used, the run will fail.

!!! tip

    Make sure to set the `--account` such that the allocation of HPC resources is accounted
    correctly.

!!! example

    The following YAML file defines a configuration section `.test-job`, and two jobs,
    `test-job-haswell` and `test-job-power9`, extending from that. The two job share the
    `before_script`, `script`, and `after_script` configuration, but differ in the
    `SCHEDULER_PARAMETERS`. The `test-job-haswell` and `test-job-power9` are scheduled on the partition
    `haswell` and partition `power9`, respectively.

    ```yaml
    .test-job:
    before_script:
        - date
        - pwd
        - hostname
    script:
        - date
        - pwd
        - hostname
    after_script:
        - date
        - pwd
        - hostname

    test-job-haswell:
    extends: .test-job
    variables:
        SCHEDULER_PARAMETERS: -p haswell


    test-job-power9:
    extends: .test-job
    variables:
        SCHEDULER_PARAMETERS: -p power9
    ```

## Current limitations

- Every runner job is currently limited to **one hour**. Once this time limit passes, the runner job
  gets canceled regardless of the requested runtime from Slurm. This time *includes* the waiting
  time for HPC resources.

## Pitfalls and Recommendations

- While the [`before_script`](https://gitlab.hrz.tu-chemnitz.de/help/ci/yaml/index#before_script)
  and [`script`](https://gitlab.hrz.tu-chemnitz.de/help/ci/yaml/index#script) array of commands are
  executed on the allocated resources, the
  [`after_script`](https://gitlab.hrz.tu-chemnitz.de/help/ci/yaml/index#after_script) runs on the
  GitLab runner node. We recommend that you do not use `after_script`.

- It is likely that all your runner jobs will be executed in a slightly different directory on the
  shared filesystem. Some build systems, for example CMake, expect that the configure and build is
  executed in the same directory. In this case, we recommend to use one job for configure and
  build.


# Compilers and Flags

The following compilers are available on the ZIH system:

|                      | GNU Compiler Collection | Clang Compiler | Intel Compiler | PGI Compiler (Nvidia HPC SDK) |
|----------------------|-------------------------|----------------|----------------|-------------------------------|
| Further information  | [GCC website](https://gcc.gnu.org/) | [Clang documentation](https://clang.llvm.org/docs/UsersManual.html) | [C/C++](https://software.intel.com/en-us/c-compilers), [Fortran](https://software.intel.com/en-us/fortran-compilers) | [PGI website](https://www.pgroup.com) |
| Module name          | GCC        | Clang      | iccifort  | PGI         |
| C Compiler           | `gcc`      | `clang`    | `icc`     | `pgcc`      |
| C++ Compiler         | `g++`      | `clang++`  | `icpc`    | `pgc++`     |
| Fortran Compiler     | `gfortran` |     -      | `ifort`   | `pgfortran` |

For an overview of the installed compiler versions, please use `module spider <module name>`
on the ZIH systems.
Additionally you can use `module av` and look below "compilers" to see all available compiler modules.

All compilers support various language standards, at least up to ISO C11, ISO C++ 2014, and Fortran 2003.
Please check the man pages to verify that your code can be compiled.

Please note that the linking of C++ files normally requires the C++ version of the compiler to link
the correct libraries.

## Compiler Flags

Common options are:

- `-g` to include information required for debugging
- `-pg` to generate gprof-like sample-based profiling information during the run
- `-O0`, `-O1`, `-O2`, `-O3` to customize the optimization level from
  no (`-O0`) to aggressive (`-O3`) optimization
- `-I` to set search path for header files
- `-L` to set search path for libraries

Please note that aggressive optimization allows deviation from the strict IEEE arithmetic.
Since the performance impact of options like `-fp-model strict` is very hard you
have to balance speed and desired accuracy of your application yourself.

The user benefits from the (nearly) same set of compiler flags for optimization for the C, C++, and
Fortran-compilers.
In the following table, only a couple of important compiler-dependent options are listed.
For more detailed information about these and further flags, the user should refer to the man
pages or use the option `--help` to list all options of the compiler.

| GCC | Intel | PGI | Description |
|----------------------|--------------|-------------|-------------------------------------------------------------------------------------|
| `-fopenmp`           | `-fopenmp`    | `-mp`       | turn on OpenMP support |
| `-std=c99`, `-std=c++11`, `-std=f2018`   | `-std=c99`, `-std=c++11`, `-std18`       | `-c99`, `--c++11`, n/a  | set language standard, for example C99, C++11, Fortran 2018 |
| `-mieee-fp` `-frounding-math`  | `-fp-model precise` or `-fp-model strict`        | `-Kieee`    | limit floating-point optimizations and maintain declared precision |
| `-ffast-math`        | `-mp1` or `-fp-model fast`  | `-Mfprelaxed`  | allow floating-point optimizations, may violate IEEE conformance |
| `-Ofast`             | `-fast`      | `-fast`     | Maximize performance, implies a couple of other flags                               |
| `-fsignaling-nans` `-fno-trapping-math` | C/C++: `-fpe-trap`, Fortran: `-fpe-all` | `-Ktrap` | controls the behavior when floating-point exceptions occur   |
| `-mavx` `-msse4.2`   | `-mavx` `-msse4.2`   | `-fastsse`  | "generally optimal flags" for supporting SSE instructions                           |
| `-flto`              | `-ipo`       | `-Mipa`     | interprocedural / link-time optimization (across source files)                                         |
| `-floop-parallelize-all -ftree-parallelize-loops=<numthreads>` | `-parallel`  | `-Mconcur`  | auto-parallelizer                                                                   |
| `-fprofile-generate` | `-prof-gen`  | `-Mpfi`     | create instrumented code to generate profile in file                                |
| `-fprofile-use`      | `-prof-use`  | `-Mpfo`     | use profile data for optimization      |

!!! note

    We can not generally give advice as to which option should be used. To gain maximum performance
    please test the compilers and a few combinations of optimization flags. In case of doubt, you
    can also contact [HPC support](../support/support.md) and ask the staff for help.

### Architecture-specific Optimizations

Different architectures of CPUs feature different vector extensions (like SSE4.2 and AVX)
to accelerate computations.
The following matrix shows proper compiler flags for the architectures at the ZIH:

| HPC System | Architecture       | GCC                  | Intel                | Nvidia HPC |
|------------|--------------------|----------------------|----------------------|-----|
| [`Alpha Centauri`](../jobs_and_resources/alpha_centauri.md) | AMD Rome   | `-march=znver2`      | `-march=core-avx2`   | `-tp=zen2`               |
| [`Barnard`](../jobs_and_resources/hardware_overview.md#barnard) | Intel Sapphire Rapids | `-march=sapphirerapids`      | `-march=core-sapphirerapids`   |      |
| [`Capella`](../jobs_and_resources/capella.md)      | AMD Genoa           | `-march=znver4`      |                      | `-tp=zen4`               |
| [`Julia`](../jobs_and_resources/julia.md)          | Intel Cascade Lake  | `-march=cascadelake` | `-march=cascadelake` | `-tp=cascadelake`        |
| [`Romeo`](../jobs_and_resources/romeo.md)          | AMD Rome            | `-march=znver2`      | `-march=core-avx2`   | `-tp=zen2`               |
| All x86                                               | Host's architecture | `-march=native`      | `-xHost` or `-march=native` |  `-tp=host`    |
| [`Power9`](../jobs_and_resources/hardware_overview.md#power9) | IBM Power9          | `-mcpu=power9` or `-mcpu=native`          | | `-tp=pwr9` or `-tp=host` |

To build an executable for different node types with the Intel compiler, use
`-axcode`, where `code` is to be replaced with one or more target architectures.
For Cascade Lake and Sapphire Rapids. the option `-axcascadelake,sapphirerapids`
(for Intel compilers) instructs the compiler to optimized code paths for the
specified architecture(s), if possible.
If the application is executed on one of these architectures, the optimized code
path will be chosen.
A baseline code path will also be generated.
This path is used on other architectures than the specified ones and is used
in code sections that were not optimized by the compiler for a specific architecture.
Other optimization flags can be used as well for, e.g. `-O3`.
However, the `-march` option cannot be used here, as this will overwrite the
`-axcode` option.
This increases the size of the program code (might result in
poorer L1 instruction cache hits) but enables to run the same program on
different hardware types with compiler optimizations.


# Singularity

[Containerization](https://www.ibm.com/cloud/learn/containerization) encapsulating or packaging up
software code and all its dependencies to run uniformly and consistently on any infrastructure. On
ZIH systems [Singularity](https://sylabs.io/) is used as a standard container solution. Singularity
enables users to have full control of their environment. This means that you don’t have to ask the
HPC support to install anything for you - you can put it in a Singularity container and run! As
opposed to Docker (the most famous container solution), Singularity is much more suited to being
used in an HPC environment and more efficient in many cases. Docker images can easily be used in
Singularity. Information about the use of Singularity on ZIH systems can be found on this page.

In some cases using Singularity requires a Linux machine with root privileges
(e.g. using the cluster `Power9`), the same architecture and a compatible kernel.
For many reasons, users on ZIH systems cannot be granted root permissions.
A solution is a Virtual Machine (VM) on the cluster `Power9` which allows users to gain
root permissions in an isolated environment.
There are two main options on how to work with Virtual Machines on ZIH systems:

1. [VM tools](singularity_power9.md): Automated algorithms for using virtual machines;
1. [Manual method](virtual_machines.md): It requires more operations but gives you more flexibility
   and reliability.

## Usage of Singularity

If you wish to containerize your workflow and/or applications, you can use Singularity containers on
ZIH systems. As opposed to Docker, this solution is much more suited to being used in an HPC
environment.

!!! note

    It is not possible for users to generate new custom containers on ZIH systems directly, because
    creating a new container requires root privileges.

However, new containers can be created on your local workstation and moved to ZIH systems for
execution. Follow the instructions for [locally installing Singularity](#local-installation) and
[container creation](#container-creation). Moreover, existing Docker container can easily be
converted, see [Import a Docker container](#import-a-docker-container).

If you are already familiar with Singularity, you might be more interested in our [Singularity
recipes and hints](singularity_recipe_hints.md).

### Local Installation

The local installation of Singularity comprises two steps: Make `go` available and then follow the
instructions from the official documentation to install Singularity.

1. Check if `go` is installed by executing `go version`.  If it is **not**:

    ```console
    marie@local$ wget 'https://storage.googleapis.com/golang/getgo/installer_linux' && chmod +x
    installer_linux && ./installer_linux && source $HOME/.bash_profile
    ```

1. Instructions to
   [install Singularity](https://github.com/sylabs/singularity/blob/master/INSTALL.md#clone-the-repo)
   from the official documentation:

    Clone the repository

    ```console
    marie@local$ mkdir -p ${GOPATH}/src/github.com/sylabs
    marie@local$ cd ${GOPATH}/src/github.com/sylabs
    marie@local$ git clone https://github.com/sylabs/singularity.git
    marie@local$ cd singularity
    ```

    Checkout the version you want (see the [GitHub releases page](https://github.com/sylabs/singularity/releases)
    for available releases), e.g.

    ```console
    marie@local$ git checkout v3.2.1
    ```

    Build and install

    ```console
    marie@local$ cd ${GOPATH}/src/github.com/sylabs/singularity
    marie@local$ ./mconfig && cd ./builddir && make
    marie@local$ sudo make install
    ```

### Container Creation

!!! note

    It is not possible for users to generate new custom containers on ZIH systems directly, because
    creating a new container requires root privileges.

There are two possibilities:

1. Create a new container on your local workstation (where you have the necessary privileges), and
   then copy the container file to ZIH systems for execution. Therefore you also have to install
   [Singularity](https://sylabs.io/guides/3.0/user-guide/quick_start.html#quick-installation-steps)
   on your local workstation.
1. You can, however, import an existing container from, e.g., Docker.

Both methods are outlined in the following.

#### New Custom Container

You can create a new custom container on your workstation, if you have root rights.

!!! attention "Respect the micro-architectures"

    You cannot create containers for the cluster `Power`, as it bases on Power9 micro-architecture
    which is different to the x86 architecture in common computers/laptops. For that you can use
    the [VM Tools](singularity_power9.md).

Creating a container is done by writing a definition file, such as `myDefinition.def`, and passing
it to `singularity` via

```console
marie@local$ singularity build myContainer.sif myDefinition.def
```

A definition file contains a bootstrap
[header](https://sylabs.io/guides/3.2/user-guide/definition_files.html#header)
where you choose the base and
[sections](https://sylabs.io/guides/3.2/user-guide/definition_files.html#sections)
where you install your software.

The most common approach is to start from an existing Docker image from DockerHub. For example, to
start from an [Ubuntu image](https://hub.docker.com/_/ubuntu) copy the following into a new file
called `ubuntu.def` (or any other filename of your choice)

```bash
Bootstrap: docker
From: ubuntu:trusty

%runscript
    echo "This is what happens when you run the container..."

%post
    apt-get install g++
```

Then you can call

```console
marie@local$ singularity build ubuntu.sif ubuntu.def
```

And it will install Ubuntu with g++ inside your container, according to your definition file.
More bootstrap options are available. The following example, for instance, bootstraps a basic CentOS
7 image.

```Bash
BootStrap: yum
OSVersion: 7
MirrorURL: http://mirror.centos.org/centos-%{OSVERSION}/%{OSVERSION}/os/$basearch/
Include: yum

%runscript
    echo "This is what happens when you run the container..."

%post
    echo "Hello from inside the container"
    yum -y install vim-minimal
```

More examples of definition files can be found at
https://github.com/singularityware/singularity/tree/master/examples.

#### Import a Docker Container

!!! hint

    As opposed to bootstrapping a container, importing from Docker does **not require root
    privileges** and therefore works on ZIH systems directly. Please note, that the Singularity
    commands are only available on the compute nodes and not on the login nodes.

You can import an image directly from the Docker repository (Docker Hub):

```console
marie@compute$ singularity build my-container.sif docker://ubuntu:latest
```

Creating a Singularity container directly from a local Docker image is possible but not
recommended. The steps are:

```console
# Start a docker registry
marie@local$ docker run -d -p 5000:5000 --restart=always --name registry registry:2

# Push local docker container to it
marie@local$ docker tag alpine localhost:5000/alpine
marie@local$ docker push localhost:5000/alpine

# Create def file for singularity like this...
marie@local$ cat example.def
Bootstrap: docker
Registry: <a href="http://localhost:5000" rel="nofollow" target="_blank">http://localhost:5000</a>
From: alpine

# Build singularity container
marie@local$ singularity build --nohttps alpine.sif example.def
```

#### Start from a Dockerfile

As Singularity definition files and Dockerfiles are very similar you can start creating a definition
file from an existing Dockerfile by "translating" each section.

There are tools to automate this. One of them is
[spython](https://github.com/singularityhub/singularity-cli) which can be installed with `pip`
(add `--user` if you don't want to install it system-wide):

```console
marie@local$ pip3 install -U spython
```

With this you can simply issue the following command to convert a Dockerfile in the current folder
into a Singularity definition file:

```console
marie@local$ spython recipe Dockerfile myDefinition.def
```

Please **verify** your generated definition and adjust where required!

There are some notable changes between Singularity definitions and Dockerfiles:

1. Command chains in Dockerfiles (`apt-get update && apt-get install foo`) must be split into
   separate commands (`apt-get update; apt-get install foo`). Otherwise a failing command before the
   ampersand is considered "checked" and does not fail the build.
1. The environment variables section in Singularity is only set on execution of the final image, not
   during the build as with Docker. So `*ENV*` sections from Docker must be translated to an entry
   in the `%environment` section and **additionally** set in the `%runscript` section if the
   variable is used there.
1. `*VOLUME*` sections from Docker cannot be represented in Singularity containers. Use the runtime
   option \`-B\` to bind folders manually.
1. `CMD` and `ENTRYPOINT` from Docker do not have a direct representation in Singularity.
   The closest is to check if any arguments are given in the `%runscript` section and call the
   command from `ENTRYPOINT` with those, if none are given call `ENTRYPOINT` with the
   arguments of `CMD`:

  ```bash
  if [ $# -gt 0 ]; then
    <ENTRYPOINT> "$@"
  else
    <ENTRYPOINT> <CMD>
  fi
  ```

### Use the Containers

#### Enter a Shell in Your Container

A read-only shell can be entered as follows:

```console
marie@compute$ singularity shell my-container.sif
```

!!! note

    In contrast to, for instance, Docker, this will mount various folders from the host system
    including $HOME. This may lead to problems with, e.g., Python that stores local packages in the
    home folder, which may not work inside the container. It also makes reproducibility harder. It
    is therefore recommended to use `--contain/-c` to not bind `$HOME` (and others like `/tmp`)
    automatically and instead set up your binds manually via `-B` parameter. Example:

    ```console
    marie@compute$ singularity shell --contain -B /data/horse,/my/folder-on-host:/folder-in-container my-container.sif
    ```

You can write into those folders by default. If this is not desired, add an `:ro` for read-only to
the bind specification (e.g. `-B /data/horse:/data/horse:ro\`).  Note that we already defined bind
paths for `/data/horse`, `/projects` and `/sw` in our global `singularity.conf`, so you needn't use
the `-B` parameter for those.

If you wish to install additional packages, you have to use the `-w` parameter to
enter your container with it being writable. This, again, must be done on a system where you have
the necessary privileges, otherwise you can only edit files that your user has the permissions for.
E.g:

```console
marie@local$ singularity shell -w my-container.sif
Singularity.my-container.sif> yum install htop
```

The `-w` parameter should only be used to make permanent changes to your container, not for your
productive runs (it can only be used writable by one user at the same time). You should write your
output to the usual ZIH filesystems like `/data/horse`. Launching applications in your container

#### Run a Command Inside the Container

While the `shell` command can be useful for tests and setup, you can also launch your applications
inside the container directly using "exec":

```console
marie@compute$ singularity exec my-container.sif /opt/myapplication/bin/run_myapp
```

This can be useful if you wish to create a wrapper script that transparently calls a containerized
application for you. E.g.:

```bash
#!/bin/bash

if ! type singularity > /dev/null 2>&1 ; then
  echo "Singularity not found. Is the module loaded?"
  exit 1
fi

singularity exec /projects/p_number_crunch/my-container.sif /opt/myapplication/run_myapp "$@"
```

The better approach is to use `singularity run`, which executes whatever was set in the `%runscript`
section of the definition file with the arguments you pass to it. Example: Build the following
definition file into an image:

```bash
Bootstrap: docker
From: ubuntu:trusty

%post
  apt-get install -y g++
  echo '#include <iostream>' > main.cpp
  echo 'int main(int argc, char** argv){ std::cout << argc << " args for " << argv[0] << std::endl; }' >> main.cpp
  g++ main.cpp -o myCoolApp
  mv myCoolApp /usr/local/bin/myCoolApp

%runscript
  myCoolApp "$@
singularity build my-container.sif example.def
```

Then you can run your application via

```console
marie@compute$ singularity run my-container.sif first_arg 2nd_arg
```

Alternatively you can execute the container directly which is equivalent:

```console
marie@compute$ ./my-container.sif first_arg 2nd_arg
```

With this you can even masquerade an application with a Singularity container as if it was an actual
program by naming the container just like the binary:

```console
marie@compute$ mv my-container.sif myCoolAp
```

### Use-Cases

One common use-case for containers is that you need an operating system with a newer
[glibc](https://www.gnu.org/software/libc/) version than what is available on ZIH systems. E.g., the
bullx Linux on ZIH systems used to be based on RHEL 6 having a rather dated glibc version 2.12, some
binary-distributed applications didn't work on that anymore. You can use one of our pre-made CentOS
7 container images (`/data/horse/lustre/scratch2/singularity/centos7.img`) to circumvent this
problem.

!!! example

    ```console
    marie@compute$ singularity exec /data/horse/lustre/scratch2/singularity/centos7.img ldd --version
    ldd (GNU libc) 2.17
    ```


# Software Installation with EasyBuild

Sometimes the [modules](modules.md) installed in the cluster are not enough for your purposes and
you need some other software or a different version of a software.

For most commonly used software, chances are high that there is already a *recipe* that EasyBuild
provides, which you can use. But what is EasyBuild?

!!! note "EasyBuild"

    [EasyBuild](https://easybuild.io/) is the software used to build and install software on ZIH
    systems.

The aim of this page is to introduce users to working with EasyBuild and to utilizing it to create
modules.

## Prerequisites

1. [Shell access](../access/ssh_login.md) to ZIH systems
1. Basic knowledge about:
    - [the ZIH system](../jobs_and_resources/hardware_overview.md)
    - [the module system](modules.md) on ZIH systems

EasyBuild uses a configuration file called recipe or "EasyConfig", which contains all the
information about how to obtain and build the software:

- Name
- Version
- Toolchain (think: Compiler + some more)
- Download URL
- Build system (e.g. `configure && make` or `cmake && make`)
- Config parameters
- Tests to ensure a successful build

The build system part is implemented in so-called "EasyBlocks" and contains the common workflow.
Sometimes, those are specialized to encapsulate behavior specific to multiple/all versions of the
software. Everything is written in Python, which gives authors a great deal of flexibility.

## Set Up a Custom Module Environment and Build Your Own Modules

Installation of the new software (or version) does not require any specific credentials.

### Prerequisites

1. An existing EasyConfig
1. a place to put your modules.

### Step by Step Guide

**Step 1:** Create a [workspace](../data_lifecycle/workspaces.md#allocate-a-workspace) where you
install your modules. You need a place where your modules are placed. This needs to be done only
once:

```console
marie@login$ ws_allocate EasyBuild 50
marie@login$ ws_list | grep 'directory.*EasyBuild'
     workspace directory  : /data/horse/ws/marie-EasyBuild
```

**Step 2:** Allocate nodes. You can do this with interactive jobs (see the example below) and/or
put commands in a batch file and source it. The latter is recommended for non-interactive jobs,
using the command `sbatch` instead of `srun`. For the sake of illustration, we use an
interactive job as an example. Depending on the partitions that you want the module to be usable on
later, you need to select nodes with the same architecture. Thus, use nodes from cluster `power` for
building, if you want to use the module on nodes of that cluster. ~~In this example, we assume
that we want to use the module on nodes with x86 architecture and thus, we use Haswell nodes.~~

```console
marie@login$ srun --nodes=1 --cpus-per-task=4 --time=08:00:00 --pty /bin/bash -l
```

!!! warning

    Using EasyBuild on the login nodes is not allowed.

**Step 3:** Specify the workspace. The rest of the guide is based on it. Please create an
environment variable called `WORKSPACE` with the path to your workspace:

```console
marie@compute$ export WORKSPACE=/data/horse/ws/marie-EasyBuild    #see output of ws_list above
```

**Step 4:** Load the correct module environment `release` according to your needs:

=== "23.04"
    ```console
    marie@compute$ module load release/23.04
    ```

**Step 5:** Load module `EasyBuild`

```console
marie@compute$ module load EasyBuild
```

**Step 6:** Set up the EasyBuild configuration.

This can be either done via environment variables:

```console
marie@compute$ export EASYBUILD_CONFIGFILES=/software/util/etc/easybuild.d/zih.cfg \
export EASYBUILD_DETECT_LOADED_MODULES=unload \
export EASYBUILD_SUBDIR_USER_MODULES= \
export EASYBUILD_BUILDPATH="/dev/shm/${USER}-EasyBuild${SLURM_JOB_ID:-}" \
export EASYBUILD_SOURCEPATH="${WORKSPACE}/sources:/software/util/sources" \
export EASYBUILD_INSTALLPATH="${WORKSPACE}/easybuild"
```

Or you can do that via the configuration file at `$HOME/.config/easybuild/config.cfg`.
An initial file can be generated with:

```console
marie@compute$ eb --confighelp > ~/.config/easybuild/config.cfg
```

Edit this file by uncommenting the above settings and specifying the respective values.
Note the difference in naming as each setting in the environment has the `EASYBUILD_` prefix
and is uppercase, while it is lowercase in the config.
For example `$EASYBUILD_DETECT_LOADED_MODULES` above corresponds to `detect-loaded-modules`
in the config file.

Note that you cannot use environment variables (like `$WORKSPACE` or `$USER`) in the config file.
So the approach with the `$EASYBUILD_` variables is more flexible but needs to be done before each
use of EasyBuild and could be forgotten.

You can also combine those approaches setting some in the config and some in the environment,
the latter will take precedence.
The first variable `$EASYBUILD_CONFIGFILES` makes sure the settings used for installing all other modules
on the cluster are used.
I.e. that config file is read before the custom one in your `$HOME`.
By that most of the configuration is already set up.\
But of course e.g. the installation path needs to be set by you.

The configuration used can be shown via:

```console
marie@compute$ eb --show-config
```

This shows all changed/non-default options while the parameter `--show-full-config` shows all options.

The hierarchical module naming scheme (used on our systems) affects e.g. location and naming of modules.
In order for EasyBuild to use the existing modules,
you need to use the "all" modules folder of the main tree.
But likely only the "Core" subdirectory is set in `$MODULEPATH`.
Nonetheless, the module search path can be extended easily with `module use`:

```console
marie@compute$ echo $MODULEPATH
/software/modules/rapids/r23.10/all/Core:/software/modules/releases/rapids
marie@compute$ module use /software/modules/rapids/r23.10/all
marie@compute$ echo $MODULEPATH
/software/modules/rapids/r23.10/all:/software/modules/rapids/r23.10/all/Core:/software/modules/releases/rapids
```

Take care to adjust the path to the release you use.
I.e. in the above example the module `release/23.10` was loaded resulting in
`/software/modules/rapids/r23.10/all/Core` on this cluster.
For the `module use` command you take that (from `$MODULEPATH`) and only strip of the `/Core`.

Or you can use this one-line command to do it automatically:

```console
marie@compute$ ml use $(echo "$MODULEPATH" | grep -oE '(^|:)[^:]+/Core:' | sed 's|/Core:||')
```

Finally, you need to tell LMod about your modules:

```console
marie@compute$ export ZIH_USER_MODULES=$EASYBUILD_INSTALLPATH
```

**Step 7:** Now search for an existing EasyConfig:

```console
marie@compute$ eb --search TensorFlow
```

**Step 8:** Build the EasyConfig and its dependencies (option `-r`)

```console
marie@compute$ eb TensorFlow-1.8.0-fosscuda-2018a-Python-3.6.4.eb -r
```

This may take a long time.

If you want to investigate what would be build by that command, first run it with `-D`:

```console
marie@compute$ eb TensorFlow-1.8.0-fosscuda-2018a-Python-3.6.4.eb -Dr
```

**Step 9:** To use your custom build modules you need to load the "base" modenv (see step 4)
and add your custom modules to the search path.

Using the variable from step 6:

```console
marie@compute$ module use "${EASYBUILD_INSTALLPATH}/modules/all"
marie@compute$ export ZIH_USER_MODULES=$EASYBUILD_INSTALLPATH
marie@compute$ export LMOD_IGNORE_CACHE=1
```

**OR** directly the path from step 1:

```console
marie@compute$ module use "/data/horse/ws/marie-EasyBuild/easybuild/modules/all"
marie@compute$ export ZIH_USER_MODULES=/data/horse/ws/marie-EasyBuild/easybuild
marie@compute$ export LMOD_IGNORE_CACHE=1
```

Then you can load it just like any other module:

```console
marie@compute$ module load TensorFlow-1.8.0-fosscuda-2018a-Python-3.6.4  #replace with the name of your module
```

The key is the `module use` command, which brings your modules into scope, so `module load` can find
them.
The `LMOD_IGNORE_CACHE` line makes `LMod` pick up the custom modules instead of searching the
system cache which doesn't include your new modules.

## Troubleshooting

When building your EasyConfig fails, you can first check the log mentioned and scroll to the bottom
to see what went wrong.

It might also be helpful to inspect the build environment EasyBuild uses. For that you can run:

```console
marie@compute$ eb myEC.eb --dump-env-script`
```

This command creates a sourceable `.env`-file with `module load` and `export` commands that show
what EasyBuild does before running, e.g., the configuration step.

It might also be helpful to use

```console
marie@compute$ export LMOD_IGNORE_CACHE=0
```

(Especially) when you want to use additional features of EasyBuild, such as the
[GitHub integration](https://docs.easybuild.io/integration-with-github/),
 you might need to set a specific Python version to use by EasyBuild.

That is unrelated to any Python module you might wish to use or install!
Furthermore, when using EasyBuild you should **not** have any other modules loaded,
not even `Python`.

Which Python executable is used by EasyBuild can be shown by executing:

```console
marie@compute$ EB_VERBOSE=1 eb --version
```

You can change it by setting `EB_PYTHON`, e.g.:

```console
marie@compute$ export EB_PYTHON=python3.8
```

In case you are using a virtualenv for use with EasyBuild
then using `python` instead of `python3.8` or similar is enough
as there will be a `python` binary available inside your virtualenv.


# Data Analytics

On ZIH systems, there are many possibilities for working with tools from the field of data
analytics. The boundaries between data analytics and machine learning are fluid.
Therefore, it may be worthwhile to search for a specific issue within the data analytics and
machine learning sections.

The following tools are available on ZIH systems, among others:

* [Python](data_analytics_with_python.md)
* [R](data_analytics_with_r.md)
* [RStudio](data_analytics_with_rstudio.md)
* [Big Data framework Spark](big_data_frameworks.md)
* [MATLAB and Mathematica](mathematics.md)

Detailed information about frameworks for machine learning, such as [TensorFlow](tensorflow.md)
and [PyTorch](pytorch.md), can be found in the [machine learning](machine_learning.md) subsection.

Other software, not listed here, can be searched with

```console
marie@compute$ module spider <software_name>
```

Refer to the section covering [modules](modules.md) for further information on the modules system.
Additional software or special versions of [individual modules](custom_easy_build_environment.md)
can be installed individually by each user. If possible, the use of
[virtual environments](python_virtual_environments.md) is
recommended (e.g. for Python). Likewise, software can be used within [containers](containers.md).

For the transfer of larger amounts of data into and within the system, the
[export nodes and Datamover](../data_transfer/overview.md) should be used.
Data is stored in the [workspaces](../data_lifecycle/workspaces.md).
Software modules or virtual environments can also be installed in workspaces to enable
collaborative work even within larger groups.
<!--General recommendations for setting up workflows can be found in the experiments section.-->


# Data Analytics with Python

Python is a high-level interpreted language widely used in research and science. Using ZIH system
allows you to work with Python quicker and more effective. Here, a general introduction to working
with Python on ZIH systems is given. Further documentation is available for specific
[machine learning frameworks](machine_learning.md).

## Python Console and Virtual Environments

Often, it is useful to create an isolated development environment, which can be shared among
a research group and/or teaching class. For this purpose,
[Python virtual environments](python_virtual_environments.md) can be used.

!!! hint
    For working with conda virtual environments, it may be necessary to configure your shell as
    described in [Python virtual environments](python_virtual_environments.md#conda-virtual-environment)

The interactive Python interpreter can also be used on ZIH systems via an interactive job:

```console
marie@login$ srun --gres=gpu:1 --ntasks=1 --cpus-per-task=7 --pty --mem-per-cpu=8000 bash
marie@compute$ module load Python
marie@compute$ python
Python 3.8.6 (default, Feb 17 2021, 11:48:51)
[GCC 10.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>>
```

## Jupyter Notebooks

Jupyter notebooks allow to analyze data interactively using your web browser. One advantage of
Jupyter is, that code, documentation and visualization can be included in a single notebook, so that
it forms a unit. Jupyter notebooks can be used for many tasks, such as data cleaning and
transformation, numerical simulation, statistical modeling, data visualization and also machine
learning.

On ZIH systems, a [JupyterHub](../access/jupyterhub.md) is available, which can be used to run a
Jupyter notebook on a node, using a GPU when needed.

## Parallel Computing with Python

### Pandas with Pandarallel

[Pandas](https://pandas.pydata.org/){:target="_blank"} is a widely used library for data
analytics in Python.
In many cases, an existing source code using Pandas can be easily modified for parallel execution by
using the [pandarallel](https://github.com/nalepae/pandarallel/tree/v1.5.2) module. The number of
threads that can be used in parallel depends on the number of cores (parameter `--cpus-per-task`)
within the Slurm request, e.g.

```console
marie@login$ srun --cpus-per-task=4 --mem=2G --hint=nomultithread --pty --time=8:00:00 bash
```

The above request allows to use 4 parallel threads.

The following example shows how to parallelize the apply method for pandas dataframes with the
pandarallel module. If the pandarallel module is not installed already, use a
[virtual environment](python_virtual_environments.md) to install the module.

??? example

    ```python
    import pandas as pd
    import numpy as np
    from pandarallel import pandarallel

    pandarallel.initialize()
    # unfortunately the initialize method gets the total number of physical cores without
    # taking into account allocated cores by Slurm, but the choice of the -c parameter is of relevance here

    N_rows = 10**5
    N_cols = 5
    df = pd.DataFrame(np.random.randn(N_rows, N_cols))

    # here some function that needs to be executed in parallel
    def transform(x):
        return(np.mean(x))

    print('calculate with normal apply...')
    df.apply(func=transform, axis=1)

    print('calculate with pandarallel...')
    df.parallel_apply(func=transform, axis=1)
    ```
For more examples of using pandarallel check out
[https://github.com/nalepae/pandarallel/blob/master/docs/examples.ipynb](https://github.com/nalepae/pandarallel/blob/master/docs/examples_mac_linux.ipynb).

### Dask

[Dask](https://dask.org/) is a flexible and open-source library
for parallel computing in Python.
It replaces some Python data structures with parallel versions
in order to provide advanced
parallelism for analytics, enabling performance at scale
for some of the popular tools.
For instance: Dask arrays replace NumPy arrays,
Dask dataframes replace Pandas dataframes.
Furthermore, Dask-ML scales machine learning APIs like Scikit-Learn and XGBoost.

Dask is composed of two parts:

- Dynamic task scheduling optimized for computation and interactive
  computational workloads.
- Big Data collections like parallel arrays, data frames, and lists that extend common interfaces
  like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments.
  These parallel collections run on top of dynamic task schedulers.

Dask supports several user interfaces:

- High-Level
    - Arrays: Parallel NumPy
    - Bags: Parallel lists
    - DataFrames: Parallel Pandas
    - Machine Learning: Parallel Scikit-Learn
    - Others from external projects, like XArray
- Low-Level
    - Delayed: Parallel function evaluation
    - Futures: Real-time parallel function evaluation

#### Dask Modules on ZIH Systems

On ZIH systems, Dask is available as a module.
Check available versions and load your preferred one:

```console
marie@compute$ module spider dask
------------------------------------------------------------------------------------------
    dask:
----------------------------------------------------------------------------------------------
    Versions:
        dask/2.8.0-fosscuda-2019b-Python-3.7.4
        dask/2.8.0-Python-3.7.4
        dask/2.8.0 (E)
[...]
marie@compute$ module load dask/2.8.0-fosscuda-2019b-Python-3.7.4
marie@compute$ python -c "import dask; print(dask.__version__)"
2021.08.1
```

The preferred way is to use Dask as a separate module as was described above.
However, you can use it as part of the **Anaconda** module, e.g: `module load Anaconda3`.

#### Scheduling by Dask

Whenever you use functions on Dask collections (Dask Array, Dask Bag, etc.), Dask models these as
single tasks forming larger task graphs in the background without you noticing.
After Dask generates these task graphs,
it needs to execute them on parallel hardware.
This is the job of a task scheduler.
Please use Distributed scheduler for your
Dask computations on the cluster and avoid using a Single machine scheduler.

##### Distributed Scheduler

There are a variety of ways to set Distributed scheduler.
However, `dask.distributed` scheduler will be used for many of them.
To use the `dask.distributed` scheduler you must set up a Client:

```python
from dask.distributed import Client
client = Client(...)  # Connect to distributed cluster and override default
df.x.sum().compute()  # This now runs on the distributed system
```

The idea behind Dask is to scale Python and distribute computation among the workers (multiple
machines, jobs).
The preferred and simplest way to run Dask on ZIH systems
today both for new or experienced users
is to use **[dask-jobqueue](https://jobqueue.dask.org/)**.

However, Dask-jobqueue is slightly oriented toward
interactive analysis
usage, and it might be better to use tools like
**[Dask-mpi](https://docs.dask.org/en/latest/setup/hpc.html#using-mpi)**
in some routine batch production workloads.

##### Dask-mpi

You can launch a Dask network using
`mpirun` or `mpiexec` and the `dask-mpi` command line executable.
This depends on the [mpi4py library](#mpi4py-mpi-for-python).
For more detailed information, please check
[the official documentation](https://docs.dask.org/en/latest/setup/hpc.html#using-mpi).

##### Dask-jobqueue

[Dask-jobqueue](https://jobqueue.dask.org/) can be used as the standard way
to use dask for most users.
It allows an easy deployment of Dask Distributed on HPC with Slurm
or other job queuing systems.

Dask-jobqueue is available as an extension
for a Dask module (which can be loaded by: `module load dask`).

The availability of the exact packages (such a Dask-jobqueue)
in the module can be checked by the
`module whatis <name_of_the_module>` command, e.g. `module whatis dask`.

Moreover, it is possible to install and use `dask-jobqueue`
in your local python environments.
You can install Dask-jobqueue with `pip` or `conda`.

###### Example of Using Dask-Jobqueue with SLURMCluster

[Dask-jobqueue](https://jobqueue.dask.org/en/latest/howitworks.html#workers-vs-jobs)
allows running jobs on the ZIH system
inside the python code and scale computations over the jobs.
[Dask-jobqueue](https://jobqueue.dask.org/en/latest/howitworks.html#workers-vs-jobs)
creates a Dask Scheduler in the Python process
where the cluster object is instantiated.
Please check the example of a definition of the cluster object
for the cluster `Alpha` (queue at the dask terms) on the ZIH system:

```python
from dask_jobqueue import SLURMCluster

cluster = SLURMCluster(queue='alpha',
  cores=4,
  processes=2,
  project='p_number_crunch',
  memory="8GB",
  walltime="00:30:00")

```

These parameters above specify the characteristics of a
single job or a single compute node,
rather than the characteristics of your computation as a whole.
It hasn’t actually launched any jobs yet.
For the full computation, you will then ask for a number of
jobs using the scale command, e.g : `cluster.scale(2)`.
Thus, you have to specify a `SLURMCluster` by `dask_jobqueue`,
scale it and use it for your computations. There is an example:

```python
from distributed import Client
from dask_jobqueue import SLURMCluster
from dask import delayed

cluster = SLURMCluster(
  cores=8,
  processes=2,
  project='p_number_crunch',
  memory="80GB",
  walltime="00:30:00",
  extra=['--resources gpu=1'])

cluster.scale(2)             #scale it to 2 workers!
client = Client(cluster)     #command will show you number of workers (python objects corresponds to jobs)
```

Please have a look at the `extra` parameter in the script above.
This could be used to specify a
special hardware availability that the scheduler
is not aware of, for example, GPUs.
Please don't forget to specify the name of your project.

The Python code for setting up Slurm clusters
and scaling clusters can be run by the `srun`
(but remember that using `srun` directly on the shell
blocks the shell and launches an
interactive job) or batch jobs or
[JupyterHub](../access/jupyterhub.md) with loaded Dask
(by module or by Python virtual environment).

!!! note
    The job to run original code (de facto an interface) with a setup should be simple and light.
    Please don't use a lot of resources for that.

The following example shows using
Dask by `dask-jobqueue` with `SLURMCluster` and `dask.array`
for the Monte-Carlo estimation of Pi.

??? example "Example of using SLURMCluster"

    ```python
    #use of dask-jobqueue for the estimation of Pi by Monte-Carlo method

    import time
    from time import time, sleep
    from dask.distributed import Client
    from dask_jobqueue import SLURMCluster
    import subprocess as sp

    import dask.array as da
    import numpy as np

    #setting up the dashboard

    uid = int( sp.check_output('id -u', shell=True).decode('utf-8').replace('\n','') )
    portdash = 10001 + uid

    #create a Slurm cluster, please specify your project

    cluster = SLURMCluster(cores=2, project='p_number_crunch', memory="8GB", walltime="00:30:00", extra=['--resources gpu=1'], scheduler_options={"dashboard_address": f":{portdash}"})

    #submit the job to the scheduler with the number of nodes (here 2) requested:

    cluster.scale(2)

    #wait for Slurm to allocate a resources

    sleep(120)

    #check resources

    client = Client(cluster)
    client

    #real calculations with a Monte Carlo method

    def calc_pi_mc(size_in_bytes, chunksize_in_bytes=200e6):
      """Calculate PI using a Monte Carlo estimate."""

      size = int(size_in_bytes / 8)
      chunksize = int(chunksize_in_bytes / 8)

      xy = da.random.uniform(0, 1, size=(size / 2, 2), chunks=(chunksize / 2, 2))

      in_circle = ((xy ** 2).sum(axis=-1) < 1)
      pi = 4 * in_circle.mean()

      return pi

    def print_pi_stats(size, pi, time_delta, num_workers):
      """Print pi, calculate offset from true value, and print some stats."""
      print(f"{size / 1e9} GB\n"
            f"\tMC pi: {pi : 13.11f}"
            f"\tErr: {abs(pi - np.pi) : 10.3e}\n"
            f"\tWorkers: {num_workers}"
            f"\t\tTime: {time_delta : 7.3f}s")

    #let's loop over different volumes of double-precision random numbers and estimate it

    for size in (1e9 * n for n in (1, 10, 100)):

      start = time()
      pi = calc_pi_mc(size).compute()
      elaps = time() - start

      print_pi_stats(size, pi, time_delta=elaps, num_workers=len(cluster.scheduler.workers))

    #Scaling the Cluster to twice its size and re-run the experiments

    new_num_workers = 2 * len(cluster.scheduler.workers)

    print(f"Scaling from {len(cluster.scheduler.workers)} to {new_num_workers} workers.")

    cluster.scale(new_num_workers)

    sleep(120)

    client

    #Re-run same experiments with doubled cluster

    for size in (1e9 * n for n in (1, 10, 100)):

      start = time()
      pi = calc_pi_mc(size).compute()
      elaps = time() - start

      print_pi_stats(size, pi, time_delta=elaps, num_workers=len(cluster.scheduler.workers))
    ```

Please check the availability of resources that you want to allocate
by the script for the example above.
You can do it with `sinfo` command. The script doesn't work
without available cluster resources.

### Mpi4py -  MPI for Python

Message Passing Interface (MPI) is a standardized and
portable message-passing standard, designed to
function on a wide variety of parallel computing architectures.

Mpi4py (MPI for Python) provides bindings of the MPI standard for
the Python programming language,
allowing any Python program to exploit multiple processors.

Mpi4py is based on MPI-2 C++ bindings. It supports almost all MPI calls.
It supports communication of pickle-able Python objects.
Mpi4py provides optimized communication of NumPy arrays.

Mpi4py is included in the SciPy-bundle modules on the ZIH system.

```console
marie@compute$ module load SciPy-bundle/2020.11-foss-2020b
Module SciPy-bundle/2020.11-foss-2020b and 28 dependencies loaded.
marie@compute$ pip list
Package                       Version
----------------------------- ----------
[...]
mpi4py                        3.0.3
[...]
```

Other versions of the package can be found with:

```console
marie@compute$ module spider mpi4py
-----------------------------------------------------------------------------------------------------------------------------------------
  mpi4py:
-----------------------------------------------------------------------------------------------------------------------------------------
     Versions:
        mpi4py/1.3.1
        mpi4py/2.0.0-impi
        mpi4py/3.0.0 (E)
        mpi4py/3.0.2 (E)
        mpi4py/3.0.3 (E)

Names marked by a trailing (E) are extensions provided by another module.

-----------------------------------------------------------------------------------------------------------------------------------------
  For detailed information about a specific "mpi4py" package (including how to load the modules), use the module's full name.
  Note that names that have a trailing (E) are extensions provided by other modules.
  For example:

     $ module spider mpi4py/3.0.3
-----------------------------------------------------------------------------------------------------------------------------------------
```

Moreover, it is possible to install mpi4py in your local conda
environment.

The example of mpi4py usage for the verification that
mpi4py is running correctly can be found below:

```python
from mpi4py import MPI
comm = MPI.COMM_WORLD
print("%d of %d" % (comm.Get_rank(), comm.Get_size()))
```

For the multi-node case, use a script similar to this:

```bash
#!/bin/bash
#SBATCH --nodes=2
#SBATCH --tasks-per-node=2
#SBATCH --cpus-per-task=1

module load release/23.04
module load Anaconda3/2022.05

eval "$(conda shell.bash hook)"
conda activate /home/marie/conda-virtual-environment/kernel2 && srun python mpi4py_test.py    #specify name of your virtual environment
```

For the verification of the multi-node case,
you can use the Python code from the previous part
(with verification of the installation) as a test file.


# Data Analytics with R

[R](https://www.r-project.org/about.html) is a programming language and environment for statistical
computing and graphics. It provides a wide variety of statistical (linear and nonlinear modeling,
classical statistical tests, time-series analysis, classification, etc.), machine learning
algorithms and graphical techniques.  R is an integrated suite of software facilities for data
manipulation, calculation and graphing.

We recommend using the clusters Barnard and/or Romeo to work with R. For more details
see our [hardware documentation](../jobs_and_resources/hardware_overview.md).

## R Console

In the following example, the `srun` command is used to start an interactive job, so that the output
is visible to the user. Please check the [Slurm page](../jobs_and_resources/slurm.md) for details.

```console
marie@login.barnard$ srun --ntasks=1 --nodes=1 --cpus-per-task=4 --mem-per-cpu=2541 --time=01:00:00 --pty bash
[marie@barnard ]$ module load release/23.10  GCC/11.3.0  OpenMPI/4.1.4 R/4.2.1
Module GCC/11.3.0, OpenMPI/4.1.4, R/4.2.1 and 109 dependencies loaded.
[marie@barnard ]$ which R
/software/rapids/r23.10/R/4.2.1-foss-2022a/bin/R
```

Using interactive sessions is recommended only for short test runs, while for larger runs batch jobs
should be used. Examples can be found on the [Slurm page](../jobs_and_resources/slurm.md).

It is also possible to run `Rscript` command directly (after loading the module):

```console
marie@barnard$ Rscript </path/to/script/your_script.R> <param1> <param2>
```

## R in JupyterHub

In addition to using interactive and batch jobs, it is possible to work with R using
[JupyterHub](../access/jupyterhub.md).

The production and test [environments](../access/jupyterhub.md#standard-profiles) of
JupyterHub contain R kernel. It can be started either in the notebook or in the console.

## RStudio

For using R with RStudio please refer to the documentation on
[Data Analytics with RStudio](data_analytics_with_rstudio.md).

## Install Packages in R

By default, user-installed packages are saved in the users home in a folder depending on
the architecture (`x86` or `PowerPC`). Therefore the packages should be installed using interactive
jobs on the compute node:

```console
marie@compute$ module load R
[...]
Module R/3.6.0-foss-2019a and 56 dependencies loaded.
marie@compute$ R -e 'install.packages("ggplot2")'
[...]
```

## Deep Learning with R

The deep learning frameworks perform extremely fast when run on accelerators such as GPU.
Therefore, using nodes with built-in GPUs, e.g., clusters
[`Capella`](../jobs_and_resources/hardware_overview.md#capella),
[`Alpha`](../jobs_and_resources/hardware_overview.md#alpha-centauri) and
[`Power9`](../jobs_and_resources/hardware_overview.md#power9) is beneficial for the examples here.

### R Interface to TensorFlow

The ["TensorFlow" R package](https://tensorflow.rstudio.com/) provides R users access to the
TensorFlow framework. [TensorFlow](https://www.tensorflow.org/) is an open-source software library
for numerical computation using data flow graphs.

The respective modules can be loaded with the following

```console
marie@compute$ module load R/3.6.2-fosscuda-2019b
[...]
Module R/3.6.2-fosscuda-2019b and 63 dependencies loaded.
marie@compute$ module load TensorFlow/2.3.1-fosscuda-2019b-Python-3.7.4
Module TensorFlow/2.3.1-fosscuda-2019b-Python-3.7.4 and 15 dependencies loaded.
```

!!! warning

     Be aware that for compatibility reasons it is important to choose [modules](modules.md) with
     the same toolchain version (in this case `fosscuda/2019b`).

In order to interact with Python-based frameworks (like TensorFlow) `reticulate` R library is used.
To configure it to point to the correct Python executable in your virtual environment, create
a file named `.Rprofile` in your project directory (e.g. R-TensorFlow) with the following
contents:

```R
Sys.setenv(RETICULATE_PYTHON = "/sw/installed/Python/3.7.4-GCCcore-8.3.0/bin/python")    #assign RETICULATE_PYTHON to the python executable
```

Let's start R, install some libraries and evaluate the result:

```rconsole
> install.packages(c("reticulate", "tensorflow"))
Installing packages into ‘~/R/x86_64-pc-linux-gnu-library/3.6’
(as ‘lib’ is unspecified)
> reticulate::py_config()
python:         /software/rome/Python/3.7.4-GCCcore-8.3.0/bin/python
libpython:      /sw/installed/Python/3.7.4-GCCcore-8.3.0/lib/libpython3.7m.so
pythonhome:     /software/rome/Python/3.7.4-GCCcore-8.3.0:/software/rome/Python/3.7.4-GCCcore-8.3.0
version:        3.7.4 (default, Mar 25 2020, 13:46:43)  [GCC 8.3.0]
numpy:          /software/rome/SciPy-bundle/2019.10-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/numpy
numpy_version:  1.17.3

NOTE: Python version was forced by RETICULATE_PYTHON

> library(tensorflow)
2021-08-26 16:11:47.110548: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
> tf$constant("Hello TensorFlow")
2021-08-26 16:14:00.269248: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-08-26 16:14:00.674878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:0b:00.0 name: A100-SXM4-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s
[...]
tf.Tensor(b'Hello TensorFlow', shape=(), dtype=string)
```

??? example

    The example shows the use of the TensorFlow package with the R for the classification problem
    related to the MNIST data set.
    ```R
    library(tensorflow)
    library(keras)

    # Data preparation
    batch_size <- 128
    num_classes <- 10
    epochs <- 12

    # Input image dimensions
    img_rows <- 28
    img_cols <- 28

    # Shuffled and split the data between train and test sets
    mnist <- dataset_mnist()
    x_train <- mnist$train$x
    y_train <- mnist$train$y
    x_test <- mnist$test$x
    y_test <- mnist$test$y

    # Redefine dimension of train/test inputs
    x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
    x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
    input_shape <- c(img_rows, img_cols, 1)

    # Transform RGB values into [0,1] range
    x_train <- x_train / 255
    x_test <- x_test / 255

    cat('x_train_shape:', dim(x_train), '\n')
    cat(nrow(x_train), 'train samples\n')
    cat(nrow(x_test), 'test samples\n')

    # Convert class vectors to binary class matrices
    y_train <- to_categorical(y_train, num_classes)
    y_test <- to_categorical(y_test, num_classes)

    # Define Model
    model <- keras_model_sequential() %>%
      layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
                    input_shape = input_shape) %>%
      layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
      layer_max_pooling_2d(pool_size = c(2, 2)) %>%
      layer_dropout(rate = 0.25) %>%
      layer_flatten() %>%
      layer_dense(units = 128, activation = 'relu') %>%
      layer_dropout(rate = 0.5) %>%
      layer_dense(units = num_classes, activation = 'softmax')

    # Compile model
    model %>% compile(
      loss = loss_categorical_crossentropy,
      optimizer = optimizer_adadelta(),
      metrics = c('accuracy')
    )

    # Train model
    model %>% fit(
      x_train, y_train,
      batch_size = batch_size,
      epochs = epochs,
      validation_split = 0.2
    )
    scores <- model %>% evaluate(
      x_test, y_test, verbose = 0
    )

    # Output metrics
    cat('Test loss:', scores[[1]], '\n')
    cat('Test accuracy:', scores[[2]], '\n')
    ```

## Parallel Computing with R

Generally, the R code is serial. However, many computations in R can be made faster by the use of
parallel computations. This section concentrates on most general methods and examples.
The [parallel](https://www.rdocumentation.org/packages/parallel/versions/3.6.2) library
will be used below.

!!! warning

    Please do not install or update R packages related to parallelism as it could lead to
    conflicts with other preinstalled packages.

### Basic lapply-Based Parallelism

`lapply()` function is a part of base R. lapply is useful for performing operations on list-objects.
Roughly speaking, lapply is a vectorization of the source code and it is the first step before
explicit parallelization of the code.

### Shared-Memory Parallelism

The `parallel` library includes the `mclapply()` function which is a shared memory version of
lapply. The "mc" stands for "multicore". This function distributes the `lapply` tasks across
multiple CPU cores to be executed in parallel.

This is a simple option for parallelization. It doesn't require much effort to rewrite the serial
code to use `mclapply` function. Check out an example below.

??? example

    ```R
    library(parallel)

    # here some function that needs to be executed in parallel
    average <- function(size){
      norm_vector <- rnorm(n=size, mean=mu, sd=sigma)
      return(mean(norm_vector))
    }

    # variable initialization
    mu <- 1.0
    sigma <- 1.0
    vector_length <- 10^7
    n_repeat <- 100
    sample_sizes <- rep(vector_length, times=n_repeat)


    # shared-memory version
    threads <- as.integer(Sys.getenv("SLURM_CPUS_ON_NODE"))
    # here the name of the variable depends on the correct sbatch configuration
    # unfortunately the built-in function gets the total number of physical cores without
    # taking into account allocated cores by Slurm

    list_of_averages <- mclapply(X=sample_sizes, FUN=average, mc.cores=threads)  # apply function "average" 100 times
    ```

The disadvantages of using shared-memory parallelism approach are, that the number of parallel tasks
is limited to the number of cores on a single node. The maximum number of cores on a single node can
be found in our [hardware documentation](../jobs_and_resources/hardware_overview.md).

Submitting a multicore R job to Slurm is very similar to submitting an
[OpenMP Job](../jobs_and_resources/binding_and_distribution_of_tasks.md),
since both are running multicore jobs on a **single** node. Below is an example:

```Bash
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --tasks-per-node=1
#SBATCH --cpus-per-task=16
#SBATCH --time=00:10:00
#SBATCH --output=test_Rmpi.out
#SBATCH --error=test_Rmpi.err

module purge
module load release/23.10  GCC/11.3.0  OpenMPI/4.1.4
module load R/4.2.1

R CMD BATCH Rcode.R
```

### Distributed-Memory Parallelism

In order to go beyond the limitation of the number of cores on a single node, a cluster of workers
shall be set up. There are three options for it: MPI, PSOCK and FORK clusters.
We use `makeCluster` function from `parallel` library to create a set of copies of R processes
running in parallel. The desired type of the cluster can be specified with a parameter `TYPE`.

#### MPI Cluster

This way of the R parallelism uses the
[Rmpi](http://cran.r-project.org/web/packages/Rmpi/index.html) package and the
[MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) (Message Passing Interface) as a
"back-end" for its parallel operations. The MPI-based job in R is very similar to submitting an
[MPI Job](../jobs_and_resources/binding_and_distribution_of_tasks.md) since both are running
multicore jobs on multiple nodes. Below is an example of running R script with the Rmpi on the ZIH
system:

```Bash
#!/bin/bash
#SBATCH --ntasks=16              # this parameter determines how many processes will be spawned, please use >=8
#SBATCH --cpus-per-task=1
#SBATCH --time=01:00:00
#SBATCH --output=test_Rmpi.out
#SBATCH --error=test_Rmpi.err

module purge
module load release/23.10  GCC/11.3.0  OpenMPI/4.1.4
module load R/4.2.1

mpirun -np 1 R CMD BATCH Rmpi.R   # specify the absolute path to the R script, like: /scratch/ws/marie-Work/R/Rmpi.R

# submit with sbatch <script_name>
```

Slurm option `--ntasks` controls the total number of parallel tasks. The number of
nodes required to complete this number of tasks will be automatically selected.
However, in some specific cases, you can specify the number of nodes and the number of necessary
tasks per node explicitly:

```Bash
#SBATCH --nodes=2
#SBATCH --tasks-per-node=16
#SBATCH --cpus-per-task=1

module purge
module load release/23.10  GCC/11.3.0  OpenMPI/4.1.4
module load R/4.2.1

mpirun -np 1 R CMD BATCH --no-save --no-restore Rmpi_c.R
```

Use an example below, where 32 global ranks are distributed over 2 nodes with 16 cores each.
Each MPI rank has 1 core assigned to it.

??? example

    ```R
    library(Rmpi)

    # initialize an Rmpi environment
    ns <- mpi.universe.size()-1
    mpi.spawn.Rslaves(nslaves=ns)

    # send these commands to the slaves
    mpi.bcast.cmd( id <- mpi.comm.rank() )
    mpi.bcast.cmd( ns <- mpi.comm.size() )
    mpi.bcast.cmd( host <- mpi.get.processor.name() )

    # all slaves execute this command
    mpi.remote.exec(paste("I am", id, "of", ns, "running on", host))

    # close down the Rmpi environment
    mpi.close.Rslaves(dellog = FALSE)
    mpi.exit()
    ```

Another example:

??? example

    ```R
    library(Rmpi)
    library(parallel)

    # here some function that needs to be executed in parallel
    average <- function(size){
      norm_vector <- rnorm(n=size, mean=mu, sd=sigma)
      return(mean(norm_vector))
    }

    # variable initialization
    mu <- 1.0
    sigma <- 1.0
    vector_length <- 10^7
    n_repeat <- 100
    sample_sizes <- rep(vector_length, times=n_repeat)

    # cluster setup
    # get number of available MPI ranks
    threads = mpi.universe.size()-1
    print(paste("The cluster of size", threads, "will be setup..."))

    # initialize MPI cluster
    cl <- makeCluster(threads, type="MPI", outfile="")

    # distribute required variables for the execution over the cluster
    clusterExport(cl, list("mu","sigma"))

    list_of_averages <- parLapply(X=sample_sizes, fun=average, cl=cl)

    # shut down the cluster
    #snow::stopCluster(cl)  # usually it hangs over here with Open MPI > 2.0. In this case this command may be avoided, Slurm will clean up after the job finishes
    ```

To use Rmpi and MPI please use one of these clusters: `Alpha`, `Barnard` or `Romeo`.

Use `mpirun` command to start the R script. It is a wrapper that enables the communication
between processes running on different nodes. It is important to use `-np 1` (the number of spawned
processes by `mpirun`), since the R takes care of it with `makeCluster` function.

#### PSOCK cluster

The `type="PSOCK"` uses TCP sockets to transfer data between nodes. PSOCK is the default on *all*
systems. The advantage of this method is that it does not require external libraries such as MPI.
On the other hand, TCP sockets are relatively
[slow](http://glennklockwood.blogspot.com/2013/06/whats-killing-cloud-interconnect.html). Creating
a PSOCK cluster is similar to launching an MPI cluster, but instead of specifying the number of
parallel workers, you have to manually specify the number of nodes according to the
hardware specification and parameters of your job.

??? example

    ```R
    library(parallel)

    # a function that needs to be executed in parallel
    average <- function(size){
      norm_vector <- rnorm(n=size, mean=mu, sd=sigma)
      return(mean(norm_vector))
    }

    # variable initialization
    mu <- 1.0
    sigma <- 1.0
    vector_length <- 10^7
    n_repeat <- 100
    sample_sizes <- rep(vector_length, times=n_repeat)

    # cluster setup

    # get number of available nodes (should be equal to "ntasks")
    mynodes = 8
    print(paste("The cluster of size", threads, "will be setup..."))

    # initialize cluster
    cl <- makeCluster(mynodes, type="PSOCK", outfile="")

    # distribute required variables for the execution over the cluster
    clusterExport(cl, list("mu","sigma"))

    list_of_averages <- parLapply(X=sample_sizes, fun=average, cl=cl)

    # shut down the cluster
    print(paste("Program finished"))
    ```

#### FORK Cluster

The `type="FORK"` method behaves exactly like the `mclapply` function discussed in the previous
section. Like `mclapply`, it can only use the cores available on a single node. However this method
requires exporting the workspace data to other processes. The FORK method in a combination with
`parLapply` function might be used in situations, where different source code should run on each
parallel process.

### Other Parallel Options

- [foreach](https://cran.r-project.org/web/packages/foreach/index.html) library.
  It is functionally equivalent to the
  [lapply-based parallelism](https://www.glennklockwood.com/data-intensive/r/lapply-parallelism.html)
  discussed before but based on the for-loop
- [future](https://cran.r-project.org/web/packages/future/index.html)
  library. The purpose of this package is to provide a lightweight and
  unified Future API for sequential and parallel processing of R
  expression via futures
- [Poor-man's parallelism](https://www.glennklockwood.com/data-intensive/r/alternative-parallelism.html#6-1-poor-man-s-parallelism)
  (simple data parallelism). It is the simplest, but not an elegant way to parallelize R code.
  It runs several copies of the same R script where each copy reads a different part of the input
  data.
- [Hands-off (OpenMP)](https://www.glennklockwood.com/data-intensive/r/alternative-parallelism.html#6-2-hands-off-parallelism)
  method. R has [OpenMP](https://www.openmp.org/resources/) support. Thus using OpenMP is a simple
  method where you don't need to know much about the parallelism options in your code. Please be
  careful and don't mix this technique with other methods!


# Data Analytics with RStudio

[RStudio](https://rstudio.com/) is an integrated development environment (IDE) for R. It includes
a console, syntax-highlighting editor that supports direct code execution, as well as tools for
plotting, history, debugging and workspace management. RStudio is also available on ZIH systems.

The easiest option is to run RStudio in JupyterHub directly in the browser. It can be started
similarly to a new kernel from [JupyterLab](../access/jupyterhub.md#jupyterlab) launcher.

![RStudio launcher in JupyterHub](misc/data_analytics_with_rstudio_launcher.jpg)
{: style="width:90%" }

!!! tip
    If an error "could not start RStudio in time" occurs, try reloading the web page with `F5`.


# Debugging

Debugging is an essential but also rather time consuming step during application development. Tools
dramatically reduce the amount of time spent to detect errors. Besides the "classical" serial
programming errors, which may usually be easily detected with a regular debugger, there exist
programming errors that result from the usage of OpenMP, Pthreads, or MPI. These errors may also be
detected with debuggers (preferably debuggers with support for parallel applications), however,
specialized tools like MPI checking tools (e.g. Marmot) or thread checking tools (e.g. Intel Thread
Checker) can simplify this task.

This page provides detailed information on classic debugging at ZIH systems.  The more specific
topic [MPI Usage Error Detection](mpi_usage_error_detection.md) covers tools to detect MPI usage
errors.

## Overview of available Debuggers at ZIH

| | GDB | Arm DDT  |
|---|:---|:---|
| Interface          | Command line   | Graphical user interface |
| Languages          | C/C++, Fortran | C/C++, Fortran, Python (limited) |
| Parallel Debugging | Threads        | Threads, MPI, GPU, hybrid |
| Licenses at ZIH    | Free           | 1024 (max. number of processes/threads) |
| Official documentation | [GDB website](https://www.gnu.org/software/gdb/) | [Arm DDT website](https://developer.arm.com/tools-and-software/server-and-hpc/debug-and-profile/arm-forge/arm-ddt) |

## General Advice

- You need to compile your code with the flag `-g` to enable
  debugging. This tells the compiler to include information about
  variable and function names, source code lines etc. into the
  executable.
- It is also recommendable to reduce or even disable optimizations
  (`-O0` or gcc's `-Og`). At least inlining should be disabled (usually
  `-fno-inline`).
- For parallel applications: try to reproduce the problem with less
  processes or threads before using a parallel debugger.
- Use the compiler's check capabilities to find typical problems at
  compile time or run time, read the manual (`man gcc`, `man ifort`, etc.)
  - Intel C++ example: `icpc -g -std=c++14 -w3 -check=stack,uninit -check-pointers=rw -fp-trap=all`
  - Intel Fortran example: `ifort -g -std03 -warn all -check all -fpe-all=0 -traceback`
  - The flag `-traceback` of the Intel Fortran compiler causes to print
    stack trace and source code location when the program terminates
    abnormally.
- If your program crashes and you get an address of the failing
  instruction, you can get the source code line with the command
  `addr2line -e <executable> <address>` (if compiled with `-g`).
- Use [Memory Debuggers](#memory-debugging) to
  verify the proper usage of memory.
- Core dumps are useful when your program crashes after a long
  runtime.
- Slides from user training: [Introduction to Parallel Debugging](misc/debugging_intro.pdf)

## GNU Debugger (GDB)

The GNU Debugger (GDB) offers only limited to no support for parallel
applications and Fortran 90. However, it might be the debugger you are
most used to. GDB works best for serial programs. You can start GDB in
several ways:

|                               | Command                        |
|-------------------------------|:-------------------------------|
| Run program under GDB         | `gdb <executable>`             |
| Attach running program to GDB | `gdb --pid <process ID>`       |
| Open a core dump              | `gdb <executable> <core file>` |

This [GDB Reference Sheet](http://users.ece.utexas.edu/~adnan/gdb-refcard.pdf) makes life easier
when you often use GDB.

Fortran 90 programmers may issue an `module load DDT` before their debug session. This makes the GDB
modified by DDT available, which has better support for Fortran 90 (e.g.  derived types).

## Arm DDT

![DDT Main Window](misc/ddt-main-window.png)

- Intuitive graphical user interface and great support for parallel applications
- We have 1024 licenses, so many user can use this tool for parallel debugging
- Don't expect that debugging an MPI program with hundreds of processes will always work without
  problems
  - The more processes and nodes involved, the higher is the probability for timeouts or other
    problems
  - Debug with as few processes as required to reproduce the bug you want to find
- Module to load before using: `module load DDT` Start: `ddt <executable>`
    - If the GUI runs too slow over your remote connection: Use
[WebVNC](../access/graphical_applications_with_webvnc.md) to start a remote desktop session in
  a web browser.
- Slides from user training: [Parallel Debugging with DDT](misc/debugging_ddt.pdf)

### Serial Program Example

```console
marie@login$ module load DDT
Module DDT/24.0.5 loaded.
marie@login$ srun --pty --x11=first --ntasks=1 --time=2:00:00 bash
srun: job 123456 queued and waiting for resources
srun: job 123456 has been allocated resources
marie@compute$ ddt ./myprog
```

- Run dialog window of DDT opens.
- Optionally: configure options like program arguments.
- Hit *Run*.

### Multi-threaded Program Example

```console
marie@login$ module load DDT
Module DDT/24.0.5 loaded.
marie@login$ srun --pty --x11=first --ntasks=1 --cpus-per-task=5 --time=2:00:00 bash
srun: job 123457 queued and waiting for resources
srun: job 123457 has been allocated resources
marie@compute$ ddt ./myprog
```

- Run dialog window of DDT opens.
- Optionally: configure options like program arguments.
- If OpenMP: set number of threads.
- Hit *Run*.

### MPI-Parallel Program Example

```console
marie@login$ salloc --x11=first --ntasks=2 --time=2:00:00
salloc: Pending job allocation 123458
salloc: job 123458 queued and waiting for resources
salloc: job 123458 has been allocated resources
salloc: Granted job allocation 123458
marie@login$ ddt srun ./myprog
```

- Run dialog window of DDT opens.
- If MPI-OpenMP-hybrid: set number of threads.
- Hit *Run*

## Memory Debugging

- Memory debuggers find memory management bugs, e.g.
  - Use of non-initialized memory
  - Access memory out of allocated bounds
- DDT has memory debugging included (needs to be enabled in the run dialog)

### Valgrind (Memcheck)

- Simulation of the program run in a virtual machine which accurately observes memory operations.
- Extreme run time slow-down: use small program runs!
- Finds more memory errors than other debuggers.
- Further information:
  - [Valgrind Website](http://www.valgrind.org)
  - [Memcheck Manual](https://www.valgrind.org/docs/manual/mc-manual.html)
    (explanation of output, command-line options)
- For serial or multi-threaded programs:

```console
marie@login$ module load Valgrind
Module Valgrind/3.14.0-foss-2018b and 12 dependencies loaded.
marie@login$ srun --ntasks=1 valgrind ./myprog
```

- Not recommended for MPI parallel programs, since usually the MPI library will throw
  a lot of errors. But you may use Valgrind the following way such that every rank
  writes its own Valgrind log file:

```console
marie@login$ module load Valgrind
Module Valgrind/3.14.0-foss-2018b and 12 dependencies loaded.
marie@login$ srun --ntasks=4 valgrind --log-file=valgrind-%p.out ./myprog
```


# Distributed Training

## Internal Distribution

Training a machine learning model can be a very time-consuming task.
Distributed training allows scaling up deep learning tasks,
so we can train very large models and speed up training time.

There are two paradigms for distributed training:

1. data parallelism:
each device has a replica of the model and computes over different parts of the data.
2. model parallelism:
models are distributed over multiple devices.

In the following, we will stick to the concept of data parallelism because it is a widely-used
technique.
There are basically two strategies to train the scattered data throughout the devices:

1. synchronous training: devices (workers) are trained over different slices of the data and at the
end of each step gradients are aggregated.
2. asynchronous training:
all devices are independently trained over the data and update variables asynchronously.

### Distributed TensorFlow

[TensorFlow](https://www.tensorflow.org/guide/distributed_training) provides a high-end API to
train your model and distribute the training on multiple GPUs or machines with minimal code changes.

The primary distributed training method in TensorFlow is `tf.distribute.Strategy`.
There are multiple strategies that distribute the training depending on the specific use case,
the data and the model.

TensorFlow refers to the synchronous training as mirrored strategy.
There are two mirrored strategies available whose principles are the same:

- `tf.distribute.MirroredStrategy` supports the training on multiple GPUs on one machine.
- `tf.distribute.MultiWorkerMirroredStrategy` for multiple machines, each with multiple GPUs.

The Central Storage Strategy applies to environments where the GPUs might not be able to store
the entire model:

- `tf.distribute.experimental.CentralStorageStrategy` supports the case of a single machine
with multiple GPUs.

The CPU holds the global state of the model and GPUs perform the training.

In some cases asynchronous training might be the better choice, for example, if workers differ on
capability, are down for maintenance, or have different priorities.
The Parameter Server Strategy is capable of applying asynchronous training:

- `tf.distribute.experimental.ParameterServerStrategy` requires several Parameter Servers and workers.

The Parameter Server holds the parameters and is responsible for updating
the global state of the models.
Each worker runs the training loop independently.

??? example "Multi Worker Mirrored Strategy"

    In this case, we will go through an example with Multi Worker Mirrored Strategy.
    Multi-node training requires a `TF_CONFIG` environment variable to be set which will
    be different on each node.

    ```console
    marie@compute$ TF_CONFIG='{"cluster": {"worker": ["10.1.10.58:12345", "10.1.10.250:12345"]}, "task": {"index": 0, "type": "worker"}}' python main.py
    ```

    The `cluster` field describes how the cluster is set up (same on each node).
    Here, the cluster has two nodes referred to as workers.
    The `IP:port` information is listed in the `worker` array.
    The `task` field varies from node to node.
    It specifies the type and index of the node.
    In this case, the training job runs on worker 0, which is `10.1.10.58:12345`.
    We need to adapt this snippet for each node.
    The second node will have `'task': {'index': 1, 'type': 'worker'}`.

    With two modifications, we can parallelize the serial code:
    We need to initialize the distributed strategy:

    ```python
    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
    ```

    And define the model under the strategy scope:

    ```python
    with strategy.scope():
        model = resnet.resnet56(img_input=img_input, classes=NUM_CLASSES)
        model.compile(
            optimizer=opt,
            loss='sparse_categorical_crossentropy',
            metrics=['sparse_categorical_accuracy'])
    model.fit(train_dataset,
        epochs=NUM_EPOCHS)
    ```

    To run distributed training, the training script needs to be copied to all nodes,
    in this case on two nodes.
    TensorFlow is available as a module.
    Check for the version.
    The `TF_CONFIG` environment variable can be set as a prefix to the command.
    Now, run the script on the cluster `alpha` simultaneously on both nodes:

    ```bash
    #!/bin/bash

    #SBATCH --job-name=distr
    #SBATCH --output=%j.out
    #SBATCH --error=%j.err
    #SBATCH --mem=64000
    #SBATCH --nodes=2
    #SBATCH --ntasks=2
    #SBATCH --ntasks-per-node=1
    #SBATCH --cpus-per-task=14
    #SBATCH --gres=gpu:1
    #SBATCH --time=01:00:00

    function print_nodelist {
        scontrol show hostname $SLURM_NODELIST
    }
    NODE_1=$(print_nodelist | awk '{print $1}' | sort -u | head -n 1)
    NODE_2=$(print_nodelist | awk '{print $1}' | sort -u | tail -n 1)
    IP_1=$(dig +short ${NODE_1}.alpha.hpc.tu-dresden.de)
    IP_2=$(dig +short ${NODE_2}.alpha.hpc.tu-dresden.de)

    module load release/23.04 GCC/10.2.0 CUDA/11.1.1 OpenMPI/4.0.5 TensorFlow/2.4.1

    # On the first node
    TF_CONFIG='{"cluster": {"worker": ["'"${NODE_1}"':33562", "'"${NODE_2}"':33561"]}, "task": {"index": 0, "type": "worker"}}' srun --nodelist=${NODE_1} --nodes=1 --ntasks=1 --gres=gpu:1 python main_ddl.py &

    # On the second node
    TF_CONFIG='{"cluster": {"worker": ["'"${NODE_1}"':33562", "'"${NODE_2}"':33561"]}, "task": {"index": 1, "type": "worker"}}' srun --nodelist=${NODE_2} --nodes=1 --ntasks=1 --gres=gpu:1 python main_ddl.py &

    wait
    ```

### Distributed PyTorch

!!! note

    This section is under construction

PyTorch provides multiple ways to achieve data parallelism to train the deep learning models
efficiently. These models are part of the `torch.distributed` sub-package that ships with the main
deep learning package.

The easiest method to quickly prototype if the model is trainable in a multi-GPU setting is to wrap
the existing model with the `torch.nn.DataParallel` class as shown below,

```python
model = torch.nn.DataParalell(model)
```

Adding this single line of code to the existing application will let PyTorch know that the model
needs to be parallelized. But since this method uses threading to achieve parallelism, it fails to
achieve true parallelism due to the well known issue of Global Interpreter Lock that exists in
Python. To work around this issue and gain performance benefits of parallelism, the use of
`torch.nn.DistributedDataParallel` is recommended. This involves little more code changes to set up,
but further increases the performance of model training. The starting step is to initialize the
process group by calling the `torch.distributed.init_process_group()` using the appropriate back end
such as NCCL, MPI or Gloo. The use of NCCL as back end is recommended as it is currently the fastest
back end when using GPUs.

#### Using Multiple GPUs with PyTorch

The example below shows how to solve that problem by using model parallelism, which in contrast to
data parallelism splits a single model onto different GPUs, rather than replicating the entire
model on each GPU.
The high-level idea of model parallelism is to place different sub-networks of a model onto
different devices.
As only part of a model operates on any individual device a set of devices can collectively
serve a larger model.

It is recommended to use
[DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html),
instead of this class, to do multi-GPU training, even if there is only a single node.
See: Use `nn.parallel.DistributedDataParallel` instead of multiprocessing or `nn.DataParallel`.
Check the [PyTorch CUDA page](https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead)
and [Distributed Data Parallel](https://pytorch.org/docs/stable/notes/ddp.html#ddp).

??? example "Parallel Model"

    The main aim of this model is to show the way how to effectively implement your neural network
    on multiple GPUs. It includes a comparison of different kinds of models and tips to improve the
    performance of your model.
    **Necessary** parameters for running this model are **2 GPU** and 14 cores.

    Download: [example_PyTorch_parallel.zip (4.2 KB)](misc/example_PyTorch_parallel.zip)

    Remember that for using [JupyterHub service](../access/jupyterhub.md) for PyTorch, you need to
    create and activate a virtual environment (kernel) with loaded essential modules.

    Run the example in the same way as the previous examples.

#### Distributed Data-Parallel

[DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)
(DDP) implements data parallelism at the module level which can run across multiple machines.
Applications using DDP should spawn multiple processes and create a single DDP instance per process.
DDP uses collective communications in the
[torch.distributed](https://pytorch.org/tutorials/intermediate/dist_tuto.html) package to
synchronize gradients and buffers.

Please also look at the [official tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html).

To use distributed data parallelism on ZIH systems, please make sure the value of
parameter `--ntasks-per-node=<N>` equals the number of GPUs you use per node.
Also, it can be useful to increase `memory/cpu` parameters if you run larger models.
Memory can be set up to:

- `--mem=250G` and `--cpus-per-task=7` for the `Power9` cluster.
- `--mem=900G` and `--cpus-per-task=6` for the `Alpha` cluster.

Keep in mind that only one memory parameter (`--mem-per-cpu=<MB>` or `--mem=<MB>`) can be specified.

## External Distribution

### Horovod

[Horovod](https://github.com/horovod/horovod) is the open-source distributed training framework
for TensorFlow, Keras and PyTorch.
It makes it easier to develop distributed deep learning projects and speeds them up.
Horovod scales well to a large number of nodes and has a strong focus on efficient training on
GPUs.

#### Why use Horovod?

Horovod allows you to easily take a single-GPU TensorFlow and PyTorch program and
train it on many GPUs!
In some cases, the MPI model is much more straightforward and requires far less code changes than
the distributed code from TensorFlow for instance, with parameter servers.
Horovod uses MPI and NCCL which gives in some cases better results than
pure TensorFlow and PyTorch.

#### Horovod as Module

Horovod is available as a module with **TensorFlow** or **PyTorch** for
**all** module environments.
Please check the [software module list](modules.md) for the current version of the software.
Horovod can be loaded like other software on ZIH system:

```console
marie@compute$ module spider Horovod           # Check available modules
------------------------------------------------------------------------------------------------
  Horovod:
------------------------------------------------------------------------------------------------
    Description:
      Horovod is a distributed training framework for TensorFlow.

     Versions:
        Horovod/0.18.2-fosscuda-2019b-TensorFlow-2.0.0-Python-3.7.4
        Horovod/0.19.5-fosscuda-2019b-TensorFlow-2.2.0-Python-3.7.4
        Horovod/0.21.1-TensorFlow-2.4.1
[...]
marie@compute$ module load Horovod/0.19.5-fosscuda-2019b-TensorFlow-2.2.0-Python-3.7.4
```

Or if you want to use Horovod on the cluster `alpha`, you can load it with the dependencies:

```console
marie@alpha$ module spider Horovod                         #Check available modules
marie@alpha$ module load release/23.04  release/23.04  GCC/11.3.0  OpenMPI/4.1.4  Horovod/0.28.1-CUDA-11.7.0-TensorFlow-2.11.0
```

#### Horovod Installation

However, if it is necessary to use another version of Horovod, it is possible to install it
manually. For that, you need to create a [virtual environment](python_virtual_environments.md) and
load the dependencies (e.g. MPI).
Installing TensorFlow can take a few hours and is not recommended.

##### Install Horovod for TensorFlow with Python and Pip

This example shows the installation of Horovod for TensorFlow.
Adapt as required and refer to the [Horovod documentation](https://horovod.readthedocs.io/en/stable/install_include.html)
for details.

```console
marie@alpha$ HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_WITH_TENSORFLOW=1 pip install --no-cache-dir horovod\[tensorflow\]
[...]
marie@alpha$ horovodrun --check-build
Horovod v0.19.5:

Available Frameworks:
    [X] TensorFlow
    [ ] PyTorch
    [ ] MXNet

Available Controllers:
    [X] MPI
    [ ] Gloo

Available Tensor Operations:
    [X] NCCL
    [ ] DDL
    [ ] CCL
    [X] MPI
    [ ] Gloo
```

If you want to use Open MPI then specify `HOROVOD_GPU_ALLREDUCE=MPI`.
To have better performance it is recommended to use NCCL instead of Open MPI.

##### Verify Horovod Works

```pycon
>>> import tensorflow
2021-10-07 16:38:55.694445: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
>>> import horovod.tensorflow as hvd                      #import horovod
>>> hvd.init()                                       #initialize horovod
>>> hvd.size()
1
>>> hvd.rank()
0
>>> print('Hello from:', hvd.rank())
Hello from: 0
```

??? example

    Follow the steps in the
    [official examples](https://github.com/horovod/horovod/tree/master/examples)
    to parallelize your code.
    In Horovod, each GPU gets pinned to a process.
    You can easily start your job with the following bash script with four processes on two nodes using the cluster Power:

    ```bash
    #!/bin/bash
    #SBATCH --nodes=2
    #SBATCH --ntasks=4
    #SBATCH --ntasks-per-node=2
    #SBATCH --gres=gpu:2
    #SBATCH --mem=250G
    #SBATCH --time=01:00:00
    #SBATCH --output=run_horovod.out

    module load release/23.04
    module load Horovod/0.19.5-fosscuda-2019b-TensorFlow-2.2.0-Python-3.7.4

    srun python your_program.py
    ```

    Do not forget to specify the total number of tasks `--ntasks` and the number of tasks per node
    `--ntasks-per-node` which must match the number of GPUs per node.


# Measure Energy Consumption

The Intel Haswell nodes of ZIH system are equipped with power instrumentation that allow the
recording and accounting of power dissipation and energy consumption data. The data is made
available through several different interfaces, which are described below.

## Summary of Measurement Interfaces

| Interface                                  | Sensors         | Rate                            |
|:-------------------------------------------|:----------------|:--------------------------------|
| Dataheap (C, Python, VampirTrace, Score-P) | Blade, (CPU)    | 1 sample/s                          |
| HDEEM\* (C, Score-P)                       | Blade, CPU, DDR | 1000 samples/s (Blade), 100 samples/s (VRs) |
| HDEEM Command Line Interface               | Blade, CPU, DDR | 1000 samples/s (Blade), 100 samples/s (VR)  |
| Slurm Accounting (`sacct`)                 | Blade           | Per Job Energy                  |
| Slurm Profiling (HDF5)                     | Blade           | Up to 1 sample/s                    |

!!! note

    Please specify `--partition=haswell --exclusive` along with your job request if you wish to use
    HDEEM.

### Accuracy, Temporal and Spatial Resolution

In addition to the above mentioned interfaces, you can access the measurements through a
[C API](#using-the-hdeem-c-api) to get the full temporal and spatial resolution:

- ** Blade:** 1000 samples/s for the whole node, includes both sockets, DRAM,
  SSD, and other on-board consumers. Since the system is directly
  water cooled, no cooling components are included in the blade
  consumption.
- **Voltage regulators (VR):** 100 samples/s for each of the six VR
  measurement points, one for each socket and four for eight DRAM
  lanes (two lanes bundled).

The GPU blades also have 1 sample/s power instrumentation but have a lower accuracy.

HDEEM measurements have an accuracy of 2 % for Blade (node) measurements, and 5 % for voltage
regulator (CPU, DDR) measurements.

## Command Line Interface

The HDEEM infrastructure can be controlled through command line tools. They are commonly used on
the node under test to start, stop, and query the measurement device.

- `startHdeem`: Start a measurement. After the command succeeds, the
  measurement data with the 1000 / 100 samples/s described above will be
  recorded on the Board Management Controller (BMC), which is capable
  of storing up to 8h of measurement data.
- `stopHdeem`: Stop a measurement. No further data is recorded and
  the previously recorded data remains available on the BMC.
- `printHdeem`: Read the data from the BMC. By default, the data is
  written into a CSV file, whose name can be controlled using the
  `-o` argument.
- `checkHdeem`: Print the status of the measurement device.
- `clearHdeem`: Reset and clear the measurement device. No further
  data can be read from the device after this command is executed
  before a new measurement is started.

!!! note

    Please always execute `clearHdeem` before `startHdeem`.

## Integration in Application Performance Traces

The per-node power consumption data can be included as metrics in application traces by using the
provided metric plugins for Score-P (and VampirTrace). The plugins are provided as modules and set
all necessary environment variables that are required to record data for all nodes that are part of
the current job.

For 1 sample/s Blade values (Dataheap):

- [Score-P](scorep.md): use the module `scorep-dataheap`
- [VampirTrace](../archive/vampirtrace.md): use the module `vampirtrace-plugins/power-1.1`
  (**Remark:** VampirTrace is outdated!)

For 1000 samples/s (Blade) and 100 samples/s (CPU{0,1}, DDR{AB,CD,EF,GH}):

- [Score-P](scorep.md): use the module `scorep-hdeem`. This
  module requires a recent version of `scorep/sync-...`. Please use
  the latest that fits your compiler and MPI version.

By default, the modules are set up to record the power data for the nodes they are used on. For
further information on how to change this behavior, please use module show on the respective module.

!!! example "Example usage with `gcc`"

  ```console
  marie@haswell$ module load scorep/trunk-2016-03-17-gcc-xmpi-cuda7.5
  marie@haswell$ module load scorep-dataheap
  marie@haswell$ scorep gcc application.c -o application
  marie@haswell$ srun ./application
  ```

Once the application is finished, a trace will be available that allows you to correlate application
functions with the component power consumption of the parallel application.

!!! note

    For energy measurements, only tracing is supported in Score-P/VampirTrace.
    The modules therefore disables profiling and enables tracing,
    please use [Vampir](vampir.md) to view the trace.

![Energy measurements in Vampir](misc/energy_measurements-vampir.png)
{: align="center"}

By default, `scorep-dataheap` records all sensors that are available. Currently this is the total
node consumption and the CPUs. `scorep-hdeem` also records all available sensors
(node, 2x CPU, 4x DDR) by default. You can change the selected sensors by setting the environment
variables:

!!! note

    The power measurement modules `scorep-dataheap` and `scorep-hdeem` are
    dynamic and only need to be loaded during execution.
    However, `scorep-hdeem` does require the application to be linked with
    a certain version of Score-P.

??? hint "For HDEEM"
    `export SCOREP_METRIC_HDEEM_PLUGIN=Blade,CPU*`

??? hint "For Dataheap"
    `export SCOREP_METRIC_DATAHEAP_PLUGIN=localhost/watts`

For more information on how to use Score-P, please refer to the [respective documentation](scorep.md).

## Access Using Slurm Tools

[Slurm](../jobs_and_resources/slurm.md) maintains its own database of job information, including
energy data. There are two main ways of accessing this data, which are described below.

### Post-Mortem Per-Job Accounting

This is the easiest way of accessing information about the energy consumed by a job and its job
steps. The Slurm tool `sacct` allows users to query post-mortem energy data for any past job or job
step by adding the field `ConsumedEnergy` to the `--format` parameter:

```console
marie@login $ sacct --format="jobid,jobname,ntasks,submit,start,end,ConsumedEnergy,nodelist,state" -j 3967027
       JobID    JobName   NTasks              Submit               Start                 End ConsumedEnergy        NodeList      State
------------ ---------- -------- ------------------- ------------------- ------------------- -------------- --------------- ----------
3967027            bash          2014-01-07T12:25:42 2014-01-07T12:25:52 2014-01-07T12:41:20                    taurusi1159  COMPLETED
3967027.0         sleep        1 2014-01-07T12:26:07 2014-01-07T12:26:07 2014-01-07T12:26:18              0     taurusi1159  COMPLETED
3967027.1         sleep        1 2014-01-07T12:29:06 2014-01-07T12:29:06 2014-01-07T12:29:16          1.67K     taurusi1159  COMPLETED
3967027.2         sleep        1 2014-01-07T12:33:25 2014-01-07T12:33:25 2014-01-07T12:33:36          1.84K     taurusi1159  COMPLETED
3967027.3         sleep        1 2014-01-07T12:34:06 2014-01-07T12:34:06 2014-01-07T12:34:11          1.09K     taurusi1159  COMPLETED
3967027.4         sleep        1 2014-01-07T12:38:03 2014-01-07T12:38:03 2014-01-07T12:39:44         18.93K     taurusi1159  COMPLETED
```

This example job consisted of 5 job steps, each executing a sleep of a different length. Note that the
`ConsumedEnergy` metric is only applicable to exclusive jobs.

### Slurm Energy Profiling

The `srun` tool offers several options for profiling job steps by adding the `--profile` parameter.
Possible profiling options are `All`, `Energy`, `Task`, `Lustre`, and `Network`. In all cases, the
profiling information is stored in an HDF5 file that can be inspected using available HDF5 tools,
e.g., `h5dump`. The files are stored under `/scratch/profiling/` for each job, job step, and node. A
description of the data fields in the file can be found
[in the official documentation](http://slurm.schedmd.com/hdf5_profile_user_guide.html#HDF5).
In general, the data files
contain samples of the current **power** consumption on a per-second basis:

```console
marie@login $ srun --partition haswell64 --acctg-freq=2,energy=1 --profile=energy sleep 10
srun: job 3967674 queued and waiting for resources
srun: job 3967674 has been allocated resources
marie@login $ h5dump /scratch/profiling/marie/3967674_0_taurusi1073.h5
[...]
  DATASET "Energy_0000000002 Data" {
    DATATYPE  H5T_COMPOUND {
      H5T_STRING {
        STRSIZE 24;
        STRPAD H5T_STR_NULLTERM;
        CSET H5T_CSET_ASCII;
        CTYPE H5T_C_S1;
      } "Date_Time";
      H5T_STD_U64LE "Time";
      H5T_STD_U64LE "Power";
      H5T_STD_U64LE "CPU_Frequency";
    }
    DATASPACE  SIMPLE { ( 1 ) / ( 1 ) }
    DATA {
    (0): {
        "",
        1389097545,  # timestamp
        174,         # power value
        1
      }
    }
  }
```

## Using the HDEEM C API

Please specify `--partition=haswell --exclusive` along with your job request if you wish to use HDEEM.

Please download the official documentation at
[http://www.bull.com/download-hdeem-library-reference-guide](http://www.bull.com/download-hdeem-library-reference-guide).

The HDEEM header and sample code are locally installed on the nodes.

??? hint "HDEEM header location"

    `/usr/include/hdeem.h`

??? hint "HDEEM sample location"

    `/usr/share/hdeem/sample/`

## Further Information and Citing

More information can be found in the paper
[HDEEM: high definition energy efficiency monitoring](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7016382)
by Daniel Hackenberg et al. Please cite this paper if you are using HDEEM for your scientific work.


# FEM Software

!!! hint "Its all in the modules"

    All packages described in this section, are organized in so-called modules. To list the available versions of a package and load a
    particular, e.g., ANSYS, version, invoke the commands

    ```console
    marie@login$ module avail ANSYS
    [...]
    marie@login$ # module load ANSYS/<version>
    marie@login$ # e.g.
    marie@login$ module load ANSYS/2022R2
    ```

    The section [runtime environment](modules.md) provides a comprehensive overview
    on the module system and relevant commands.

## Abaqus

[Abaqus](https://www.3ds.com/de/produkte-und-services/simulia/produkte/abaqus/) is a general-purpose
finite element method program designed for advanced linear and nonlinear engineering analysis
applications with facilities for linking-in user developed material models, elements, friction laws,
etc.

### Guide by User

Eike Dohmen (from Inst. f. Leichtbau und Kunststofftechnik) sent us the description of his
Abaqus calculations. Please try to adapt your calculations in that way. Eike is normally a
Windows user and his description contains also some hints for basic Unix commands:
[Abaqus-Slurm.pdf (only in German)](misc/abaqus-slurm.pdf).

### General

Abaqus calculations should be started using a job file (aka. batch script). Please refer to the
page covering the [batch system Slurm](../jobs_and_resources/slurm.md) if you are not familiar with
Slurm or [writing job files](../jobs_and_resources/slurm.md#job-files).

??? example "Usage of Abaqus"

    (Thanks to Benjamin Groeger, Inst. f. Leichtbau und Kunststofftechnik)).

    1. Prepare an Abaqus input-file. You can start with the input example from Benjamin:
    [Rot-modell-BenjaminGroeger.inp](misc/Rot-modell-BenjaminGroeger.inp)
    2. Prepare a job file on ZIH systems like this
    ```bash
    #!/bin/bash
    ### needs ca 20 sec with 4cpu
    ### generates files:
    ###  yyyy.com
    ###  yyyy.dat
    ###  yyyy.msg
    ###  yyyy.odb
    ###  yyyy.prt
    ###  yyyy.sim
    ###  yyyy.sta
    #SBATCH --nodes=1               # with >1 node Abaqus needs a nodeliste
    #SBATCH --ntasks-per-node=4
    #SBATCH --mem=2048               # total memory
    #SBATCH --time=00:04:00
    #SBATCH --job-name=yyyy         # give a name, what ever you want
    #SBATCH --mail-type=END,FAIL    # send email when the job finished or failed
    #SBATCH --mail-user=marie@tu-dresden.de  # set your email
    #SBATCH --account=p_number_crunch       # charge compute time to project p_number_crunch


    # Abaqus has its own MPI
    unset SLURM_GTIDS

    # load module and start Abaqus
    module load ABAQUS/2022
    abaqus interactive input=Rot-modell-BenjaminGroeger.inp job=yyyy cpus=4 mp_mode=mpi
    ```
    3. Start the job file (e.g., name `batch-Rot-modell-BenjaminGroeger.sh`)
    ```
    marie@login$ sbatch batch-Rot-modell-BenjaminGroeger.sh      # Slurm will provide the Job Id (e.g., 3130522)
    ```
    4. Control the status of the job
    ```
    marie@login$ squeue --me     # in column "ST" (Status) you will find a R=Running or P=Pending (waiting for resources)
    ```

## Ansys

Ansys is a general-purpose finite element method program for engineering analysis, and includes
preprocessing, solution, and post-processing functions. It is used in a wide range of disciplines
for solutions to mechanical, thermal, and electronic problems.
[Ansys and Ansys CFX](http://www.ansys.com) used to be separate packages in the past and are now
combined.

In general, HPC systems are not designed for interactive working with GUIs. Even so, it is possible to
start a Ansys workbench on the login nodes interactively for short tasks. The second and
**recommended way** is to use job files. Both modes are documented in the following.

!!! note ""

    Since the MPI library that Ansys uses internally (Platform MPI) has some problems integrating
    seamlessly with Slurm, you have to unset the enviroment variable `SLURM_GTIDS` in your
    environment befor running Ansysy workbench in interactive and batch mode.

### Using Workbench Interactively

Ansys workbench (`runwb2`) can be invoked interactively on the login nodes of ZIH systems for short
tasks.

!!! note

    [X11 forwarding](../access/ssh_login.md#x11-forwarding) needs to enabled when establishing the
    SSH connection. For OpenSSH the corresponding option is `-X` and it is valuable to use
    compression of all data via `-C`.

=== "startup script"

    ```console
    [marie@login ~]$ start_ansysworkbench.sh
    ```

=== "customized start"

    ```console
    # SSH connection established using -CX
    [marie@login$ ~]$ # module load ANSYS/<version>
    [marie@login$ ~]$ # e.g.
    [marie@login$ ~]$ module load ANSYS/2022R2
    [marie@login$ ~]$ runwb2
    ```

If more time is needed, a CPU has to be allocated like this (see
[batch systems Slurm](../jobs_and_resources/slurm.md) for further information):

```console
[marie@login$ ~]$ # module load ANSYS/<version>
[marie@login$ ~]$ # e.g.
[marie@login$ ~]$ module load ANSYS/2023R1
[marie@login$ ~]$ srun --time=00:30:00 --x11=first [SLURM_OPTIONS] --pty bash
[...]
[marie@compute$ ~]$ runwb2
```

!!! hint "Better use DCV"

    The software NICE Desktop Cloud Visualization (DCV) enables to
    remotly access OpenGL-3D-applications running on ZIH systems using its GPUs
    (cf. [virtual desktops](virtual_desktops.md)).

Ansys can be used under DCV to make use of GPU acceleration. Follow the instructions within
[virtual desktops](virtual_desktops.md) to set up a DCV session. Then, load a Ansys module, unset
the environment variable `SLURM_GTIDS`, and finally start the workbench:

```console
marie@gpu$ module load ANSYS
marie@gpu$ unset SLURM_GTIDS
marie@gpu$ runwb2
```

### Using Workbench in Batch Mode

The Ansys workbench (`runwb2`) can also be used in a job file to start calculations (the solver,
not GUI) from a workbench project into the background. To do so, you have to specify the `-B`
parameter (for batch mode), `-F` for your project file, and can then either add different commands via
`-E parameters directly`, or specify a workbench script file containing commands via `-R`.

??? example "Ansys Job File"

    ```bash
    #!/bin/bash
    #SBATCH --time=0:30:00
    #SBATCH --nodes=1
    #SBATCH --ntasks=2
    #SBATCH --mem-per-cpu=1000M

    unset SLURM_GTIDS              # Odd, but necessary!

    # module load ANSYS/<version>
    # e.g.
    module load ANSYS/2020R2

    runwb2 -B -F Workbench_Taurus.wbpj -E 'Project.Update' -E 'Save(Overwrite=True)'
    #or, if you wish to use a workbench replay file, replace the -E parameters with: -R mysteps.wbjn
    ```

### Running Workbench in Parallel

Unfortunately, the number of CPU cores you wish to use cannot simply be given as a command line
parameter to your `runwb2` call. Instead, you have to enter it into an XML file in your home
directory. This setting will then be **used for all** your `runwb2` jobs. While it is also possible
to edit this setting via the Mechanical GUI, experience shows that this can be problematic via
X11-forwarding and we only managed to use the GUI properly via [DCV](virtual_desktops.md), so we
recommend you simply edit the XML file directly with a text editor of your choice. It is located
under:

`$HOME/.mw/Application Data/Ansys/v181/SolveHandlers.xml`

(mind the space in there.) You might have to adjust the Ansys version
(here `v181`) in the path to your preferred version. In this file, you can find the parameter

`<MaxNumberProcessors>2</MaxNumberProcessors>`

that you can simply change to something like 16 or 24. For now, you should stay within single-node
boundaries, because multi-node calculations require additional parameters. The number you choose
should match your used `--cpus-per-task` parameter in your job file.

### Running MAPDL

*Ansys Parametric Design Language* (APDL) is a powerful structured scripting language used to
interact with the Ansys Mechanical solver. Mechanical APDL (MAPDL), a finite element analysis
program, is driven by APDL. APDL and MAPDL can be used for many tasks, ranging from creating
geometries for analysis to setting up sophisticated solver settings for highly complex analyses

#### Shared-Memory Mode

MAPDL can be invoked in so-called shared-memory mode to make use of threads in order to speedup
computation. The multi-threading approach is restricted to one node. In contrast, MAPDL offers a
MPI-parallel mode to distribute the computation across  multiple nodes. This mode is described
below.

##### Interactive mode

```console
marie@login$ srun --nodes 1 --ntasks-per-node=4 --time=0:20:00 --mem-per-cpu=1700 --pty bash -l
```

```console
marie@node$ module load ANSYS/2021R2
marie@node$ mapdl -smp -np ${SLURM_NTASKS}
```

##### Batch mode

```bash
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=4
#SBATCH --mem=2000
#SBATCH --job-name=ansys_mapdl
#SBATCH --output=output_ansys_mapdl
#SBATCH --time=01:00:00

module load ANSYS/2021R2

# -smp use shared memory parallel version
# -b (batch mode)
# -np specify number of cpu's to use
# -j jobname

mapdl -smp -b -np ${SLURM_NTASKS} -j solution -i <input-file>
```

```console
marie@login$ sbatch mapdl_job.sh
```

#### Distributed-Memory Mode

MAPDL can be run in distributed-memory mode using multiple compute nodes in either interactive as
well as batch mode as shown in the following.

In both cases, it is necessary to create a nodelist and provide it to MAPDL via `-machines` command
line option.

##### Interactive Mode

```console
marie@login$ srun --nodes 4 --ntasks-per-node=4 --time=0:20:00 --mem-per-cpu=1700 --pty bash -l

# generate node list
marie@node$ NODELIST=$(for node in $( scontrol show hostnames ${SLURM_JOB_NODELIST} | uniq ); do echo -n "${node}:${SLURM_NTASKS_PER_NODE}:"; done | sed 's/:$//')

marie@node$ KMP_AFFINITY=none mapdl -machines ${NODELIST}
```

##### Batch Mode

```bash
#!/bin/bash
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=4
#SBATCH --mem=2000
#SBATCH --job-name=ansys_mapdl
#SBATCH --output=output_ansys_mapdl
#SBATCH --time=01:00:00

module purge
module load ANSYS/2021R2

# generate node list
NODELIST=$(for node in $( scontrol show hostnames ${SLURM_JOB_NODELIST} | uniq ); do echo -n "${node}:${SLURM_NTASKS_PER_NODE}:"; done | sed 's/:$//')

# -b (batch mode)
# -machines xxx   specify machines list for distributed Ansys
# -j jobname
setenv KMP_AFFINITY none

mapdl -b -machines ${NODELIST} -j solution -i <input-file>
```

```console
marie@login$ sbatch mapdl_job.sh
```

## COMSOL Multiphysics

[COMSOL Multiphysics](https://www.comsol.com) (formerly FEMLAB) is a finite element analysis, solver
and Simulation software package for various physics and engineering applications, especially coupled
phenomena, or multiphysics.

COMSOL may be used remotely on ZIH systems or locally on the desktop, using ZIH license server.

For using COMSOL on ZIH systems, we recommend the interactive client-server mode (see COMSOL
manual).

### Client-Server Mode

In this mode, COMSOL runs as server process on the ZIH system and as client process on your local
workstation. The client process needs a dummy license for installation, but no license for normal
work. Using this mode is almost undistinguishable from working with a local installation. It also works
well with Windows clients. For this operation mode to work, you must build an SSH tunnel through the
firewall of ZIH. For further information, please refer to the COMSOL manual.

### Usage

??? example "Server Process"

    Start the server process with 2 cores, 4 GB RAM and max. 2 hours running time using an
    interactive Slurm job like this:

    ```console
    marie@login$ module load COMSOL
    marie@login$ srun --ntasks=1 --cpus-per-task=2 --mem-per-cpu=4096 --time=02:00:00 --pty --x11=first server -np 2
    ```

??? example "Background Job"

    Interactive working is great for debugging and setting experiments up. But, if you have a huge
    workload, you should definitively rely on job files. I.e., you put the necessary steps to get
    the work done into scripts and submit these scripts to the batch system. These two steps are
    outlined:

    1. Create a [job file](../jobs_and_resources/slurm.md#job-files), e.g.
    ```bash
    #!/bin/bash
    #SBATCH --time=24:00:00
    #SBATCH --nodes=2
    #SBATCH --ntasks-per-node=2
    #SBATCH --cpus-per-task=12
    #SBATCH --mem-per-cpu=2500

    module load COMSOL
    srun comsol -mpi=intel batch -inputfile ./MyInputFile.mph
    ```

### Interactive Usage with X11 Forwarding

=== "starter script"

    ```console
    marie@login$ start_comsol.sh
    ```

=== "customized startup"

    If you'd like to work interactively using COMSOL, you can request for an interactive job with,
    e.g., 4 cores and 2500 MB RAM for 8 hours and X11 forwarding to open the COMSOL GUI:

    ```console
    marie@login$ module load COMSOL
    marie@login$ srun --ntasks=1 --cpus-per-task=2 --mem-per-cpu=2500 --time=01:00:00 --pty --x11=first comsol -np 2
    ```

!!! note

    Please make sure, that the option *Preferences* --> Graphics --> *Renedering* is set to *software
    rendering*. Than, you can work from within the campus network.

## LS-DYNA

[LS-DYNA](https://www.dynamore.de/de) is a general-purpose, implicit and explicit FEM software for
nonlinear structural analysis. Both, the shared memory version and the distributed memory version
(`mpp`) are installed on ZIH systems.

You need a job file (aka. batch script) to run the MPI version.

??? example "Minimal Job File"

    ```bash
    #!/bin/bash
    #SBATCH --time=01:00:00       # walltime
    #SBATCH --ntasks=16           # number of processor cores (i.e. tasks)
    #SBATCH --mem-per-cpu=1900M   # memory per CPU core

    module load LS-DYNA
    srun mpp-dyna i=neon_refined01_30ms.k memory=120000000
    ```

    Submit the job file named `job.sh` to the batch system via

    ```console
    marie@login$ sbatch job.sh
    ```

    Please refer to the section [Slurm](../jobs_and_resources/slurm.md) for further details and
    options on the batch system as well as monitoring commands.


# GPU Programming

## Available GPUs

The full hardware specifications of the GPU-compute nodes may be found in the
[HPC Resources](../jobs_and_resources/hardware_overview.md#hpc-resources) page.
Note that the clusters may have different [modules](modules.md#module-environments) available:

E.g. the available CUDA versions can be listed with

```bash
marie@compute$ module spider CUDA
```

Note that some modules use a specific CUDA version which is visible in the module name,
e.g. `GDRCopy/2.1-CUDA-11.1.1` or `Horovod/0.28.1-CUDA-11.7.0-TensorFlow-2.11.0`.

This especially applies to the optimized CUDA libraries like `cuDNN`, `NCCL` and `magma`.

!!! important "CUDA-aware MPI"

    When running CUDA applications using MPI for interprocess communication you need to additionally load the modules
    that enable CUDA-aware MPI which may provide improved performance.
    Those are `UCX-CUDA` and `UCC-CUDA` which supplement the `UCX` and `UCC` modules respectively.
    Some modules, like `NCCL`, load those automatically.

## Using GPUs with Slurm

For general information on how to use Slurm, read the respective [page in this compendium](../jobs_and_resources/slurm.md).
When allocating resources on a GPU-node, you must specify the number of requested GPUs by using the
`--gres=gpu:<N>` option, like this:

=== "cluster `Alpha` or `Capella`"
    ```bash
    #!/bin/bash                           # Batch script starts with shebang line

    #SBATCH --ntasks=1                    # All #SBATCH lines have to follow uninterrupted
    #SBATCH --time=01:00:00               # after the shebang line
    #SBATCH --account=p_number_crunch     # Comments start with # and do not count as interruptions
    #SBATCH --job-name=fancyExp
    #SBATCH --output=simulation-%j.out
    #SBATCH --error=simulation-%j.err
    #SBATCH --gres=gpu:1                  # request GPU(s) from Slurm

    module purge                          # Set up environment, e.g., clean modules environment
    module load module/version module2    # and load necessary modules

    srun ./application [options]          # Execute parallel application with srun
    ```

Alternatively, you can work on the clusters interactively:

```bash
marie@login.<cluster_name>$ srun --nodes=1 --gres=gpu:<N> --runtime=00:30:00 --pty bash
marie@compute$ module purge; module switch release/<env>
```

## Directive Based GPU Programming

Directives are special compiler commands in your C/C++ or Fortran source code. They tell the
compiler how to parallelize and offload work to a GPU. This section explains how to use this
technique.

### OpenACC

[OpenACC](https://www.openacc.org) is a directive based GPU programming model. It currently
only supports NVIDIA GPUs as a target.

Please use the following information as a start on OpenACC:

#### Introduction

OpenACC can be used with the PGI and NVIDIA HPC compilers. The NVIDIA HPC compiler, as part of the
[NVIDIA HPC SDK](https://docs.nvidia.com/hpc-sdk/index.html), supersedes the PGI compiler.

The `nvc` compiler (NOT the `nvcc` compiler, which is used for CUDA) is available for the NVIDIA
Tesla V100 and Nvidia A100 nodes.

#### Using OpenACC with PGI compilers

* Load the latest version via `module load PGI` or search for available versions with
`module search PGI`
* For compilation, please add the compiler flag `-acc` to enable OpenACC interpreting by the
  compiler
* `-Minfo` tells you what the compiler is actually doing to your code
* Add `-ta=nvidia:ampere` to enable optimizations for the A100 GPUs
* You may find further information on the PGI compiler in the
[user guide](https://docs.nvidia.com/hpc-sdk/pgi-compilers/20.4/x86/pgi-user-guide/index.htm)
and in the [reference guide](https://docs.nvidia.com/hpc-sdk/pgi-compilers/20.4/x86/pgi-ref-guide/index.htm),
which includes descriptions of available
[command line options](https://docs.nvidia.com/hpc-sdk/pgi-compilers/20.4/x86/pgi-ref-guide/index.htm#cmdln-options-ref)

#### Using OpenACC with NVIDIA HPC compilers

* Switch into the correct module environment for your selected compute nodes
(see [list of available GPUs](#available-gpus))
* Load the `NVHPC` module for the correct module environment.
Either load the default (`module load NVHPC`) or search for a specific version.
* Use the correct compiler for your code: `nvc` for C, `nvc++` for C++ and `nvfortran` for Fortran
* Use the `-acc` and `-Minfo` flag as with the PGI compiler
* To create optimized code for either the V100 or A100, use `-gpu=cc70` or `-gpu=cc80`, respectively
* Further information on this compiler is provided in the
[user guide](https://docs.nvidia.com/hpc-sdk/compilers/hpc-compilers-user-guide/index.html) and the
[reference guide](https://docs.nvidia.com/hpc-sdk/compilers/hpc-compilers-ref-guide/index.html),
which includes descriptions of available
[command line options](https://docs.nvidia.com/hpc-sdk/compilers/hpc-compilers-ref-guide/index.html#cmdln-options-ref)
* Information specific the use of OpenACC with the NVIDIA HPC compiler is compiled in a
[guide](https://docs.nvidia.com/hpc-sdk/compilers/openacc-gs/index.html)

### OpenMP target offloading

[OpenMP](https://www.openmp.org/) supports target offloading as of version 4.0. A dedicated set of
compiler directives can be used to annotate code-sections that are intended for execution on the
GPU (i.e., target offloading). Not all compilers with OpenMP support target offloading, refer to
the [official list](https://www.openmp.org/resources/openmp-compilers-tools/) for details.
Furthermore, some compilers, such as GCC, have basic support for target offloading, but do not
enable these features by default and/or achieve poor performance.

On the ZIH system, compilers with OpenMP target offloading support are provided on the clusters
`power9` and `alpha`. Two compilers with good performance can be used: the NVIDIA HPC compiler and the
IBM XL compiler.

#### Using OpenMP target offloading with NVIDIA HPC compilers

* Load the module environments and the NVIDIA HPC SDK as described in the
[OpenACC](#using-openacc-with-nvidia-hpc-compilers) section
* Use the `-mp=gpu` flag to enable OpenMP with offloading
* `-Minfo` tells you what the compiler is actually doing to your code
* The same compiler options as mentioned [above](#using-openacc-with-nvidia-hpc-compilers) are
available for OpenMP, including the `-gpu=ccXY` flag as mentioned above.
* OpenMP-specific advice may be found in the
[respective section in the user guide](https://docs.nvidia.com/hpc-sdk/compilers/hpc-compilers-user-guide/#openmp-use)

#### Using OpenMP target offloading with the IBM XL compilers

The IBM XL compilers (`xlc` for C, `xlc++` for C++ and `xlf` for Fortran (with sub-version for
different versions of Fortran)) are only available on the cluster `power9` with NVIDIA Tesla V100 GPUs.

* The `-qsmp -qoffload` combination of flags enables OpenMP target offloading support
* Optimizations specific to the V100 GPUs can be enabled by using the
[`-qtgtarch=sm_70`](https://www.ibm.com/docs/en/xl-c-and-cpp-linux/16.1.1?topic=descriptions-qtgtarch)
flag.
* IBM provides a [XL compiler documentation](https://www.ibm.com/docs/en/xl-c-and-cpp-linux/16.1.1)
with a
[list of supported OpenMP directives](https://www.ibm.com/docs/en/xl-c-and-cpp-linux/16.1.1?topic=reference-pragma-directives-openmp-parallelization)
and information on
[target-offloading specifics](https://www.ibm.com/docs/en/xl-c-and-cpp-linux/16.1.1?topic=gpus-programming-openmp-device-constructs)

## Native GPU Programming

### CUDA

Native [CUDA](http://www.nvidia.com/cuda) programs can sometimes offer a better performance.
NVIDIA provides some [introductory material and links](https://developer.nvidia.com/how-to-cuda-c-cpp).
An [introduction to CUDA](https://developer.nvidia.com/blog/even-easier-introduction-cuda/) is
provided as well. The [toolkit documentation page](https://docs.nvidia.com/cuda/index.html) links to
the [programming guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) and the
[best practice guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html).
Optimization guides for supported NVIDIA architectures are available, including for
[Volta (V100)](https://docs.nvidia.com/cuda/volta-tuning-guide/index.html) and
[Ampere (A100)](https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html).

In order to compile an application with CUDA use the `nvcc` compiler command, which is described in
detail in [nvcc documentation](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html).
This compiler is available via several `CUDA` packages, a default version can be loaded via
`module load CUDA`. Additionally, the `NVHPC` modules provide CUDA tools as well.

For using CUDA with Open MPI at multiple nodes, the `OpenMPI` module loaded shall have be compiled
with CUDA support. If you aren't sure if the module you are using has support for it you can check
it as following:

```console
ompi_info --parsable --all | grep mpi_built_with_cuda_support:value | awk -F":" '{print "Open MPI supports CUDA:",$7}'
```

#### Usage of the CUDA Compiler

The simple invocation `nvcc <code.cu>` will compile a valid CUDA program. `nvcc` differentiates
between the device and the host code, which will be compiled in separate phases. Therefore, compiler
options can be defined specifically for the device as well as for the host code. By default, the GCC
is used as the host compiler. The following flags may be useful:

* `--generate-code` (`-gencode`): generate optimized code for a target GPU (caution: these binaries
cannot be used with GPUs of other generations).
    * For Volta (V100): `--generate-code arch=compute_70,code=sm_70`,
    * For Ampere (A100): `--generate-code arch=compute_80,code=sm_80`
* `-Xcompiler`: pass flags to the host compiler. E.g., generate OpenMP-parallel host code:
`-Xcompiler -fopenmp`.
The `-Xcompiler` flag has to be invoked for each host-flag

## Performance Analysis

Consult NVIDIA's [Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html)
and the [performance guidelines](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#performance-guidelines)
for possible steps to take for the performance analysis and optimization.

Multiple tools can be used for the performance analysis.
For the analysis of applications on the newer GPUs (V100 and A100),
we recommend the use of the newer NVIDIA Nsight tools, [Nsight Systems](https://developer.nvidia.com/nsight-systems)
for a system-wide sampling and tracing and [Nsight Compute](https://developer.nvidia.com/nsight-compute)
for a detailed analysis of individual kernels.

### NVIDIA nvprof & Visual Profiler

The nvprof command line and the Visual Profiler are available once a CUDA module has been loaded.
For a simple analysis, you can call `nvprof` without any options, like such:

```bash
marie@compute$ nvprof ./application [options]
```

For a more in-depth analysis, we recommend you use the command line tool first to generate a report
file, which you can later analyze in the Visual Profiler. In order to collect a set of general
metrics for the analysis in the Visual Profiler, use the `--analysis-metrics` flag to collect
metrics and `--export-profile` to generate a report file, like this:

```bash
marie@compute$ nvprof --analysis-metrics --export-profile  <output>.nvvp ./application [options]
```

[Transfer the report file to your local system](../data_transfer/dataport_nodes.md) and analyze it in
the Visual Profiler (`nvvp`) locally. This will give the smoothest user experience. Alternatively,
you can use [X11-forwarding](../access/ssh_login.md). Refer to the documentation for details about
the individual
[features and views of the Visual Profiler](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual-views).

Besides these generic analysis methods, you can profile specific aspects of your GPU kernels.
`nvprof` can profile specific events. For this, use

```bash
marie@compute$ nvprof --query-events
```

to get a list of available events.
Analyze one or more events by using specifying one or more events, separated by comma:

```bash
marie@compute$ nvprof --events <event_1>[,<event_2>[,...]] ./application [options]
```

Additionally, you can analyze specific metrics.
Similar to the profiling of events, you can get a list of available metrics:

```bash
marie@compute$ nvprof --query-metrics
```

One or more metrics can be profiled at the same time:

```bash
marie@compute$ nvprof --metrics <metric_1>[,<metric_2>[,...]] ./application [options]
```

If you want to limit the profiler's scope to one or more kernels, you can use the
`--kernels <kernel_1>[,<kernel_2>]` flag. For further command line options, refer to the
[documentation on command line options](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-command-line-options).

### NVIDIA Nsight Systems

Use [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) for a system-wide sampling
of your code. Refer to the
[NVIDIA Nsight Systems User Guide](https://docs.nvidia.com/nsight-systems/UserGuide/index.html) for
details. With this, you can identify parts of your code that take a long time to run and are
suitable optimization candidates.

Use the command-line version to sample your code and create a report file for later analysis:

```bash
marie@compute$ nsys profile [--stats=true] ./application [options]
```

The `--stats=true` flag is optional and will create a summary on the command line. Depending on your
needs, this analysis may be sufficient to identify optimizations targets.

The graphical user interface version can be used for a thorough analysis of your previously
generated report file. For an optimal user experience, we recommend a local installation of NVIDIA
Nsight Systems. In this case, you can
[transfer the report file to your local system](../data_transfer/dataport_nodes.md).
Alternatively, you can use [X11-forwarding](../access/ssh_login.md). The graphical user interface is
usually available as `nsys-ui`.

Furthermore, you can use the command line interface for further analyses. Refer to the
documentation for a
[list of available command line options](https://docs.nvidia.com/nsight-systems/UserGuide/index.html#cli-options).

### NVIDIA Nsight Compute

Nsight Compute is used for the analysis of individual GPU-kernels. It supports GPUs from the Volta
architecture onward (on the ZIH system: V100 and A100). If you are familiar with nvprof,
you may want to consult the [Nvprof Transition Guide](https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#nvprof-guide),
as Nsight Compute uses a new scheme for metrics.
We recommend those kernels as optimization targets that require a large portion of you run time,
according to Nsight Systems. Nsight Compute is particularly useful for CUDA code, as you have much
greater control over your code compared to the directive based approaches.

Nsight Compute comes in a
[command line](https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html)
and a [graphical version](https://docs.nvidia.com/nsight-compute/NsightCompute/index.html).
Refer to the
[Kernel Profiling Guide](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html)
to get an overview of the functionality of these tools.

You can call the command line version (`ncu`) without further options to get a broad overview of
your kernel's performance:

```bash
marie@compute$ ncu ./application [options]
```

As with the other profiling tools, the Nsight Compute profiler can generate report files like this:

```bash
marie@compute$ ncu --export <report> ./application [options]
```

The report file will automatically get the file ending `.ncu-rep`, you do not need to specify this
manually.

This report file can be analyzed in the graphical user interface profiler. Again, we recommend you
generate a report file on a compute node and
[transfer the report file to your local system](../data_transfer/dataport_nodes.md).
Alternatively, you can use [X11-forwarding](../access/ssh_login.md). The graphical user interface is
usually available as `ncu-ui` or `nv-nsight-cu`.

Similar to the `nvprof` profiler, you can analyze specific metrics. NVIDIA provides a
[Metrics Guide](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-guide). Use
`--query-metrics` to get a list of available metrics, listing them by base name. Individual metrics
can be collected by using

```bash
marie@compute$ ncu --metrics <metric_1>[,<metric_2>,...] ./application [options]
```

Collection of events is no longer possible with Nsight Compute. Instead, many nvprof events can be
[measured with metrics](https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#nvprof-event-comparison).

You can collect metrics for individual kernels by specifying the `--kernel-name` flag.


# Hyperparameter Optimization (OmniOpt)

Classical simulation methods as well as machine learning methods (e.g. neural networks) have a large
number of hyperparameters that significantly determine the accuracy, efficiency, and transferability
of the method. In classical simulations, the hyperparameters are usually determined by adaptation to
measured values. Esp. in neural networks, the hyperparameters determine the network architecture:
number and type of layers, number of neurons, activation functions, measures against overfitting
etc. The most common methods to determine hyperparameters are intuitive testing, grid search or
random search.

The tool OmniOpt performs hyperparameter optimization within a broad range of applications as
classical simulations or machine learning algorithms. OmniOpt is robust and it checks and installs
all dependencies automatically and fixes many problems in the background. While OmniOpt optimizes,
no further intervention is required. You can follow the ongoing output live in the console.
Overhead of OmniOpt is minimal and virtually imperceptible.

## Quick start with OmniOpt

The following instructions demonstrate the basic usage of OmniOpt on the ZIH system, based on the
hyperparameter optimization for a neural network.

The typical OmniOpt workflow comprises at least the following steps:

1. [Prepare application script and software environment](#prepare-application-script-and-software-environment)
1. [Configure and run OmniOpt](#configure-and-run-omniopt)
1. [Check and evaluate OmniOpt results](#check-and-evaluate-omniopt-results)

### Prepare Application Script and Software Environment

The following example application script was created from
[https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)
as a starting point.
Therein, a neural network is trained on the MNIST Fashion data set.

There are the following script preparation steps for OmniOpt:

1. Changing hard-coded hyperparameters (chosen here: batch size, epochs, size of layer 1 and 2) into
   command line parameters.  Esp. for this example, the Python module `argparse` (see the docs at
   [https://docs.python.org/3/library/argparse.html](https://docs.python.org/3/library/argparse.html)
   is used.

    ??? note "Parsing arguments in Python"
        There are many ways for parsing arguments into Python scripts. The easiest approach is
        the `sys` module (see
        [www.geeksforgeeks.org/how-to-use-sys-argv-in-python](https://www.geeksforgeeks.org/how-to-use-sys-argv-in-python)),
        which would be fully sufficient for usage with OmniOpt. Nevertheless, this basic approach
        has no consistency checks or error handling etc.

1. Mark the output of the optimization target (chosen here: average loss) by prefixing it with the
   RESULT string. OmniOpt takes the **last appearing value** prefixed with the RESULT string. In
   the example, different epochs are performed and the average from the last epoch is caught by
   OmniOpt. Additionally, the `RESULT` output has to be a **single line**. After all these changes,
   the final script is as follows (with the lines containing relevant changes highlighted).

    ??? example "Final modified Python script: MNIST Fashion "

        ```python linenums="1" hl_lines="18-33 52-53 66-68 72 74 76 85 125-126"
        #!/usr/bin/env python
        # coding: utf-8

        # # Example for using OmniOpt
        #
        # source code taken from: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html
        # parameters under consideration:#
        # 1. batch size
        # 2. epochs
        # 3. size output layer 1
        # 4. size output layer 2

        import torch
        from torch import nn
        from torch.utils.data import DataLoader
        from torchvision import datasets
        from torchvision.transforms import ToTensor, Lambda, Compose
        import argparse

        # parsing hpyerparameters as arguments
        parser = argparse.ArgumentParser(description="Demo application for OmniOpt for hyperparameter optimization, example: neural network on MNIST fashion data.")

        parser.add_argument("--out-layer1", type=int, help="the number of outputs of layer 1", default = 512)
        parser.add_argument("--out-layer2", type=int, help="the number of outputs of layer 2", default = 512)
        parser.add_argument("--batchsize", type=int, help="batchsize for training", default = 64)
        parser.add_argument("--epochs", type=int, help="number of epochs", default = 5)

        args = parser.parse_args()

        batch_size = args.batchsize
        epochs = args.epochs
        num_nodes_out1 = args.out_layer1
        num_nodes_out2 = args.out_layer2

        # Download training data from open data sets.
        training_data = datasets.FashionMNIST(
            root="data",
            train=True,
            download=True,
            transform=ToTensor(),
        )

        # Download test data from open data sets.
        test_data = datasets.FashionMNIST(
            root="data",
            train=False,
            download=True,
            transform=ToTensor(),
        )

        # Create data loaders.
        train_dataloader = DataLoader(training_data, batch_size=batch_size)
        test_dataloader = DataLoader(test_data, batch_size=batch_size)

        for X, y in test_dataloader:
            print("Shape of X [N, C, H, W]: ", X.shape)
            print("Shape of y: ", y.shape, y.dtype)
            break

        # Get cpu or gpu device for training.
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print("Using {} device".format(device))

        # Define model
        class NeuralNetwork(nn.Module):
            def __init__(self, out1, out2):
                self.o1 = out1
                self.o2 = out2
                super(NeuralNetwork, self).__init__()
                self.flatten = nn.Flatten()
                self.linear_relu_stack = nn.Sequential(
                    nn.Linear(28*28, out1),
                    nn.ReLU(),
                    nn.Linear(out1, out2),
                    nn.ReLU(),
                    nn.Linear(out2, 10),
                    nn.ReLU()
                )

            def forward(self, x):
                x = self.flatten(x)
                logits = self.linear_relu_stack(x)
                return logits

        model = NeuralNetwork(out1=num_nodes_out1, out2=num_nodes_out2).to(device)
        print(model)

        loss_fn = nn.CrossEntropyLoss()
        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)

        def train(dataloader, model, loss_fn, optimizer):
            size = len(dataloader.dataset)
            for batch, (X, y) in enumerate(dataloader):
                X, y = X.to(device), y.to(device)

                # Compute prediction error
                pred = model(X)
                loss = loss_fn(pred, y)

                # Backpropagation
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                if batch % 200 == 0:
                    loss, current = loss.item(), batch * len(X)
                    print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

        def test(dataloader, model, loss_fn):
            size = len(dataloader.dataset)
            num_batches = len(dataloader)
            model.eval()
            test_loss, correct = 0, 0
            with torch.no_grad():
                for X, y in dataloader:
                    X, y = X.to(device), y.to(device)
                    pred = model(X)
                    test_loss += loss_fn(pred, y).item()
                    correct += (pred.argmax(1) == y).type(torch.float).sum().item()
            test_loss /= num_batches
            correct /= size
            print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")


            #print statement esp. for OmniOpt (single line!!)
            print(f"RESULT: {test_loss:>8f} \n")

        for t in range(epochs):
            print(f"Epoch {t+1}\n-------------------------------")
            train(train_dataloader, model, loss_fn, optimizer)
            test(test_dataloader, model, loss_fn)
        print("Done!")
        ```

1. Testing script functionality and determine software requirements for the chosen
   [cluster](../jobs_and_resources/hardware_overview.md). In the following, the
   cluster `Alpha` is used. Please note the parameters `--out-layer1`, `--batchsize`, `--epochs` when
   calling the Python script. Additionally, note the `RESULT` string with the output for OmniOpt.

    ??? hint "Hint for installing Python modules"

        Note that for this example the module `torchvision` is not available on the cluster `alpha`
        and it is installed by creating a [virtual environment](python_virtual_environments.md). It is
        recommended to install such a virtual environment into a
        [workspace](../data_lifecycle/workspaces.md).

        ```console
        marie@login$ module load release/23.04  GCC/11.3.0  OpenMPI/4.1.4 PyTorch/1.12.1
        marie@login$ mkdir </path/to/workspace/python-environments>    #create folder
        marie@login$ virtualenv --system-site-packages </path/to/workspace/python-environments/torchvision_env>
        marie@login$ source </path/to/workspace/python-environments/torchvision_env>/bin/activate #activate virtual environment
        marie@login$ pip install torchvision #install torchvision module
        ```

    ```console
    # Job submission on alpha nodes with 1 GPU on 1 node with 800 MB per CPU
    marie@login$ srun --gres=gpu:1 -n 1 -c 7 --pty --mem-per-cpu=800 bash
    marie@alpha$ module load release/23.04 GCC/11.3.0  OpenMPI/4.1.4 PyTorch/1.12.1
    # Activate virtual environment
    marie@alpha$ source </path/to/workspace/python-environments/torchvision_env>/bin/activate

    marie@alpha$ python </path/to/your/script/mnistFashion.py> --out-layer1=200 --batchsize=10 --epochs=3
    [...]
    Epoch 3
    -------------------------------
    loss: 1.422406  [    0/60000]
    loss: 0.852647  [10000/60000]
    loss: 1.139685  [20000/60000]
    loss: 0.572221  [30000/60000]
    loss: 1.516888  [40000/60000]
    loss: 0.445737  [50000/60000]
    Test Error:
     Accuracy: 69.5%, Avg loss: 0.878329

    RESULT: 0.878329

    Done!
    ```

Using the modified script within OmniOpt requires configuring and loading of the software
environment. The recommended way is to wrap the necessary calls in a shell script.

??? example "Example for wrapping with shell script"

    ```bash
    #!/bin/bash -l
    # ^ Shebang-Line, so that it is known that this is a bash file
    # -l means 'load this as login shell', so that /etc/profile gets loaded and you can use 'module load' or 'ml' as usual

    # If you don't use this script via `./run.sh' or just `srun run.sh', but like `srun bash run.sh', please add the '-l' there too.
    # Like this:
    # srun bash -l run.sh

    # Load modules your program needs, always specify versions!
    module load release/23.04 GCC/10.2.0 CUDA/11.1.1 OpenMPI/4.0.5 PyTorch/1.7.1
    source </path/to/workspace/python-environments/torchvision_env>/bin/activate #activate virtual environment

    # Load your script. $@ is all the parameters that are given to this shell file.
    python </path/to/your/script/mnistFashion.py> $@
    ```

When the wrapped shell script is running properly, the preparations are finished and the next step
is configuring OmniOpt.

### Configure and Run OmniOpt

Configuring OmniOpt is done via the GUI at
[https://imageseg.scads.ai/omnioptgui/](https://imageseg.scads.ai/omnioptgui/).
This GUI guides through the configuration process and as result a configuration file is created
automatically according to the GUI input. If you are more familiar with using OmniOpt later on,
this configuration file can be modified directly without using the GUI.

A screenshot of
[the GUI](https://imageseg.scads.ai/omnioptgui/?maxevalserror=5&mem_per_worker=1000&number_of_parameters=3&param_0_values=10%2C50%2C100&param_1_values=8%2C16%2C32&param_2_values=10%2C15%2C30&param_0_name=out-layer1&param_1_name=batchsize&param_2_name=batchsize&account=&projectname=mnist_fashion_optimization_set_1&partition=alpha&searchtype=tpe.suggest&param_0_type=hp.choice&param_1_type=hp.choice&param_2_type=hp.choice&max_evals=1000&objective_program=bash%20%3C%2Fpath%2Fto%2Fwrapper-script%2Frun-mnist-fashion.sh%3E%20--out-layer1%3D%28%24x_0%29%20--batchsize%3D%28%24x_1%29%20--epochs%3D%28%24x_2%29&workdir=%3C%2Fscratch%2Fws%2Fomniopt-workdir%2F%3E),
including a properly configuration for the MNIST fashion example is shown below.

Please modify the paths for `objective program` and `workdir` according to your needs.

![GUI for configuring OmniOpt](misc/hyperparameter_optimization-OmniOpt-GUI.png)
{: align="center"}

Using OmniOpt for a first trial example, it is often sufficient to concentrate on the following
configuration parameters:

1. **Optimization run name:** A name for an OmniOpt run given a belonging configuration.
1. **Partition:** Choose the cluster on the ZIH system that fits the programs' needs.
1. **Enable GPU:** Decide whether a program could benefit from GPU usage or not.
1. **Workdir:** The directory where OmniOpt is saving its necessary files and all results. Derived
   from the optimization run name, each configuration creates a single directory.
   Make sure that this working directory is writable from the compute nodes. It is recommended to
   use a [workspace](../data_lifecycle/workspaces.md).
1. **Objective program:** Provide all information for program execution. Typically, this will
   contain the command for executing a wrapper script.
1. **Parameters:** The hyperparameters to be optimized with the names OmniOpt should use. For the
   example here, the variable names are identical to the input parameters of the Python script.
   However, these names can be chosen differently, since the connection to OmniOpt is realized via
   the variables (`$x_0`), (`$x_1`), etc. from the GUI section "Objective program". Please note that
   it is not necessary to name the parameters explicitly in your script but only within the OmniOpt
   configuration.

After all parameters are entered into the GUI, the call for OmniOpt is generated automatically and
displayed on the right. This command contains all necessary instructions (including requesting
resources with Slurm). **Thus, this command can be executed directly on a login node on the ZIH
system.**

![GUI for configuring OmniOpt](misc/hyperparameter_optimization-OmniOpt-final-command.png)
{: align="center"}

After executing this command OmniOpt is doing all the magic in the background and there are no
further actions necessary.

??? hint "Hints on the working directory"

    1. Starting OmniOpt without providing a working directory will store OmniOpt into the present directory.
    1. Within the given working directory, a new folder named "omniopt" as default, is created.
    1. Within one OmniOpt working directory, there can be multiple optimization projects.
    1. It is possible to have as many working directories as you want (with multiple optimization runs).
    1. It is recommended to use a [workspace](../data_lifecycle/workspaces.md) as working directory, but not the home directory.

### Check and Evaluate OmniOpt Results

For getting informed about the current status of OmniOpt or for looking into results, the evaluation
tool of OmniOpt is used. Switch to the OmniOpt folder and run `evaluate-run.sh`.

```console
marie@login$ bash </data/horse/ws/omniopt-workdir/>evaluate-run.sh
```

After initializing and checking for updates in the background, OmniOpt is asking to select the
optimization run of interest.  After selecting the optimization run, there will be a menu with the
items as shown below.  If OmniOpt has still running jobs there appear some menu items that refer to
these running jobs (image shown below to the right).

evaluation options (all jobs finished)                            |  evaluation options (still running jobs)
:--------------------------------------------------------------:|:-------------------------:
![GUI for configuring OmniOpt](misc/OmniOpt-evaluate-menu.png)  |  ![GUI for configuring OmniOpt](misc/OmniOpt-still-running-jobs.png)

For now, we assume that OmniOpt has finished already.
In order to look into the results, there are the following basic approaches.

1. **Graphical approach:**
    There are basically two graphical approaches: two dimensional scatter plots and parallel plots.

    Below there is shown a parallel plot from the MNIST fashion example.
    ![GUI for configuring OmniOpt](misc/OmniOpt-parallel-plot.png){: align="center"}

    ??? hint "Hints on parallel plots"

        Parallel plots are suitable especially for dealing with multiple dimensions. The parallel
        plot created by OmniOpt is an interactive `html` file that is stored in the OminOpt working
        directory under `projects/<name_of_optimization_run>/parallel-plot`. The interactivity
        of this plot is intended to make optimal combinations of the hyperparameters visible more
        easily. Get more information about this interactivity by clicking the "Help" button at the
        top of the graphic (see red arrow on the image above).

    After creating a 2D scatter plot or a parallel plot, OmniOpt will try to display the
    corresponding file (`html`, `png`) directly on the ZIH system. Therefore, X11 forwarding must be
    enabled, either by [SSH configuration
    ](../access/ssh_login.md#configuring-default-parameters-for-ssh) or by using e.g. `ssh -XC alpha`
    while logging in. Nevertheless, because of latency using X11 forwarding, it is recommended to
    download the created files and explore them on the local machine (esp. for the parallel plot).
    The created files are saved at
    `projects/<name_of_optimization_run>/{2d-scatterplots,parallel-plot}`.

1. **Getting the raw data:**
    As a second approach, the raw data of the optimization process can be exported as a CSV file.
    The created output files are stored in the folder `projects/<name_of_optimization_run>/csv`.


# External Licenses

It is possible (please [contact the support team](../support/support.md) first) for users to install
their own software and use their own license servers, e.g. FlexLM. The outbound IP addresses from
ZIH systems are:

- NAT via 141.76.5.48/28 (for both login and compute nodes)

The IT department of the external institute has to open the firewall for license communications
(might be multiple ports) from ZIH systems and enable handing-out license to these IPs and login.

The user has to configure the software to use the correct license server. This can typically be done
by environment variable or file.

!!! attention

    If you are using software we have installed, but bring your own license key (e.g.
    commercial ANSYS), make sure that to substitute the environment variables we are using as default!
    (To verify this, run `printenv|grep licenses` and make sure that you dont' see entries refering to
    our ZIH license server.)

## How to adjust the license setting

Most programs, that work with the FlexLM license manager,
can be instructed to look for another license server,
by overwriting the environment variable "LM_LICENSE_FILE".
Do note that not all proprietary software looks for that environment variable.

!!! example "Changing the license server"
    ```console
    marie@compute$ export LM_LICENSE_FILE=12345@example.com
    ```
    Here "12345" is the port on which the license server is listening,
    while "example.com" is the network addresss of the license server.

Some licensed software comes with a license file,
it can be similarly specified like this:

!!! example "Changing license"
    ```bash
    export LM_LICENSE_FILE=<SOME_PATH>
    ```

    Example:

    ```console
    export LM_LICENSE_FILE=$HOME/mylicense.dat
    ```


# Record Course of Events with lo2s

Lightweight node-level performance monitoring tool `lo2s` creates parallel OTF2 traces with a focus
on both application and system view. The traces can contain any of the following information:

* From running threads
    * Calling context samples based on instruction overflows
    * The calling context samples are annotated with the disassembled assembler instruction string
    * The frame pointer-based call-path for each calling context sample
    * Per-thread performance counter readings
    * Which thread was scheduled on which CPU at what time
* From the system
    * Metrics from tracepoints (e.g., the selected C-state or P-state)
    * The node-level system tree (CPUs (HW-threads), cores, packages)
    * CPU power measurements (x86_energy)
    * Microarchitecture specific metrics (x86_adapt, per package or core)
    * Arbitrary metrics through plugins (Score-P compatible)

In general, `lo2s` operates either in **process monitoring** or **system monitoring** mode.

With **process monitoring**, all information is grouped by each thread of a monitored process
group - it shows you *on which CPU is each monitored thread running*. `lo2s` either acts as a
prefix command to run the process (and also tracks its children) or `lo2s` attaches to a running
process.

In the **system monitoring** mode, information is grouped by logical CPU - it shows you
*which thread was running on a given CPU*. Metrics are also shown per CPU.

In both modes, `lo2s` always groups system-level metrics (e.g., tracepoints) by their respective
system hardware component.

## Usage

Only the basic usage is shown in this Wiki. For a more detailed explanation, refer to the
[Lo2s website](https://github.com/tud-zih-energy/lo2s).

Before using `lo2s`, set up the correct environment with

```console
marie@login$ module load lo2s
```

As `lo2s` is built upon [perf](perf_tools.md), its usage and limitations are very similar to that.
In particular, you can use `lo2s` as a prefix command just like `perf`. Even some of the command
line arguments are inspired by `perf`. The main difference to `perf` is that `lo2s` will output
a [Vampir trace](vampir.md), which allows a full-blown performance analysis almost like
[Score-P](scorep.md).

To record the behavior of an application, prefix the application run with `lo2s`. We recommend
using the double dash `--` to prevent mixing command line arguments between `lo2s` and the user
application. In the following example, we run `lo2s` on the application `sleep 2`.

```console
marie@compute$ lo2s --no-kernel -- sleep 2
[ lo2s: sleep 2  (0), 1 threads, 0.014082s CPU, 2.03315s total ]
[ lo2s: 5 wakeups, wrote 2.48 KiB lo2s_trace_2021-10-12T12-39-06 ]
```

This will record the application in the `process monitoring mode`. This means, the applications
process, its forked processes, and threads are recorded and can be analyzed using Vampir.
The main view will represent each process and thread over time. There will be a metric "CPU"
indicating for each process, on which CPU it was executed during the runtime.

## Required Permissions

By design, `lo2s` almost exclusively utilizes Linux Kernel facilities such as perf and tracepoints
to perform the application measurements. For security reasons, these facilities require special
permissions, in particular `perf_event_paranoid` and read permissions to the `debugfs` under
`/sys/kernel/debug`.

Luckily, for the `process monitoring mode` the default settings allow you to run `lo2s` just fine.
All you need to do is pass the `--no-kernel` parameter like in the example above.

For the `system monitoring mode` you can get the required permission with the Slurm parameter
`--exclusive`. (Note: Regardless of the actual requested processes per node, you will accrue
cpu-hours as if you had reserved all cores on the node.)

## Memory Requirements

When requesting memory for your jobs, you need to take into account that `lo2s` needs a substantial
amount of memory for its operation. Unfortunately, the amount of memory depends on the application.
The amount mainly scales with the number of processes spawned by the traced application. For each
processes, there is a fixed-sized buffer. This should be fine for a typical HPC application, but
can lead to extreme cases there the buffers are orders of magnitude larger than the resulting trace.
For instance, recording a CMake run, which spawns hundreds of processes, each running only for
a few milliseconds, leaving each buffer almost empty. Still, the buffers needs to be allocated
and thus require a lot of memory.

Given such a case, we recommend to use the `system monitoring mode` instead, as the memory in this
mode scales with the number of logical CPUs instead of the number of processes.

## Advanced Topic: System Monitoring

The `system monitoring mode` gives a different view. As the name implies, the focus isn't on processes
anymore, but the system as a whole. In particular, a trace recorded in this mode will show a timeline
for each logical CPU of the system. To enable this mode, you need to pass `-a` parameter.

```console
marie@compute$ lo2s -a
^C[ lo2s (system mode): monitored processes: 0, 0.136623s CPU, 13.7872s total ]
[ lo2s (system mode): 36 wakeups, wrote 301.39 KiB lo2s_trace_2021-11-01T09-44-31 ]
```

Note: As you can read in the above example, `lo2s` monitored zero processes even though it was run
in the `system monitoring mode`. Certainly, there are more than none processes running on a system.
However, as the user accounts on our HPC systems are limited to only see their own processes and `lo2s`
records in the scope of the user, it will only see the users own processes. Hence, in the example
above, there are no other processes running.

When using the `system monitoring mode` without passing a program, `lo2s` will run indefinitely.
You can stop the measurement by sending `lo2s` a `SIGINT` signal or hit `ctrl+C`. However, if you pass
a program, `lo2s` will start that program and run the measurement until the started process finishes.
Of course, the process and any of its child processes and threads will be visible in the resulting trace.

```console
marie@compute$ lo2s -a -- sleep 10
[ lo2s (system mode): sleep 10  (0), 1 threads, monitored processes: 1, 0.133598s CPU, 10.3996s total ]
[ lo2s (system mode): 39 wakeups, wrote 280.39 KiB lo2s_trace_2021-11-01T09-55-04 ]
```

Like in the `process monitoring mode`, `lo2s` can also sample instructions in the system monitoring mode.
You can enable the instruction sampling by passing the parameter `--instruction-sampling` to `lo2s`.

```console
marie@compute$ lo2s -a --instruction-sampling -- make -j
[ lo2s (system mode): make -j  (0), 268 threads, monitored processes: 286, 258.789s CPU, 445.076s total ]
[ lo2s (system mode): 3815 wakeups, wrote 39.24 MiB lo2s_trace_2021-10-29T15-08-44 ]
```

## Advanced Topic: Metric Plugins

`Lo2s` is compatible with [Score-P](scorep.md) metric plugins, but only a subset will work.
In particular, `lo2s` only supports asynchronous plugins with the per host or once scope.
You can find a large set of plugins in the [Score-P Organization on GitHub](https://github.com/score-p).

To activate plugins, you can use the same environment variables as with Score-P, or with `LO2S` as
prefix:

  - LO2S_METRIC_PLUGINS
  - LO2S_METRIC_PLUGIN
  - LO2S_METRIC_PLUGIN_PLUGIN


# Machine Learning

This is an introduction of how to run machine learning (ML) applications on ZIH systems.
We recommend using the GPU clusters `Alpha`, `Capella`, and `Power9` for machine learning purposes.
The hardware specification of each cluster can be found in the page
[HPC Resources](../jobs_and_resources/hardware_overview.md).

## Modules

The way of loading modules is identical on each cluster. Here we show an example
 on the cluster `Alpha` how to load the module environment:

```console
marie@alpha$ module load release/23.04
```

!!! note

    Software and their available versions may differ among the clusters. Check the available
    modules with `module spider <module_name>`

## Machine Learning via Console

### Python and Virtual Environments

Python users should use a [virtual environment](python_virtual_environments.md) when conducting
machine learning tasks via console.

For more details on machine learning or data science with Python see
[data analytics with Python](data_analytics_with_python.md).

### R

R also supports machine learning via console. It does not require a virtual environment due to a
different package management.

For more details on machine learning or data science with R see
[data analytics with R](data_analytics_with_r.md#r-console).

## Machine Learning with Jupyter

The [Jupyter Notebook](https://jupyter.org/) is an open-source web application that allows you to
create documents containing live code, equations, visualizations, and narrative text.
[JupyterHub](../access/jupyterhub.md) allows to work with machine learning frameworks (e.g.
TensorFlow or PyTorch) on ZIH systems and to run your Jupyter notebooks on HPC nodes.

After accessing JupyterHub, you can start a new session and configure it. For machine learning
purposes, select either cluster `Alpha`, `Capella` or `Power9` and the resources, your application requires.

In your session you can use [Python](data_analytics_with_python.md#jupyter-notebooks),
[R](data_analytics_with_r.md#r-in-jupyterhub) or [RStudio](data_analytics_with_rstudio.md) for your
machine learning and data science topics.

## Machine Learning with Containers

Some machine learning tasks require using containers. In the HPC domain, the
[Singularity](https://singularity.hpcng.org/) container system is a widely used tool. Docker
containers can also be used by Singularity. You can find further information on working with
containers on ZIH systems in our [containers documentation](containers.md).

The official source for Docker containers with TensorFlow, PyTorch and many other packages is
the [PowerAI container](https://hub.docker.com/r/ibmcom/powerai/) DockerHub repository of IBM.

!!! note

    You could find other versions of software in the container on the "tag" tab on the Docker web
    page of the container.

In the following example, we build a Singularity container on the cluster `Power9`
 with TensorFlow from the DockerHub and start it:

```console
marie@power9$ singularity build my-ML-container.sif docker://ibmcom/powerai:1.6.2-tensorflow-ubuntu18.04-py37-ppc64le    #create a container from the DockerHub with TensorFlow version 1.6.2
[...]
marie@power9$ singularity run --nv my-ML-container.sif    #run my-ML-container.sif container supporting the Nvidia's GPU. You can also work with your container by: singularity shell, singularity exec
[...]
```

## Additional Libraries for Machine Learning

The following NVIDIA libraries are available on all nodes:

| Name  |  Path                                   |
|-------|-----------------------------------------|
| NCCL  | `/usr/local/cuda/targets/ppc64le-linux` |
| cuDNN | `/usr/local/cuda/targets/ppc64le-linux` |

!!! note

    For optimal NCCL performance it is recommended to set the
    **NCCL_MIN_NRINGS** environment variable during execution. You can try
    different values but 4 should be a pretty good starting point.

```console
marie@compute$ export NCCL_MIN_NRINGS=4
```

### HPC-Related Software

The following HPC related software is installed on all nodes:

| Name             |  Path                    |
|------------------|--------------------------|
| IBM Spectrum MPI | `/opt/ibm/spectrum_mpi/` |
| PGI compiler     | `/opt/pgi/`              |
| IBM XLC Compiler | `/opt/ibm/xlC/`          |
| IBM XLF Compiler | `/opt/ibm/xlf/`          |
| IBM ESSL         | `/opt/ibmmath/essl/`     |
| IBM PESSL        | `/opt/ibmmath/pessl/`    |

## Datasets for Machine Learning

There are many different datasets designed for research purposes. If you would like to download some
of them, keep in mind that many machine learning libraries have direct access to public datasets
without downloading it, e.g. [TensorFlow Datasets](https://www.tensorflow.org/datasets). If you
still need to download some datasets use [Datamover](../data_transfer/datamover.md) machine.

### The ImageNet Dataset

The ImageNet project is a large visual database designed for use in visual object recognition
software research. In order to save space in the filesystem by avoiding to have multiple duplicates
of this lying around, we have put a copy of the ImageNet database (ILSVRC2012 and ILSVR2017) under
`/data/horse/shared/imagenet` which you can use without having to download it again. For the future,
the ImageNet dataset will be available in
[Warm Archive](../data_lifecycle/workspaces.md#mid-term-storage). ILSVR2017 also includes a dataset
for recognition objects from a video. Please respect the corresponding
[Terms of Use](https://image-net.org/download.php).


# Mathematics Applications

!!! cite "Galileo Galilei"

    Nature is written in mathematical language.

<!--*Please do not run expensive interactive sessions on the login nodes.  Instead, use* `srun --pty-->
<!--...` *to let the batch system place it on a compute node.*-->

## Mathematica

Mathematica is a general computing environment, organizing many algorithmic, visualization, and user
interface capabilities within a document-like user interface paradigm.

### Fonts

To remotely use the graphical front-end, you have to add the Mathematica fonts to the local
font manager.

#### Linux Workstation

You need to copy the fonts from ZIH systems to your local system and expand the font path

```console
marie@local$ scp -r dataport:/software/rapids/r24.04/Mathematica/13.0.1/SystemFiles/Fonts/Type1/
~/.fonts
marie@local$ xset fp+ ~/.fonts/Type1
```

!!! important "SCP command"

    The previous SCP command requires that you have already set up your [SSH configuration
    ](../access/ssh_login.md#configuring-default-parameters-for-ssh).

#### Windows Workstation

You have to add additional Mathematica fonts at your local PC
[download fonts archive](misc/Mathematica-Fonts.zip).

If you use **Xming** as X-server at your PC (refer to
[remote access from Windows](../access/ssh_login.md), follow these steps:

1. Create a new folder `Mathematica` in the directory `fonts` of the installation directory of Xming
   (mostly: `C:\\Programme\\Xming\\fonts\\`)
1. Extract the fonts archive into this new directory `Mathematica`.  In result you should have the
   two directories `DBF` and `Type1`.
1. Add the path to these font files into the file `font-dirs`.  You can find it in
   `C:\\Programme\\Xming\\`.

```shell-session
# font-dirs
# comma-separated list of directories to add to the default font path
# defaults are built-ins, misc, TTF, Type1, 75dpi, 100dpi
# also allows entries on individual lines
C:\Programme\Xming\fonts\dejavu,C:\Programme\Xming\fonts\cyrillic
C:\Programme\Xming\fonts\Mathematica\DBF
C:\Programme\Xming\fonts\Mathematica\Type1
C:\WINDOWS\Fonts
```

### Using Textual Interface of Mathematica

Once a Mathematica module is loaded, you can use Mathematica's textual interface via the command
`math`.

```console
marie@login$ module load release/24.04 Mathematica/13.0.1
[...]
marie@login$ math
Mathematica 13.0.1 Kernel for Linux x86 (64-bit)
Copyright 1988-2022 Wolfram Research, Inc.

In[1]:= Sum[i, {i,1,100}]
Out[1]= 5050
In[2]:= Quit[];
```

### Mathematica and Slurm

For running compute intensive and long calculations, please use the batch system
[Slurm](../jobs_and_resources/slurm.md). To submit Mathematica jobs to the resource scheduler Slurm,
the Mathematica commands to be executed must be containined in a single `.m`-script. The `.m`-script
will then be passed to the `math` command in a job file.

??? example "1. Hello-world-script for Mathematica"

    The file `math_example.m` is your input script that includes the calculation statements for
    Mathematica.

    ```
    Print["Hello, World!"];

    (* Perform some calculations *)
    x = 10;
    y = 20;

    sum = x + y;
    product = x * y;

    Print["The sum of ", x, " and ", y, " is: ", sum];
    Print["The product of ", x, " and ", y, " is: ", product];
    ```

??? example "2. Job file"

    The file `jobfile.sh` is your jobfile with the resource specifications and the commands to
    executed.

    ```bash
    #!/bin/bash
    #SBATCH --time=00:05:00
    #SBATCH --ntasks=1

    # prepare environment; load module
    module purge
    module load release/24.04 Mathematica/13.0.1

    # run the math kernel; perform calculations
    math -script math_example.m > math.output
    ```

??? example "3. Submit the job"

    Invoke the command `sbatch jobfile.sh` to submit the job to the scheduler. The scheduler will
    provide you the unqiue job id.

    ```
    marie@login$ sbatch jobfile.sh
    Submitted batch job 10705392
    ```

    All output of the `math` command  will be saved in the file `math.output`.

!!! note

    Mathematica licenses are limited.

There exist two types, *MathKernel* and *SubMathKernel* licenses. Every sequential job you start
will consume a MathKernel license of which we only have 61. We do have, however, 488 SubMathKernel
licenses, so please, don't start many sequential jobs but try to parallelize your calculation,
utilizing multiple SubMathKernel licenses per job, in order to achieve a more reasonable license
usage.

## MATLAB

[MATLAB](https://de.mathworks.com/products/matlab.html) is a numerical computing environment and
programming language. Created by The MathWorks, MATLAB allows easy matrix manipulation, plotting of
functions and data, implementation of algorithms, creation of user interfaces, and interfacing with
programs in other languages.  Although it specializes in numerical computing, an optional toolbox
interfaces with the Maple symbolic engine, allowing it to be part of a full computer algebra system.

Running MATLAB via the batch system could look like this (for 2 Gb RAM per core and 12 cores
reserved). Please adapt this to your needs!

```console
marie@login$ module load MATLAB
marie@login$ srun --time=8:00 --cpus-per-task=12 --mem-per-cpu=2000 --pty --x11=first bash
marie@compute$ matlab
```

With following command you can see a list of installed software - also
the different versions of MATLAB.

```console
marie@login$ module avail MATLAB
```

Please choose one of these, then load the chosen software with the command:

```bash
marie@login$ module load MATLAB/<version>
```

Or use:

```console
marie@login$ module load MATLAB
```

(then you will get the most recent MATLAB version.
[Refer to the modules page for details.](modules.md#module-commands))

### Interactive MATLAB-GUI

If you have a connection with X11 forwarding enabled.

#### Interactive

You should allocate a CPU for your working with command:

```console
marie@login$ srun --time=01:00:00 --pty --x11=first bash
```

- now you can call "matlab" (you have 1h to work with the MATLAB-GUI)

#### MATLAB container

We provide MATLAB containers that can be started as following:

```console
marie@login$ start_matlab.sh
```

### Non-interactive

Using Scripts

You have to start matlab-calculation as a Batch-Job via command

```console
marie@login$ srun --pty matlab -batch <basename_of_your_matlab_script>
# NOTE: you must omit the file extension ".m" here, because -r expects a matlab command or function call, not a file-name.
```

!!! info "License occupying"

    While running your calculations as a script this way is possible, it is generally frowned upon,
    because you are occupying MATLAB licenses for the entire duration of your calculation when doing so.
    Since the available licenses are limited, it is highly recommended you first compile your script via
    the MATLAB Compiler (`mcc`) before running it for a longer period of time on our systems.  That way,
    you only need to check-out a license during compile time (which is relatively short) and can run as
    many instances of your calculation as you'd like, since it does not need a license during runtime
    when compiled to a binary.

You can find detailed documentation on the MATLAB compiler at
[MathWorks' help pages](https://de.mathworks.com/help/compiler/).

### Using the MATLAB Compiler

Compile your `.m` script into a binary:

```bash
marie@login$ mcc -m name_of_your_matlab_script.m -o compiled_executable -R -nodisplay -R -nosplash
```

This will also generate a wrapper script called `run_compiled_executable.sh` which sets the required
library path environment variables in order to make this work. It expects the path to the MATLAB
installation as an argument, you can use the environment variable `$EBROOTMATLAB` as set by the
module file for that.

Then run the binary via the wrapper script in a job (just a simple example, you should be using an
[sbatch script](../jobs_and_resources/slurm.md#job-submission) for that)

```bash
marie@login$ srun ./run_compiled_executable.sh ${EBROOTMATLAB}
```

### Parallel MATLAB

#### With 'local' Configuration

- If you want to run your code in parallel, please request as many cores as you need!
- Start a batch job with the number `N` of processes, e.g., `srun --cpus-per-task=4 --pty
  --x11=first bash -l`
- Run MATLAB with the GUI or the CLI or with a script
- Inside MATLAB use `parpool open 4` to start parallel processing

!!! example "Example for 1000*1000 matrix-matrix multiplication"

    ```bash
    R = distributed.rand(1000);
    D = R * R
    ```

- Close parallel task using `delete(gcp('nocreate'))`

#### With parfor

- Start a batch job with the number `N` of processes (,e.g., `N=12`)
- Inside use `matlabpool open N` or `matlabpool(N)` to start parallel processing. It will use
  the 'local' configuration by default.
- Use `parfor` for a parallel loop, where the **independent** loop iterations are processed by `N`
  processes

!!! example

    ```bash
    parfor i = 1:3
        c(:,i) = eig(rand(1000));
    end
    ```

Please refer to the documentation `help parfor` for further information.

### MATLAB Parallel Computing Toolbox

In the following, the steps to configure MATLAB to submit jobs to a cluster, retrieve results, and
debug errors are outlined.

#### Configuration – MATLAB client on the cluster

After logging into the HPC system, you configure MATLAB to run parallel jobs on the HPC system by
calling the shell script `configCluster.sh`.  This only needs to be called once per version of
MATLAB.

```console
marie@login$ module load MATLAB
marie@login$ configCluster.sh
```

Jobs will now default to the HPC system rather than submit to the local machine.

#### Installation and Configuration – MATLAB client off the cluster

The MATLAB support package for ZIH Systems can be found as follows:

* Windows:
    * [tud.nonshared.R2021b.zip](misc/tud.nonshared.R2021b.zip)
    * [tud.nonshared.R2022a.zip](misc/tud.nonshared.R2022a.zip)
* Linux/Mac:
    * [tud.nonshared.R2021b.tar.gz](misc/tud.nonshared.R2021b.tar.gz)
    * [tud.nonshared.R2022a.tar.gz](misc/tud.nonshared.R2022a.tar.gz)

Download the appropriate archive file and start MATLAB. The archive file should be extracted
in the location returned by calling

```matlabsession
>> userpath
```

Configure MATLAB to run parallel jobs on ZIH Systems by calling `configCluster`. `configCluster`
only needs to be called once per version of MATLAB.

```matlabsession
>> configCluster
```

Submission to the remote cluster requires SSH credentials. You will be prompted for your SSH
username and password or identity file (private key). The username and location of the private key
will be stored in MATLAB for future sessions. Jobs will now default to the cluster rather than
submit to the local machine.

!!! note

    If you would like to submit to the local machine then run the following command:

    ```matlab
    >> % Get a handle to the local resources
    >> c = parcluster('local');
    ```

#### Configuring Jobs

Prior to submitting the job, you can specify various parameters to pass to your jobs, such as queue,
e-mail, walltime, etc. *Only `MemPerCpu` and `QueueName` are required*.

```matlabsession
>> % Get a handle to the cluster
>> c = parcluster;

[REQUIRED]

>> % Specify memory to use, per core (default: 2gb)
>> c.AdditionalProperties.MemPerCpu = '4gb';

>> % Specify the walltime (e.g., 5 hours)
>> c.AdditionalProperties.WallTime = '05:00:00';

[OPTIONAL]

>> % Specify the account to use
>> c.AdditionalProperties.Account = 'account-name';

>> % Request constraint
>> c.AdditionalProperties.Constraint = 'a-constraint';

>> % Request job to run on exclusive node(s) (default: false)
>> c.AdditionalProperties.EnableExclusive = true;

>> % Request email notification of job status
>> c.AdditionalProperties.EmailAddress = 'user-id@tu-dresden.de';

>> % Specify number of GPUs to use (GpuType is optional)
>> c.AdditionalProperties.GpusPerNode = 1;
>> c.AdditionalProperties.GpuType = 'gpu-card';

>> % Specify the queue to use
>> c.AdditionalProperties.Partition = 'queue-name';

>> % Specify a reservation to use
>> c.AdditionalProperties.Reservation = 'a-reservation';
```

Save changes after modifying `AdditionalProperties` for the above changes to persist between MATLAB
sessions.

```matlabsession
>> c.saveProfile
```

To see the values of the current configuration options, display `AdditionalProperties`.

```matlabsession
>> % To view current properties
>> c.AdditionalProperties
```

You can unset a value when no longer needed.

```matlabsession
>> % Turn off email notifications
>> c.AdditionalProperties.EmailAddress = '';
>> c.saveProfile
```

#### Interactive Jobs - MATLAB Client on the Cluster

To run an interactive pool job on the ZIH systems, continue to use `parpool` as you’ve done before.

```matlabsession
>> % Get a handle to the cluster
>> c = parcluster;

>> % Open a pool of 64 workers on the cluster
>> pool = c.parpool(64);
```

Rather than running local on your machine, the pool can now run across multiple nodes on the
cluster.

```matlabsession
>> % Run a parfor over 1000 iterations
>> parfor idx = 1:1000
      a(idx) = …
   end
```

Once you are done with the pool, delete it.

```matlabsession
>> % Delete the pool
>> pool.delete
```

#### Independent Batch Job

Use the batch command to submit asynchronous jobs to the HPC system. The `batch` command will return
a job object which is used to access the output of the submitted job. See the MATLAB documentation
for more help on `batch`.

```matlabsession
>> % Get a handle to the cluster
>> c = parcluster;

>> % Submit job to query where MATLAB is running on the cluster
>> job = c.batch(@pwd, 1, {},  ...
       'CurrentFolder','.', 'AutoAddClientPath',false);

>> % Query job for state
>> job.State

>> % If state is finished, fetch the results
>> job.fetchOutputs{:}

>> % Delete the job after results are no longer needed
>> job.delete
```

To retrieve a list of currently running or completed jobs, call `parcluster` to retrieve the cluster
object. The cluster object stores an array of jobs that were run, are running, or are queued to
run. This allows us to fetch the results of completed jobs. Retrieve and view the list of jobs as
shown below.

```matlabsession
>> c = parcluster;
>> jobs = c.Jobs;
```

Once you have identified the job you want, you can retrieve the results as done previously.

`fetchOutputs` is used to retrieve function output arguments; if calling `batch` with a script, use
`load` instead. Data that has been written to files on the cluster needs be retrieved directly
from the filesystem (e.g. via ftp). To view results of a previously completed job:

```matlabsession
>> % Get a handle to the job with ID 2
>> job2 = c.Jobs(2);
```

!!! note

    You can view a list of your jobs, as well as their IDs, using the above `c.Jobs` command.

    ```matlabsession
    >> % Fetch results for job with ID 2
    >> job2.fetchOutputs{:}
    ```

#### Parallel Batch Job

You can also submit parallel workflows with the `batch` command. Let’s use the following example
for a parallel job, which is saved as `parallel_example.m`.

```matlab
function [t, A] = parallel_example(iter)

if nargin==0
    iter = 8;
end

disp('Start sim')

t0 = tic;
parfor idx = 1:iter
    A(idx) = idx;
    pause(2)
    idx
end
t = toc(t0);

disp('Sim completed')

save RESULTS A

end
```

This time when you use the `batch` command, to run a parallel job, you will also specify a MATLAB
Pool.

```matlabsession
>> % Get a handle to the cluster
>> c = parcluster;

>> % Submit a batch pool job using 4 workers for 16 simulations
>> job = c.batch(@parallel_example, 1, {16}, 'Pool',4, ...
       'CurrentFolder','.', 'AutoAddClientPath',false);

>> % View current job status
>> job.State

>> % Fetch the results after a finished state is retrieved
>> job.fetchOutputs{:}
ans =
  8.8872
```

The job ran in 8.89 seconds using four workers. Note that these jobs will always request N+1 CPU
cores, since one worker is required to manage the batch job and pool of workers. For example, a
job that needs eight workers will consume nine CPU cores.

You might run the same simulation but increase the Pool size. This time, to retrieve the results later,
you will keep track of the job ID.

!!! note

    For some applications, there will be a diminishing return when allocating too many workers, as
    the overhead may exceed computation time.

    ```matlabsession
    >> % Get a handle to the cluster
    >> c = parcluster;

    >> % Submit a batch pool job using 8 workers for 16 simulations
    >> job = c.batch(@parallel_example, 1, {16}, 'Pool', 8, ...
           'CurrentFolder','.', 'AutoAddClientPath',false);

    >> % Get the job ID
    >> id = job.ID
    id =
      4
    >> % Clear job from workspace (as though you quit MATLAB)
    >> clear job
    ```

Once you have a handle to the cluster, you can call the `findJob` method to search for the job with
the specified job ID.

```matlabsession
>> % Get a handle to the cluster
>> c = parcluster;

>> % Find the old job
>> job = c.findJob('ID', 4);

>> % Retrieve the state of the job
>> job.State
ans =
  finished
>> % Fetch the results
>> job.fetchOutputs{:};
ans =
  4.7270
```

The job now runs in 4.73 seconds using eight workers. Run code with different number of workers to
determine the ideal number to use. Alternatively, to retrieve job results via a graphical user
interface, use the Job Monitor (Parallel > Monitor Jobs).

![Job monitor](misc/matlab_monitor_jobs.png)
{: summary="Retrieve job results via GUI using the Job Monitor." align="center"}

#### Debugging

If a serial job produces an error, call the `getDebugLog` method to view the error log file.  When
submitting independent jobs, with multiple tasks, specify the task number.

```matlabsession
>> c.getDebugLog(job.Tasks(3))
```

For Pool jobs, only specify the job object.

```matlabsession
>> c.getDebugLog(job)
```

When troubleshooting a job, the cluster admin may request the scheduler ID of the job.  This can be
derived by calling `schedID`.

```matlabsession
>> schedID(job)
ans =
  25539
```

#### Further Reading

To learn more about the MATLAB Parallel Computing Toolbox, check out these resources:

* [Parallel Computing Coding
    Examples](https://www.mathworks.com/help/parallel-computing/examples.html)
* [Parallel Computing Documentation](http://www.mathworks.com/help/distcomp/index.html)
* [Parallel Computing Overview](http://www.mathworks.com/products/parallel-computing/index.html)
* [Parallel Computing
    Tutorials](http://www.mathworks.com/products/parallel-computing/tutorials.html)
* [Parallel Computing Videos](http://www.mathworks.com/products/parallel-computing/videos.html)
* [Parallel Computing Webinars](http://www.mathworks.com/products/parallel-computing/webinars.html)
* [MATLAB NHR Tutorial Slides: Parallel Computing with MATLAB](https://event.zih.tu-dresden.de/nhr/matlab/module1/materials)
* [MATLAB NHR Tutorial Slides: Machine Learning with MATLAB](https://event.zih.tu-dresden.de/nhr/matlab/module2/materials)
* [MATLAB NHR Tutorial Slides: Deep Learning with MATLAB](https://event.zih.tu-dresden.de/nhr/matlab/module3/materials)
* [MATLAB NHR Tutorial Slides: Interoperability of MATLAB and Python](https://event.zih.tu-dresden.de/nhr/matlab/module4/materials)


# Mathematics Libraries

Many software heavily relies on math libraries, e.g., for linear algebra or FFTW calculations.
Writing portable and fast math functions is a really challenging task. You can try it for fun, but you
really should avoid writing you own matrix-matrix multiplication. Thankfully, there are several
high quality math libraries available at ZIH systems.

In the following, a few often-used interfaces/specifications and libraries are described. All
libraries are available as [modules](modules.md).

## BLAS, LAPACK and ScaLAPACK

Over the last decades, the three de-facto standard specifications BLAS, LAPACK and ScaLAPACK for
basic linear algebra routines have been emerged.

The [BLAS](https://www.netlib.org/blas/) (Basic Linear Algebra Subprograms) specification contains routines
for common linear algebra operations such as vector addition, matrix-vector multiplication, and dot
product. BLAS routines can be understood as basic building blocks for advanced numerical algorithms.

The [Linear Algebra PACKage](https://www.netlib.org/lapack/) (LAPACK) provides more
sophisticated numerical algorithms, such as solving linear systems of equations, matrix
factorization, and eigenvalue problems.

<!--With [libFlame](#amd-optimizing-cpu-libraries-aocl) and [MKL](#math-kernel-library-mkl) there are-->
<!--two highly optimised LAPACK implementations aiming for AMD and Intel architecture, respectively.-->

The [Scalable Linear Algebra PACKage](https://www.netlib.org/scalapack) (ScaLAPACK) takes the
idea of high-performance linear algebra routines to parallel distributed memory machines. It offers
functionality to solve dense and banded linear systems, least squares problems, eigenvalue
problems, and singular value problems.

<!--There is also an [optimized implementation](https://developer.amd.com/amd-aocl/scalapack/) addressing-->
<!--AMD architectures.-->

Many concrete implementations, often tuned and optimized for specific hardware architectures, have
been developed over the last decades. The two hardware vendors Intel and AMD each offer their own math
library - [Intel MKL](#math-kernel-library-mkl) and [AOCL](#amd-optimizing-cpu-libraries-aocl).
Both libraries are worth to consider from a users point of view, since they provide extensive math
functionality ranging from BLAS and LAPACK to random number generators and Fast Fourier
Transformation with consistent interfaces and the "promises" to be highly tuned and optimized and
continuously developed further.

- [BLAS reference implementation](https://www.netlib.org/blas/) in Fortran
- [LAPACK reference implementation](https://www.netlib.org/lapack/)
- [ScaLAPACK reference implementation](https://www.netlib.org/scalapack/)
- [OpenBlas](http://www.openblas.net)
- For GPU implementations, refer to the [GPU section](#libraries-for-gpus)

## AMD Optimizing CPU Libraries (AOCL)

[AMD Optimizing CPU Libraries](https://developer.amd.com/amd-aocl/) (AOCL) is a set of numerical
libraries tuned specifically for AMD EPYC processor family. AOCL offers linear algebra libraries
([BLIS](https://developer.amd.com/amd-cpu-libraries/blas-library/),
 [libFLAME](https://developer.amd.com/amd-cpu-libraries/blas-library/#libflame),
 [ScaLAPACK](https://developer.amd.com/amd-aocl/scalapack/),
 [AOCL-Sparse](https://developer.amd.com/amd-aocl/aocl-sparse/),
 [FFTW routines](https://developer.amd.com/amd-aocl/fftw/),
 [AMD Math Library (LibM)](https://developer.amd.com/amd-cpu-libraries/amd-math-library-libm/),
 as well as
 [AMD Random Number Generator Library](https://developer.amd.com/amd-cpu-libraries/rng-library/)
 and
 [AMD Secure RNG Library](https://developer.amd.com/amd-cpu-libraries/rng-library/#securerng).

## Math Kernel Library (MKL)

The
[Intel Math Kernel Library](https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-mkl-for-dpcpp/top.html)
(Intel MKL) provides extensively threaded math routines which are highly optimized for Intel CPUs.
It contains routines for linear algebra, direct and iterative sparse solvers, random number
generators and Fast Fourier Transformation (FFT).

!!! note

    MKL comes in an OpenMP-parallel version. If you want to use it, make sure you know how
    to place your jobs. [^1]
    [^1]: In \[c't 18, 2010\], Andreas Stiller proposes the usage of
    `GOMP_CPU_AFFINITY` to allow the mapping of AMD cores. KMP_AFFINITY works only for Intel processors.

The available MKL modules can be queried as follows

```console
marie@login$ module avail imkl
```

### Linking

For linker flag combinations, we highly recommend the
[MKL Link Line Advisor](http://software.intel.com/en-us/articles/intel-mkl-link-line-advisor/)
(please make sure that JavaScript is enabled for this page).

## Libraries for GPUs

GPU implementations of math functions and routines are often much faster compared to CPU
implementations. This also holds for basic routines from BLAS and LAPACK. You should consider using
GPU implementations in order to obtain better performance.

There are several math libraries for Nvidia GPUs, e.g.

- [cuBLAS](https://docs.nvidia.com/cuda/cublas/index.html)
- [cuSOLVER](https://developer.nvidia.com/cusolver) (reduced set of LAPACK routines)
- [cuSPARSE](https://developer.nvidia.com/cusparse) (sparse matrix library)
- [cuFFT](https://developer.nvidia.com/cufft)

Nvidia provides a
[comprehensive overview and starting point](https://developer.nvidia.com/gpu-accelerated-libraries#linear-algebra).

### MAGMA

The project [Matrix Algebra on GPU and Multicore Architectures](http://icl.cs.utk.edu/magma/) (MAGMA)
aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid
architectures, starting with current "Multicore+GPU" systems. `MAGMA` is available at ZIH systems in
different versions. You can list the available modules using

```console
marie@login$ module spider magma
[...]
        magma/2.5.4-fosscuda-2019b
        magma/2.5.4
```

## FFTW

FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) in one or more
dimensions, of arbitrary input size, and of both real and complex data (as well as of even/odd data,
i.e. the discrete cosine/sine transforms or DCT/DST). Before using this library, please check out
the functions of vendor-specific libraries such as [AOCL](#amd-optimizing-cpu-libraries-aocl)
or [Intel MKL](#math-kernel-library-mkl)


# Environment Modules

Usage of software on HPC systems is managed by a **modules system**.

!!! note "Module"

    A module is a user interface that provides utilities for the dynamic modification of a user's
    environment, e.g. prepending paths to

    * `PATH`
    * `LD_LIBRARY_PATH`
    * `MANPATH`
    * and more

    to help you to access compilers, loader, libraries and utilities.

    By using modules, you can smoothly switch between different versions of
    installed software packages and libraries.

## Module Commands

Using modules is quite straightforward and the following table lists the basic commands.

| Command                       | Description                                                      |
|:------------------------------|:-----------------------------------------------------------------|
| `module help`                 | Show all module options                                          |
| `module list`                 | List active modules in the user environment                      |
| `module purge`                | Remove modules from the user environment                         |
| `module avail [modname]`      | List all available modules                                       |
| `module spider [modname]`     | Search for modules across all environments                       |
| `module load <modname>`       | Load module `modname` in the user environment                    |
| `module unload <modname>`     | Remove module `modname` from the user environment                |
| `module switch <mod1> <mod2>` | Replace module `mod1` with module `mod2`                         |
| `module show <modname>`       | Show the commands in the module file                             |

Module files are ordered by their topic on ZIH systems. By default, with `module avail` you will
see all topics and their available module files. If you just wish to see the installed versions of a
certain module, you can use `module avail softwarename` and it will display the available versions of
`softwarename` only.

### Examples

???+ example "Searching for software"

    The process of searching for a particular software you want to use on an HPC system consits of
    two steps: Login to the target HPC system and invoke `module spider` command to search for the
    software and list available versions.

    For example, if you want to search for available MATLAB versions on `Barnard`, the steps might
    be:

    ```console
    marie@login.barnard$ module spider matlab

    ---------------------------------------------------------------------------------------------------------------------------------------------------------
      MATLAB: MATLAB/2022b
    ---------------------------------------------------------------------------------------------------------------------------------------------------------
    Description:
      MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with
      traditional programming languages such as C, C++, and Fortran.


    You will need to load all module(s) on any one of the lines below before the "MATLAB/2022b" module is available to load.

      release/23.04
      release/23.10
      [...]
    ```

    As you can see, `MATLAB` in version `2022b` is available on Barnard within the releases `23.04`
    and`23.10`. Additionally, the output provides the information how to load it:

    ```console
    marie@login.barnard$ module load release/23.10 MATLAB/2022b
    Module MATLAB/2022b and 1 dependency loaded.
    ```

???+ example "Finding available software"

    This examples illustrates the usage of the command `module avail` to search for available MATLAB
    installations.

    ```console
    marie@compute$ module avail matlab

    ------------------------------ /sw/modules/scs5/math ------------------------------
       MATLAB/2017a    MATLAB/2018b    MATLAB/2020a
       MATLAB/2018a    MATLAB/2019b    MATLAB/2021a (D)

      Wo:
       D:  Standard Modul.

    Verwenden Sie "module spider" um alle verfügbaren Module anzuzeigen.
    Verwenden Sie "module keyword key1 key2 ...", um alle verfügbaren Module
    anzuzeigen, die mindestens eines der Schlüsselworte enthält.
    ```

???+ example "Loading and removing modules"

    A particular module or several modules are loaded into your environment using the `module load`
    command. The counter part to remove a module or several modules is `module unload`.

    ```console
    marie@compute$ module load Python/3.8.6
    Module Python/3.8.6-GCCcore-10.2.0 and 11 dependencies loaded.
    ```

???+ example "Removing all modules"

    To remove all loaded modules from your environment with one keystroke, invoke

    ```console
    marie@compute$ module purge
    Die folgenden Module wurden nicht entladen:
      (Benutzen Sie "module --force purge" um alle Module zu entladen):

      1) release/23.04
    Module Python/3.8.6-GCCcore-10.2.0 and 11 dependencies unloaded.
    ```

???+ example "Show the command in module file"

    The option `show <modname>` will output the commands in the module file. Using this command,
    you can find out what paths are prepended and what environment variables are set.

    ```console
    marie@login$ module show GCCcore
    ---------------------------------------------------------------------------------------------------------------------------------------------------------
   /sw/modules/scs5/compiler/GCCcore/11.2.0.lua:
    ---------------------------------------------------------------------------------------------------------------------------------------------------------
    help([[
    Description
    ===========
    The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada,
    as well as libraries for these languages (libstdc++, libgcj,...).

    [...]
    conflict("GCCcore")
    prepend_path("CMAKE_LIBRARY_PATH","/sw/installed/GCCcore/11.2.0/lib64")
    prepend_path("CMAKE_PREFIX_PATH","/sw/installed/GCCcore/11.2.0")
    prepend_path("LD_LIBRARY_PATH","/sw/installed/GCCcore/11.2.0/lib64")
    prepend_path("MANPATH","/sw/installed/GCCcore/11.2.0/share/man")
    prepend_path("PATH","/sw/installed/GCCcore/11.2.0/bin")
    prepend_path("XDG_DATA_DIRS","/sw/installed/GCCcore/11.2.0/share")
    setenv("EBROOTGCCCORE","/sw/installed/GCCcore/11.2.0")
    setenv("EBVERSIONGCCCORE","11.2.0")
    setenv("EBDEVELGCCCORE","/sw/installed/GCCcore/11.2.0/easybuild/GCCcore-11.2.0-easybuild-devel")
    ```

### Front-End ml

There is a front end for the module command, which helps you to type less. It is `ml`.
 Any module command can be given after `ml`:

| ml Command        | module Command                            |
|:------------------|:------------------------------------------|
| `ml`              | `module list`                             |
| `ml foo bar`      | `module load foo bar`                     |
| `ml -foo -bar baz`| `module unload foo bar; module load baz`  |
| `ml purge`        | `module purge`                            |
| `ml show foo`     | `module show foo`                         |

???+ example "Usage of front-end ml"

    ```console
    marie@compute$ ml +Python/3.8.6
    Module Python/3.8.6-GCCcore-10.2.0 and 11 dependencies loaded.
    marie@compute$ ml

    Derzeit geladene Module:
      1) release/23.04                (S)   5) bzip2/1.0.8-GCCcore-10.2.0       9) SQLite/3.33.0-GCCcore-10.2.0  13) Python/3.8.6-GCCcore-10.2.0
      2) GCCcore/10.2.0                     6) ncurses/6.2-GCCcore-10.2.0      10) XZ/5.2.5-GCCcore-10.2.0
      3) zlib/1.2.11-GCCcore-10.2.0         7) libreadline/8.0-GCCcore-10.2.0  11) GMP/6.2.0-GCCcore-10.2.0
      4) binutils/2.35-GCCcore-10.2.0       8) Tcl/8.6.10-GCCcore-10.2.0       12) libffi/3.3-GCCcore-10.2.0

      Wo:
       S:  Das Modul ist angeheftet. Verwenden Sie "--force", um das Modul zu entladen.

    marie@compute$ ml -Python/3.8.6 +ANSYS/2020R2
    Module Python/3.8.6-GCCcore-10.2.0 and 11 dependencies unloaded.
    Module ANSYS/2020R2 loaded.
    ```

## Module Environments

On ZIH systems, there exist different **module environments**, each containing a set of software
modules.
They are activated via the meta module `release` which has different versions,
one of which is loaded by default.
You can switch between them by simply loading the desired version, e.g.

```console
marie@compute$ module load release/23.10
```

### Searching for Software

The command `module spider <modname>` allows searching for a specific software across all module
environments.
It will also display information on how to load a particular module when giving a
precise module (with version) as the parameter.

??? example "Spider command"

    ```console
    marie@login$ module spider p7zip

    ----------------------------------------------------------------------------------------------------------------------------------------------------------
      p7zip:
    ----------------------------------------------------------------------------------------------------------------------------------------------------------
        Beschreibung:
          p7zip is a quick port of 7z.exe and 7za.exe (command line version of 7zip) for Unix. 7-Zip is a file archiver with highest compression ratio.

         Versionen:
            p7zip/9.38.1
            p7zip/17.03-GCCcore-10.2.0
            p7zip/17.03

    ----------------------------------------------------------------------------------------------------------------------------------------------------------
      Um detaillierte Informationen über ein bestimmtes "p7zip"-Modul zu erhalten (auch wie das Modul zu laden ist), verwenden sie den vollständigen Namen des Moduls.
      Zum Beispiel:
        $ module spider p7zip/17.03
    ----------------------------------------------------------------------------------------------------------------------------------------------------------
    ```

In some cases a desired software is available as an extension of a module.

??? example "Extension module"
    ```console  hl_lines="9"
    marie@login$ module spider tensorboard

    --------------------------------------------------------------------------------------------------------------------------------
    tensorboard:
    --------------------------------------------------------------------------------------------------------------------------------
    Versions:
        tensorboard/2.4.1 (E)

    Names marked by a trailing (E) are extensions provided by another module.
    [...]
    ```

    You retrieve further information using the `spider` command.

    ```console
    marie@login$  module spider tensorboard/2.4.1

    --------------------------------------------------------------------------------------------------------------------------------
    tensorboard: tensorboard/2.4.1 (E)
    --------------------------------------------------------------------------------------------------------------------------------
    This extension is provided by the following modules. To access the extension you must load one of the following modules. Note that any module names in parentheses show the module location in the software hierarchy.

        TensorFlow/2.4.1 (release/23.04 GCC/10.2.0 CUDA/11.1.1 OpenMPI/4.0.5)

    Names marked by a trailing (E) are extensions provided by another module.
    ```

    Finaly, you can load the dependencies and `tensorboard/2.4.1` and check the version.

    ```console
    marie@login$ module load release/23.04  GCC/11.3.0  OpenMPI/4.1.4

    Modules GCC/10.2.0, CUDA/11.1.1, OpenMPI/4.0.5 and 15 dependencies loaded.
    marie@login$ module load TensorFlow/2.11.0-CUDA-11.7.0

    Aktiviere Module:
      1) CUDA/11.7.0     2) GDRCopy/2.3

    Module TensorFlow/2.11.0-CUDA-11.7.0 and 39 dependencies loaded.

    marie@login$ tensorboard --version
    2.11.1
    ```

## Toolchains

A program or library may break in various ways (e.g. not starting, crashing or producing wrong
results) when it is used with a software of a different version than it expects.So each module
specifies the exact other modules it depends on. They get loaded automatically when the dependent
module is loaded.

Loading a single module is easy as there can't be any conflicts between dependencies. However when
loading multiple modules they can require different versions of the same software. This conflict is
currently handled in that loading the same software with a different version automatically unloads
the earlier loaded module.  As the dependents of that module are **not** automatically unloaded this
means they now have a wrong dependency (version) which can be a problem (see above).

To avoid this there are (versioned) toolchains and for each toolchain there is (usually) at most
one version of each software.
A "toolchain" is a set of modules used to build the software for other modules.
The most common one is the `foss`-toolchain consisting of `GCC`, `OpenMPI`, `OpenBLAS` & `FFTW`.

This toolchain can be broken down into a sub-toolchain called `gompi` consisting of only
`GCC` & `OpenMPI`, or further to `GCC` (the compiler and linker)
and even further to `GCCcore` which is only the runtime libraries required to run programs built
with the GCC standard library.

!!! hint

    As toolchains are regular modules you can display their parts via `module show foss/2019a`.

This way the toolchains form a hierarchy and adding more modules makes them "higher" than another.

Examples:

| Toolchain | Components |
| --------- | ---------- |
| `foss`    | `GCC` `OpenMPI` `OpenBLAS` `FFTW` |
| `gompi`   | `GCC` `OpenMPI` |
| `GCC`     | `GCCcore` `binutils` |
| `GCCcore` | none |
| `intel`   | `intel-compilers` `impi` `imkl` |
| `iimpi`   | `intel-compilers` `impi` |
| `intel-compilers` | `GCCcore` `binutils` |

As you can see `GCC` and `intel-compilers` are on the same level, as are `gompi` and `iimpi`,
although they are one level higher than the former.

You can load and use modules from a lower toolchain with modules from
one of its parent toolchains.
For example `Python/3.6.6-foss-2019a` can be used with `Boost/1.70.0-gompi-2019a`.

But you cannot combine toolchains or toolchain versions.
So `QuantumESPRESSO/6.5-intel-2019a` and `OpenFOAM/8-foss-2020a`
are both incompatible with `Python/3.6.6-foss-2019a`.
However `LLVM/7.0.1-GCCcore-8.2.0` can be used with either
`QuantumESPRESSO/6.5-intel-2019a` or `Python/3.6.6-foss-2019a`
because `GCCcore-8.2.0` is a sub-toolchain of `intel-2019a` and `foss-2019a`.

With the hierarchical module scheme we use at ZIH modules from other toolchains cannot be directly
loaded and don't show up in `module av` which avoids loading incompatible modules.
So the concept if this hierarchical toolchains is already built into this module environment.

!!! info

    The toolchains usually have a year and letter as their version corresponding to their release.
    So `2019a` and `2020b` refer to the first half of 2019 and the 2nd half of 2020 respectively.

## Per-Architecture Builds

Since we have a heterogeneous cluster, we do individual builds of the software for each
architecture present.
This ensures that, no matter what partition/cluster the software runs on, a build
optimized for the host architecture is used automatically.

However, not every module will be available on all clusters.
Use `ml av` or `ml spider` to search for modules available on the sub-cluster you are on.

## Advanced Usage

For writing your own module files please have a look at the
[Guide for writing project and private module files](private_modules.md).

## Troubleshooting

### When I log in, the wrong modules are loaded by default

Reset your currently loaded modules with `module purge`.
Then run `module save` to overwrite the
list of modules you load by default when logging in.

### I can't load module TensorFlow

Check the dependencies by e.g. calling `module spider TensorFlow/2.4.1`
it will list a number of modules that need to be loaded
before the TensorFlow module can be loaded.

??? example "Loading the dependencies"

    ```console
    marie@compute$ module load TensorFlow/2.4.1
    Lmod hat den folgenden Fehler erkannt:  Diese Module existieren, aber
    können nicht wie gewünscht geladen werden: "TensorFlow/2.4.1"
       Versuchen Sie: "module spider TensorFlow/2.4.1" um anzuzeigen, wie die Module
    geladen werden.


    marie@compute$ module spider TensorFlow/2.4.1

    ----------------------------------------------------------------------------------
      TensorFlow: TensorFlow/2.4.1
    ----------------------------------------------------------------------------------
        Beschreibung:
          An open-source software library for Machine Intelligence


        Sie müssen alle Module in einer der nachfolgenden Zeilen laden bevor Sie das Modul "TensorFlow/2.4.1" laden können.

          release/23.04  GCC/10.2.0  CUDA/11.1.1  OpenMPI/4.0.5
         This extension is provided by the following modules. To access the extension you must load one of the following modules. Note that any module names in parentheses show the module location in the software hierarchy.


           TensorFlow/2.4.1 (release/23.04 GCC/10.2.0 CUDA/11.1.1 OpenMPI/4.0.5)


        This module provides the following extensions:

           absl-py/0.10.0 (E), astunparse/1.6.3 (E), cachetools/4.2.0 (E), dill/0.3.3 (E), gast/0.3.3 (E), google-auth-oauthlib/0.4.2 (E), google-auth/1.24.0 (E), google-pasta/0.2.0 (E), grpcio/1.32.0 (E), gviz-api/1.9.0 (E), h5py/2.10.0 (E), Keras-Preprocessing/1.1.2 (E), Markdown/3.3.3 (E), oauthlib/3.1.0 (E), opt-einsum/3.3.0 (E), portpicker/1.3.1 (E), pyasn1-modules/0.2.8 (E), requests-oauthlib/1.3.0 (E), rsa/4.7 (E), tblib/1.7.0 (E), tensorboard-plugin-profile/2.4.0 (E), tensorboard-plugin-wit/1.8.0 (E), tensorboard/2.4.1 (E), tensorflow-estimator/2.4.0 (E), TensorFlow/2.4.1 (E), termcolor/1.1.0 (E), Werkzeug/1.0.1 (E), wrapt/1.12.1 (E)

        Help:
          Description
          ===========
          An open-source software library for Machine Intelligence


          More information
          ================
           - Homepage: https://www.tensorflow.org/


          Included extensions
          ===================
          absl-py-0.10.0, astunparse-1.6.3, cachetools-4.2.0, dill-0.3.3, gast-0.3.3,
          google-auth-1.24.0, google-auth-oauthlib-0.4.2, google-pasta-0.2.0,
          grpcio-1.32.0, gviz-api-1.9.0, h5py-2.10.0, Keras-Preprocessing-1.1.2,
          Markdown-3.3.3, oauthlib-3.1.0, opt-einsum-3.3.0, portpicker-1.3.1,
          pyasn1-modules-0.2.8, requests-oauthlib-1.3.0, rsa-4.7, tblib-1.7.0,
          tensorboard-2.4.1, tensorboard-plugin-profile-2.4.0, tensorboard-plugin-
          wit-1.8.0, TensorFlow-2.4.1, tensorflow-estimator-2.4.0, termcolor-1.1.0,
          Werkzeug-1.0.1, wrapt-1.12.1


    Names marked by a trailing (E) are extensions provided by another module.



    marie@compute$ ml +GCC/10.2.0  +CUDA/11.1.1 +OpenMPI/4.0.5 +TensorFlow/2.4.1

    Die folgenden Module wurden in einer anderen Version erneut geladen:
      1) GCC/7.3.0-2.30 => GCC/10.2.0        3) binutils/2.30-GCCcore-7.3.0 => binutils/2.35
      2) GCCcore/7.3.0 => GCCcore/10.2.0

    Module GCCcore/7.3.0, binutils/2.30-GCCcore-7.3.0, GCC/7.3.0-2.30, GCC/7.3.0-2.30 and 3 dependencies unloaded.
    Module GCCcore/7.3.0, GCC/7.3.0-2.30, GCC/10.2.0, CUDA/11.1.1, OpenMPI/4.0.5, TensorFlow/2.4.1 and 50 dependencies loaded.
    marie@compute$ module list

    Derzeit geladene Module:
      1) release/23.04              (S)  28) Tcl/8.6.10
      2) GCCcore/10.2.0                  29) SQLite/3.33.0
      3) zlib/1.2.11                     30) GMP/6.2.0
      4) binutils/2.35                   31) libffi/3.3
      5) GCC/10.2.0                      32) Python/3.8.6
      6) CUDAcore/11.1.1                 33) pybind11/2.6.0
      7) CUDA/11.1.1                     34) SciPy-bundle/2020.11
      8) numactl/2.0.13                  35) Szip/2.1.1
      9) XZ/5.2.5                        36) HDF5/1.10.7
     10) libxml2/2.9.10                  37) cURL/7.72.0
     11) libpciaccess/0.16               38) double-conversion/3.1.5
     12) hwloc/2.2.0                     39) flatbuffers/1.12.0
     13) libevent/2.1.12                 40) giflib/5.2.1
     14) Check/0.15.2                    41) ICU/67.1
     15) GDRCopy/2.1-CUDA-11.1.1         42) JsonCpp/1.9.4
     16) UCX/1.9.0-CUDA-11.1.1           43) NASM/2.15.05
     17) libfabric/1.11.0                44) libjpeg-turbo/2.0.5
     18) PMIx/3.1.5                      45) LMDB/0.9.24
     19) OpenMPI/4.0.5                   46) nsync/1.24.0
     20) OpenBLAS/0.3.12                 47) PCRE/8.44
     21) FFTW/3.3.8                      48) protobuf/3.14.0
     22) ScaLAPACK/2.1.0                 49) protobuf-python/3.14.0
     23) cuDNN/8.0.4.30-CUDA-11.1.1      50) flatbuffers-python/1.12
     24) NCCL/2.8.3-CUDA-11.1.1          51) typing-extensions/3.7.4.3
     25) bzip2/1.0.8                     52) libpng/1.6.37
     26) ncurses/6.2                     53) snappy/1.1.8
     27) libreadline/8.0                 54) TensorFlow/2.4.1

      Wo:
       S:  Das Modul ist angeheftet. Verwenden Sie "--force", um das Modul zu entladen.
    ```


# Check MPI Correctness with MUST

MPI as the de-facto standard for parallel applications of the message passing paradigm offers
more than one hundred different API calls with complex restrictions. As a result, developing
applications with this interface is error prone and often time consuming. Some usage errors of MPI
may only manifest on some platforms or some application runs, which further complicates the
detection of these errors. Thus, special debugging tools for MPI applications exist that
automatically check whether an application conforms to the MPI standard and whether its MPI calls
are safe. At ZIH, we maintain and support **MUST** for this task, though different types of these
tools exist (see last section).

## MUST

[MUST](https://itc.rwth-aachen.de/must/) checks if your application conforms to the MPI
standard and will issue warnings if there are errors or non-portable constructs. You can apply MUST
without modifying your source code, though we suggest to add the debugging flag `-g` during
compilation.

See also [MUST Introduction Slides](misc/parallel_debugging_must.pdf) for a starting point.

### Setup and Modules

You need to load a module file in order to use MUST. Each MUST installation uses a specific
combination of a compiler and an MPI library, make sure to use a combination that fits your needs.
Right now we provide two combinations, [contact us](../support/support.md) if you need further
combinations. You can query for the available modules with:

```console
marie@login$ module avail must
   MUST/1.6.0-rc3-intel-2018a    MUST/1.7.2-intel-2020a (D)
```

You can load a MUST module as follows:

```console
marie@login$ module load MUST
Module MUST/1.7.2-intel-2020a and 16 dependencies loaded.
```

Besides loading a MUST module, no further changes are needed during compilation and linking.

### Running your Application with MUST

In order to launch your application with MUST you need to replace the `srun` command with
`mustrun --must:mpiexec srun --must:np --ntasks`:

```console
marie@login$ mustrun --must:mpiexec srun --must:np --ntasks --ntasks <number of MPI processes> ./<your binary>
```

Besides replacing the `srun` command you need to be aware that **MUST always allocates an extra
process**, i.e. if you issue a
`mustrun --must:mpiexec srun --must:np --ntasks --ntasks 4 ./<your binary>` then
MUST will start **5 processes** instead. This is usually not critical. However, in interactive and
batch jobs **make sure to allocate an extra CPU for this task**.

Suppose your application is called `fancy-program` and is normally run with 4 processes.
The MUST workflow should then be

```console
marie@login$ module load MUST

# Compile your application with the debugging flag "-g" on the correct architecture, e.g.:
marie@login$ srun --ntasks 1 --partition <partition> mpicc -g -o fancy-program fancy-program.c

# Allocate interactive session with 1 extra process for MUST
marie@login$ salloc --ntasks 5 --partition <partition>

marie@login$ mustrun --must:mpiexec srun --must:np --ntasks --must:stacktrace backward --ntasks 4 ./fancy-program
[MUST] MUST configuration ... centralized checks with fall-back application crash handling (very slow)
[MUST] Weaver ... success
[MUST] Code generation ... success
[MUST] Build file generation ... success
[MUST] Configuring intermediate build ... success
[MUST] Building intermediate sources ... success
[MUST] Installing intermediate modules ... success
[MUST] Generating P^nMPI configuration ... success
[MUST] Search for linked P^nMPI ... not found ... using LD_PRELOAD to load P^nMPI ... success
[MUST] Executing application:
{...}
[MUST] Execution finished, inspect "/home/marie/MUST_Output.html"!
```

??? hint "Twice `--ntasks`"

    You might wonder about the two `--ntasks` arguments in the above outlined `mustrun` comannd.
    Mustrun is able to use invoke another command instead of mpiexec. For ZIH systems, this will be
    `srun` (`--must-mpiexec: srun`). Now, you need to specify what argument of the MPI run arguments
    holds the number of application processes. For Slurm, it is `--ntasks <N>`. Thus, you need to
    specify `--must:np --ntasks --ntasks <N>`.

With the additional flag `--must:stacktrace backward` you can produce an additional stacktrace
with line number of the error location which allows to pinpoint the error location in your code.
This might slow down code execution slightly.

Finally, MUST assumes that your application may crash at any time. To still gather correctness
results under this assumption is extremely expensive in terms of performance overheads. Thus, if
your application does not crash, you should add `--must:nocrash` to the `mustrun` command to make
MUST aware of this knowledge. Overhead is drastically reduced with this switch.
Further details on alternative launch modes are described in the MUST documentation.

### Result Files

After running your application with MUST you will have its output in the working directory of your
application. The output is named `MUST_Output.html`. Open this files in a browser to analyze the
results. The HTML file is color coded:

- Entries in green represent notes and useful information
- Entries in yellow represent warnings
- Entries in red represent errors

### Example Usage of MUST

In this section, we provide a detailed example explaining the usage of MUST. The example is taken
from the [MUST documentation v1.7.2](https://hpc.rwth-aachen.de/must/files/Documentation-1.7.2.pdf).

??? example "example.c"

    This C programm contains three MPI usage errors. Save it as `example.c`.

    ```
    #include <stdio.h>
    #include <mpi.h>

    int main (int argc , char ** argv) {
      int rank ,
          size ,
          sBuf [ 2 ] = { 1 , 2 } ,
          rBuf [ 2 ] ;
      MPI_Status status ;
      MPI_Datatype newType ;

      MPI_Init(&argc ,&argv ) ;
      MPI_Comm_rank (MPI_COMM_WORLD, &rank ) ;
      MPI_Comm_size (MPI_COMM_WORLD, &size ) ;

       // Enough tasks?
      if ( size < 2 ) {
        printf("This test needs at least 2 processes ! \n");
        MPI_Finalize();
        return 1 ;
      }

      // Say hello
      printf("Hello, I am rank %d of %d processes. \n", rank , size);

      //) Create a datatype
      MPI_Type_contiguous( 2, MPI_INT, &newType);
      MPI_Type_commit(&newType);

      // 2) Use MPI Sendrecv to perform a ring communication
      MPI_Sendrecv(sBuf, 1, newType, (rank+1)%size, 123,
                   rBuf, sizeof(int)*2, MPI_BYTE, (rank=1+size) %size, 123 , MPI_COMM_WORLD, &status ) ;

      // 3) Use MPI Send and MPI Recv to perform a ring communication
      MPI_Send(sBuf, 1, newType, (rank+1)%size, 456, MPI_COMM_WORLD);
      MPI_Recv(rBuf, sizeof(int)*2, MPI_BYTE, (rank=1+size)%size, 456, MPI_COMM_WORLD, &status);

      // Say bye bye
      printf("Signing off, rank %d. \n" , rank);

      MPI_Finalize();
      return 0 ;
    }
    /*EOF*/
    ```

??? example "Compile and execute"

    The first step is to prepare the environment by loading a MUST module.

    ```console
    marie@login$ module purge
    marie@login$ module load MUST
    Module MUST/1.7.2-intel-2020a and 16 dependencies loaded.
    ```

    Now, you compile the `example.c` program using the MPI compiler wrapper. The compiled binary is
    called `example`.

    ```console
    marie@login$ mpicc example.c -g -o example
    ```

    Finally, you execute the example application on the compute nodes. As you can see, the following
    command line will submit a job to the batch system.

    ```
    marie@login $ mustrun --must:mpiexec srun --must:np --ntasks --ntasks 4 --time 00:10:00 example
    [MUST] MUST configuration ... centralized checks with fall-back application crash handling (very slow)
    [MUST] Information: overwritting old intermediate data in directory "/scratch/ws/0/marie-must/must_temp"!
    [MUST] Using prebuilt infrastructure at /sw/installed/MUST/1.7.2-intel-2020a/modules/mode1-layer2
    [MUST] Weaver ... success
    [MUST] Generating P^nMPI configuration ... success
    [MUST] Search for linked P^nMPI ... not found ... using LD_PRELOAD to load P^nMPI ... success
    [MUST] Executing application:
    srun: job 32765491 queued and waiting for resources
    srun: job 32778008 has been allocated resources
    Hello , I am rank 2 of 4 processes.
    Hello , I am rank 3 of 4 processes.
    Hello , I am rank 0 of 4 processes.
    Hello , I am rank 1 of 4 processes.
    ============MUST===============
    ERROR: MUST detected a deadlock, detailed information is available in the MUST output file. You should either investigate details with a debugger or abort, the operation of MUST will stop from now.
    ===============================
    ```

??? example "Analysis of MUST output files and MPI usage errors"

    MUST produces an `MUST_Output.html` file and a directory `MUST_Output-files` with additional
    html files. Copy the files to your local host, e.g.

    ```console
    marie@local$ scp -r dataport1.hpc.tu-dresden.de:/data/horse/ws/marie-must/{MUST_Output-files,MUST_Output.html}
    ```

    and open the file `MUST_Output.html` using a webbrowser. Alternativly, you can open the html file with a
    `firefox` instance on the HPC sytems. This requires to [forward the X11 support via SSH](../access/ssh_login.md#x11-forwarding).

    MUST detects all three MPI usage errors within this example:

    * A type mismatch
    * A send-send deadlock
    * A leaked datatype

    The type mismatch is reported as follows:

    ![MUST error](misc/must-error-01.png)
    {: align="center" summary="Type mismatch error report from MUST."}

    MUST also offers a detailed page for the type mismatch error.

    ![MUST error](misc/must-error-02.png)
    {: summary="Retrieve job results via GUI using the Job Monitor." align="center"}

    In order not to exceed the scope of this example, we do not explain the MPI usage errors in more
    details. Please, feel free to deep-dive into the error description provided in the official
    [MUST documentation v1.7.2](https://hpc.rwth-aachen.de/must/files/Documentation-1.7.2.pdf) (Sec.
    4).

## Further MPI Correctness Tools

Besides MUST, there exist further MPI correctness tools, these are:

- Marmot (predecessor of MUST)
- MPI checking library of the Intel Trace Collector
- ISP (From Utah)
- Umpire (predecessor of MUST)

ISP provides a more thorough deadlock detection as it investigates alternative execution paths,
however its overhead is drastically higher as a result. Contact our support if you have a specific
use cases that needs one of these tools.


# Nanoscale Simulations

## ABINIT

[ABINIT](http://www.abinit.org) is a package whose main program allows one to find the total energy,
charge density and electronic structure of systems made of electrons and nuclei (molecules and
periodic solids) within Density Functional Theory (DFT), using pseudopotentials and a planewave
basis. ABINIT also includes options to optimize the geometry according to the DFT forces and
stresses, or to perform molecular dynamics simulations using these forces, or to generate dynamical
matrices, Born effective charges, and dielectric tensors. Excited states can be computed within the
Time-Dependent Density Functional Theory (for molecules), or within Many-Body Perturbation Theory
(the GW approximation).

ABINIT is available as [modules](modules.md). Installed versions can be listed and loaded with the
following commands

```console
marie@login$ module avail ABINIT
---------------------------- /sw/modules/scs5/chem -----------------------------
   ABINIT/8.6.3-intel-2018a         Wannier90/2.0.1.1-foss-2018b-abinit
   ABINIT/8.10.3-intel-2018b        Wannier90/2.0.1.1-intel-2018b-abinit
   ABINIT/9.2.1-intel-2020a  (D)
[...]
marie@login$ module load ABINIT
Module ABINIT/9.2.1-intel-2020a and 16 dependencies loaded.
```

## CP2K

[CP2K](http://cp2k.berlios.de/) performs atomistic and molecular simulations of solid state, liquid,
molecular and biological systems. It provides a general framework for different methods such as e.g.
density functional theory (DFT) using a mixed Gaussian and plane waves approach (GPW), and classical
pair and many-body potentials.

CP2K is available as [modules](modules.md). Available packages can be listed and loaded with the
following commands:

```console
marie@login$ module avail CP2K
---------------------------- /sw/modules/scs5/chem -----------------------------
   CP2K/5.1-intel-2018a          CP2K/6.1-intel-2018a-spglib
   CP2K/6.1-foss-2019a-spglib    CP2K/6.1-intel-2018a        (D)
   CP2K/6.1-foss-2019a
[...]
marie@login$ module load CP2K
Module CP2K/6.1-intel-2018a and 25 dependencies loaded.
```

## CPMD

The CPMD code is a plane wave/pseudopotential implementation of Density Functional Theory,
particularly designed for ab-initio molecular dynamics. For examples and documentations, see
[CPMD homepage](https://www.lcrc.anl.gov/for-users/software/available-software/cpmd/).

CPMD is currently not installed as a module.
Please, contact [hpc-support@tu-dresden.de](mailto:hpc-support@tu-dresden.de) if you need assistance.

## GAMESS

GAMESS is an ab-initio quantum mechanics program, which provides many methods for computation of the
properties of molecular systems using standard quantum chemical methods. For a detailed description,
please look at the [GAMESS home page](https://www.msg.chem.iastate.edu/gamess/index.html).

GAMESS is available as [modules](modules.md) within the classic environment. Available packages can
be listed and loaded with the following commands:

```console
marie@login$:~> module avail gamess
----------------------- /sw/modules/taurus/applications ------------------------
   gamess/2013
[...]
marie@login$ module load gamess
Start gamess like this:
 rungms.slurm <inputfile> [scratch_path]
Module gamess/2013 and 2 dependencies loaded.
```

For runs with [Slurm](../jobs_and_resources/slurm.md), please use a script like this:

```Bash
#!/bin/bash
#SBATCH --time=120
#SBATCH --ntasks=8
#SBATCH --ntasks-per-node=2
## you have to make sure that an even number of tasks runs on each node !!
#SBATCH --mem-per-cpu=1900

module load gamess
rungms.slurm cTT_M_025.inp /data/horse/ws/marie-gamess
#                          the third parameter is the location of your horse directory
```

*GAMESS should be cited as:* "General Atomic and Molecular Electronic Structure System",
M.W.Schmidt, K.K.Baldridge, J.A.Boatz, S.T.Elbert, M.S.Gordon, J.H.Jensen, S.Koseki, N.Matsunaga,
K.A.Nguyen, S.J.Su, T.L.Windus, M.Dupuis, J.A.Montgomery, J.Comput.Chem. 14, 1347-1363(1993).

## Gaussian

Starting from the basic laws of quantum mechanics, [Gaussian](http://www.gaussian.com) predicts the
energies, molecular structures, and vibrational frequencies of molecular systems, along with
numerous molecular properties derived from these basic computation types. It can be used to study
molecules and reactions under a wide range of conditions, including both stable species and
compounds which are difficult or impossible to observe experimentally such as short-lived
intermediates and transition structures.

Access to the Gaussian installation on our system is limited to members
of the  UNIX group `s_gaussian`. Please, contact
[hpc-support@tu-dresden.de](mailto:hpc-support@tu-dresden.de) if you can't
access it, yet wish to use it.

### Guidance on Data Management with Gaussian

We have a general description about
[how to utilize workspaces for your I/O intensive jobs](../data_lifecycle/workspaces.md).
However hereafter we have an example on how that might look like for Gaussian:

???+ example "Using workspaces with Gaussian"

```
#!/bin/bash
#SBATCH --time=04:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --mem=68000
## good default on Barnard: set mem = 4000 (for gaussian executable) + cpus-per-task * 4000 (for gaussian data)

# Adjust the path to where your input file is located
INPUTFILE="/path/to/my/inputfile.gjf"

# Unexpected errors result in instant script termination
set -e

# Load the software you need here
module purge
module load release/23.10
module load Gaussian/16.C.01

# Check existance of input file
if test ! -f "${INPUTFILE}"; then
   echo "Error: could not find the input file ${INPUTFILE}"
   exit 1
fi

# Allocate workspace. Adjust time span to time limit of the job (-d <N>).
COMPUTE_WS=gaussian_${SLURM_JOB_ID}
export GAUSS_SCRDIR=$(ws_allocate --name ${COMPUTE_WS} --duration 7)
echo ${GAUSS_SCRDIR}

# Check if workspace allocation was successful
if [ ! -d "${GAUSS_SCRDIR}" ]; then
   echo "Error: cannot allocate workspace ${COMPUTE_WS}"
   exit 1
fi

# Check consistency of resource allocation (memory & cores). The Slurm and Gaussian configuration should match.
if grep -i -m 3 -e "%mem" -e "%cpu" -e "%nprocshared" "${INPUTFILE}"; then
   echo "Info: are the above parameters in your input file within your Slurm job parameters SLURM_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK} and SLURM_MEM_PER_NODE=${SLURM_MEM_PER_NODE}?"
fi

# Change to workspace directory and execute application
cp "${INPUTFILE}" ${GAUSS_SCRDIR}/inputfile.gjf
cd ${GAUSS_SCRDIR}
# m -4000 ensures that gaussian binary and runtime itself fits into memory
if ! g16 -p=${SLURM_CPUS_PER_TASK} -m=$(( ${SLURM_MEM_PER_NODE} - 4000 ))MB <./inputfile.gjf >logfile.log; then
   echo "Error: gaussian terminated with error code $?"
fi

# Move compressed result files to user home
if ! bzip2 --compress --stdout logfile.log >${HOME}/gaussian_job-${SLURM_JOB_ID}.bz2; then
   echo "Error: compression of results failed!"
   echo "Please check ${GAUSS_SCRDIR} for uncompressed results."
   exit 1
fi

# Clean and release temporary workspace
if test -d ${GAUSS_SCRDIR}; then
   rm -rf ${GAUSS_SCRDIR}/*
   ws_release ${COMPUTE_WS}
fi
```

## GROMACS

GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations
of motion for systems with hundreds to millions of particles. It is primarily designed for
biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded
interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that
usually dominate simulations), many groups are also using it for research on non-biological systems,
e.g., polymers. For documentations see [GROMACS homepage](https://www.gromacs.org/).

GROMACS is available as [modules](modules.md). Available packages can be listed and loaded with the
following commands:

```console
marie@login$:~> module avail GROMACS
----------------------------- /sw/modules/scs5/bio -----------------------------
   GROMACS/2018.2-foss-2018a-CUDA-9.2.88    GROMACS/2019.4-fosscuda-2019a
   GROMACS/2018.2-intel-2018a               GROMACS/2020-fosscuda-2019b   (D)
[...]
marie@login$ module load GROMACS
Module GROMACS/2020-fosscuda-2019b and 17 dependencies loaded.
```

## LAMMPS

[LAMMPS](https://www.lammps.org) is a classical molecular dynamics code that models an ensemble of
particles in a liquid, solid, or gaseous state. It can model atomic, polymeric, biological,
metallic, granular, and coarse-grained systems using a variety of force fields and boundary
conditions. For examples of LAMMPS simulations, documentations, and more visit
[LAMMPS sites](https://www.lammps.org).

LAMMPS is available as [modules](modules.md). Available packages can be listed and loaded with the
following commands:

```console
marie@login$:~> module avail LAMMPS
---------------------------- /sw/modules/scs5/chem -----------------------------
   LAMMPS/3Mar2020-foss-2020a-Python-3.8.2-kokkos
   LAMMPS/3Mar2020-intel-2020a-Python-3.8.2-kokkos
   LAMMPS/7Aug19-foss-2019a-Python-2.7.15
   LAMMPS/12Dec2018-foss-2019a                     (D)
   LAMMPS/20180316-foss-2018a-Python-3.6.4
[...]
marie@login$ module load LAMMPS
[...]
Module LAMMPS/12Dec2018-foss-2019a and 33 dependencies loaded.
```

## NAMD

[NAMD](https://www.ks.uiuc.edu/Research/namd) is a parallel molecular dynamics code designed for
high-performance simulation of large biomolecular systems.

NAMD can be started as parallel program with `srun`. Since
the parallel performance strongly depends on the size of the given problem, one cannot give a general
advice for the optimum number of CPUs to use. (Please check this by running NAMD with your molecules
and just a few time steps.)

NAND is available as [modules](modules.md). Available packages can be listed and loaded with the
following commands:

```console
marie@login$:~> module avail NAMD
---------------------------- /sw/modules/scs5/chem -----------------------------
   NAMD/2.12-intel-2018a-mpi
[...]
marie@login$ module load NAMD
[...]
Module NAMD/2.12-intel-2018a-mpi and 12 dependencies loaded.
```

Any published work which utilizes NAMD shall include the following reference:

*James C. Phillips, Rosemary Braun, Wei Wang, James Gumbart, Emad Tajkhorshid, Elizabeth Villa, Christophe
Chipot, Robert D.  Skeel, Laxmikant Kale, and Klaus Schulten. Scalable molecular dynamics with NAMD.
Journal of Computational Chemistry, 26:1781-1802, 2005.*

Electronic documents will include a direct link to the [official NAMD page](https://www.ks.uiuc.edu/Research/namd)

## ORCA

ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry with
specific emphasis on spectroscopic properties of open-shell molecules. It features a wide variety of
standard quantum chemical methods ranging from semiempirical methods to DFT to single- and
multireference correlated ab initio methods. It can also treat environmental and relativistic
effects.

To run ORCA jobs in parallel, you have to specify the number of processes in your input file (here
for example 16 processes):

```Bash
%pal nprocs 16 end
```

Note, that ORCA spawns MPI processes itself, so you must not use `srun` to launch it in your batch
file. Just set `--ntasks` to the same number as in your input file and call the `orca` executable
directly. For parallel runs, it must be called with the full path:

```Bash
#!/bin/bash
#SBATCH --ntasks=16
#SBATCH --nodes=1
#SBATCH --mem-per-cpu=2000M

$ORCA_ROOT/orca example.inp
```

ORCA is available as [modules](modules.md). Available packages can be listed and loaded with the
following commands:

```console
marie@login$:~> module avail ORCA
---------------------------- /sw/modules/scs5/chem -----------------------------
   ORCA/4.1.1-OpenMPI-2.1.5    ORCA/4.2.1-gompi-2019b (D)
[...]
marie@login$ module load ORCA
[...]
Module ORCA/4.2.1-gompi-2019b and 11 dependencies loaded.
```

## Siesta

[Siesta](https://siesta-project.org/siesta) (Spanish Initiative for Electronic Simulations with
Thousands of Atoms) is both a method and its computer program implementation,
to perform electronic structure calculations and ab initio
molecular dynamics simulations of molecules and solids.

Siesta is available as [modules](modules.md). Available packages can be listed and loaded with the
following commands:

```console
marie@login$:~> module avail Siesta
---------------------------- /sw/modules/scs5/phys -----------------------------
   Siesta/4.1-b3-intel-2018a    Siesta/4.1-b4-intel-2019b

---------------------------- /sw/modules/scs5/chem -----------------------------
   Siesta/4.1-MaX-1.0-intel-2019b (D)
[...]
marie@login$ module load Siesta
[...]
Module Siesta/4.1-MaX-1.0-intel-2019b and 26 dependencies loaded.
```

In any paper or other academic publication containing results wholly or partially derived from the
results of use of the SIESTA package, the following papers must be cited in the normal manner:

1. "Self-consistent order-N density-functional calculations for very large systems",
P.  Ordejon, E. Artacho and J. M. Soler, Phys. Rev. B (Rapid Comm.) 53, R10441-10443 (1996).
1. "The SIESTA method for ab initio order-N materials simulation", J. M. Soler, E. Artacho,
J. D. Gale, A. Garcia, J. Junquera, P. Ordejon, and D. Sanchez-Portal, J. Phys.: Condens. Matt. 14,
2745-2779 (2002).

## VASP

> VAMP/VASP is a package for performing ab-initio quantum-mechanical molecular dynamics (MD) using
> pseudopotentials and a plane wave basis set. (see [VASP](https://www.vasp.at)).

VASP is available as [modules](modules.md). Available packages can be listed and loaded with the
following commands:

```console
marie@login$:~> module avail VASP
---------------------------- /sw/modules/scs5/phys -----------------------------
   VASP/5.4.4-intel-2018a-optics    VASP/5.4.4-intel-2019b (L,D)
   VASP/5.4.4-intel-2018a
[...]
marie@login$ module load VASP
[...]
Module VASP/5.4.4-intel-2019b loaded.
```


# GPU-accelerated Containers for Deep Learning (NGC Containers)

A [container](containers.md) is an executable and portable unit of software.
On ZIH systems, [Singularity](https://sylabs.io/) is used as a standard container solution.

[NGC](https://developer.nvidia.com/ai-hpc-containers),
a registry of highly GPU-optimized software,
has been enabling scientists and researchers by providing regularly updated
and validated containers of HPC and AI applications.
NGC containers are **GPU-optimized** containers
for deep learning, machine learning, visualization:

- Built-in libraries and dependencies;
- Faster training with Automatic Mixed Precision (AMP);
- Opportunity to scale up from single-node to multi-node systems;
- Performance optimized.

!!! note "Advantages of NGC containers"
    - NGC containers were highly optimized for cluster usage.
        The performance provided by NGC containers is comparable to the performance
        provided by the modules on the ZIH system (which is potentially the most performant way).
        NGC containers are a quick and efficient way to apply the best models
        on your dataset on a ZIH system;
    - NGC containers allow using an exact version of the software
        without installing it with all prerequisites manually.
        Manual installation can result in poor performance (e.g. using conda to install a software).

## Run NGC Containers on the ZIH System

The first step is a choice of the necessary software (container) to run.
The [NVIDIA NGC catalog](https://ngc.nvidia.com/catalog)
contains a host of GPU-optimized containers for deep learning,
machine learning, visualization, and HPC applications that are tested
for performance, security, and scalability.
It is necessary to register to have full access to the catalog.

To find a container that fits the requirements of your task, please check
the [official examples page](https://github.com/NVIDIA/DeepLearningExamples)
with the list of main containers with their features and peculiarities.

### Run NGC container on a Single GPU

!!! note
    Almost all NGC containers can work with a single GPU.

To use NGC containers, it is necessary to understand the main Singularity commands.

If you are not familiar with Singularity's syntax, please find the information on the
[official page](https://sylabs.io/guides/3.0/user-guide/quick_start.html#interact-with-images).
However, some main commands will be explained.

Create a container from the image from the NGC catalog.
(For this example, the cluster alpha is used):

```console
marie@login.alpha$ srun --nodes=1 --ntasks-per-node=1 --ntasks=1 --gres=gpu:1 --time=08:00:00 --pty --mem=50000 bash
[...]
marie@alpha$ cd /data/horse/ws/<name_of_your_workspace>/containers   #please create a Workspace
[...]
marie@alpha$ singularity pull pytorch:21.08-py3.sif docker://nvcr.io/nvidia/pytorch:21.08-py3
```

Now, you have a fully functional PyTorch container.

Please pay attention, using `srun` directly on the shell will lead to
background by using batch jobs.
For that, you can conveniently put the parameters directly into the job file,
which you can submit using `sbatch` command.

In the majority of cases, the container doesn't contain the dataset for training models.
To download the dataset, please follow the
[instructions](https://github.com/NVIDIA/DeepLearningExamples) for the exact container.
Also, you can find the instructions in a README file which you can find inside the container:

```console
marie@alpha$ singularity exec pytorch:21.06-py3_beegfs vim /workspace/examples/resnet50v1.5/README.md
```

It is recommended to run the container with a single command.
However, for the educational purpose, the separate commands will be presented below:

```console
marie@login.alpha$ srun --nodes=1 --ntasks-per-node=1 --ntasks=1 --gres=gpu:1 --time=08:00:00 --pty --mem=50000 bash
```

Run a shell within a container with the `singularity shell` command:

```console
marie@alpha$ singularity shell --nv -B /data/horse/imagenet:/data/imagenet pytorch:21.06-py3
```

The flag `--nv` in the command above was used to enable Nvidia support for GPU usage
and a flag `-B` for a user-bind path specification.

Run the training inside the container:

```console
marie@container$ python /workspace/examples/resnet50v1.5/multiproc.py --nnodes=1 --nproc_per_node=1 \
                --node_rank=0 /workspace/examples/resnet50v1.5/main.py --data-backend dali-cpu \
                --raport-file raport.json -j16 -p 100 --lr 2.048 --optimizer-batch-size 2048 --warmup 8 \
                --arch resnet50 -c fanin --label-smoothing 0.1 --lr-schedule cosine --mom 0.875 \
                --wd 3.0517578125e-05 -b 256 --epochs 90 /data/imagenet
```

!!! warning
    Please keep in mind that it is necessary to specify the amount of resources that you use inside
    the container, especially if you have allocated more resources in the cluster. Regularly, you
    can do it with flags such as `--nproc_per_node`. You can find more information in the README
    file inside the container.

As an example, please find the full command to run the ResNet50 model
on the ImageNet dataset inside the PyTorch container:

```console
marie@login.alpha$ srun --nodes=1 --ntasks-per-node=1 --ntasks=1 --gres=gpu:1 --time=08:00:00 --pty --mem=50000 \
                singularity exec --nv -B /data/horse/ws/anpo879a-ImgNet/imagenet:/data/imagenet pytorch:21.06-py3 \
                python /workspace/examples/resnet50v1.5/multiproc.py --nnodes=1 --nproc_per_node 1 \
                --node_rank=0 /workspace/examples/resnet50v1.5/main.py --data-backend dali-cpu --raport-file raport.json \
                -j16 -p 100 --lr 2.048 --optimizer-batch-size 2048 --warmup 8 --arch resnet50 -c fanin --label-smoothing 0.1 \
                --lr-schedule cosine --mom 0.875 --wd 3.0517578125e-05 -b 256 --epochs 90 /data/imagenet
```

### Multi-GPU Usage

The majority of the NGC containers allow you to use multiple GPUs from one node
to run the model inside the container.
However, the NGC containers were made by Nvidia for the Nvidia cluster,
which is not ZIH system.
Moreover, editing NGC containers requires root privileges,
which can be done only with [containers](containers.md) on ZIH systems.
Thus, there is no guarantee that all NGC containers work right out of the box.

However, PyTorch and TensorFlow containers support multi-GPU usage.

An example of using the PyTorch container for the training of the ResNet50 model
on the classification task on the ImageNet dataset is presented below:

```console
marie@login.alpha$ srun --nodes=1 --ntasks-per-node=8 --ntasks=8 --gres=gpu:8 --time=08:00:00 --pty --mem=700G bash
```

```console
marie@alpha$ singularity exec --nv -B /data/horse/ws/marie-ImgNet/imagenet:/data/imagenet pytorch:21.06-py3 \
                python /workspace/examples/resnet50v1.5/multiproc.py --nnodes=1 --nproc_per_node 8 \
                --node_rank=0 /workspace/examples/resnet50v1.5/main.py --data-backend dali-cpu \
                --raport-file raport.json -j16 -p 100 --lr 2.048 --optimizer-batch-size 2048 --warmup 8 \
                --arch resnet50 -c fanin --label-smoothing 0.1 --lr-schedule cosine --mom 0.875 \
                --wd 3.0517578125e-05 -b 256 --epochs 90 /data/imagenet
```

Please pay attention to the parameter `--nproc_per_node`.
The value is equal to 8 because 8 GPUs per node were allocated with `--gres=gpu:8`.

### Multi-node Usage

There are few NGC containers with Multi-node support
[available](https://github.com/NVIDIA/DeepLearningExamples).
Moreover, the realization of the multi-node usage depends on the authors
of the exact container.
Thus, right now, it is not possible to run NGC containers with multi-node support
on the ZIH system without changing the source code inside the container.


# Environment and Software

A joyful and efficient usage of ZIH systems bases on a profound understanding of the working
environment, which comprises your personal *user environment* and the *software environment*.

## User Environment

All ZIH systems use global home directories to provide homogeneous user environments across all
systems. The default login shell is `bash`. Personal additions and modifications can be put into
so called dotfiles in your home directory, e.g., `~/.bashrc` or `~/.bash_profile`.

## Software Environment

There are different options to work with software on ZIH systems: [modules](#modules),
[Jupyter Notebook](#jupyter-notebook) and [containers](#containers). Brief descriptions and related
links on these options are provided below.

## Modules

Usage of software on ZIH systems, e.g., frameworks, compilers, loader and libraries, is
almost always managed by a **modules system**. Thus, it is crucial to be familiar with the
[modules concept and its commands](modules.md).  A module is a user interface that provides
utilities for the dynamic modification of a user's environment without manual modifications.

Modules are used to set up the environment when working on ZIH systems via batch system (e.g.,
`srun` or `sbatch`), and the [JupyterHub](../access/jupyterhub.md).

## Jupyter Notebook

The [Jupyter Notebook](https://jupyter.org/) is an open-source web application that allows creating
documents containing live code, equations, visualizations, and narrative text. There is a
[JupyterHub](../access/jupyterhub.md) service on ZIH systems, where you can simply run your Jupyter
notebook on compute nodes using [modules](#modules), preloaded or custom virtual environments.
Moreover, you can run a [manually created remote Jupyter server](../archive/install_jupyter.md)
for more specific cases.

## Containers

Some tasks require using containers. It can be done on ZIH Systems by [Singularity](containers.md).


# Read CPU Performance Counters with PAPI

## Introduction

The **P**erformance **A**pplication **P**rogramming **I**nterface (PAPI) provides tool designers and
application engineers with a consistent interface and methodology for the use of low-level
performance counter hardware found across the entire compute system (i.e. CPUs, GPUs, on/off-chip
memory, interconnects, I/O system, energy/power, etc.). PAPI enables users to see, in near real
time, the relations between software performance and hardware events across the entire computer
system.

Only the basic usage is outlined in this compendium. For a comprehensive PAPI user manual please
refer to the [PAPI wiki website](https://bitbucket.org/icl/papi/wiki/Home).

## PAPI Counter Interfaces

To collect performance events, PAPI provides two APIs, the *high-level* and *low-level* API.

### High-Level API

The high-level API provides the ability to record performance events inside instrumented regions of
serial, multi-processing (MPI, SHMEM) and thread (OpenMP, Pthreads) parallel applications. It is
designed for simplicity, not flexibility. More details can be found in the
[PAPI wiki High-Level API description](https://bitbucket.org/icl/papi/wiki/PAPI-HL.md).

The following code example shows the use of the high-level API by marking a code section.

??? example "C"

    ```C
    #include "papi.h"

    int main()
    {
        int retval;

        retval = PAPI_hl_region_begin("computation");
        if ( retval != PAPI_OK )
            handle_error(1);

        /* Do some computation here */

        retval = PAPI_hl_region_end("computation");
        if ( retval != PAPI_OK )
            handle_error(1);
    }
    ```

??? example "Fortran"

    ```fortran
    #include "fpapi.h"

    program main
    integer retval

    call PAPIf_hl_region_begin("computation", retval)
    if ( retval .NE. PAPI_OK ) then
       write (*,*) "PAPIf_hl_region_begin failed!"
    end if

    !do some computation here

    call PAPIf_hl_region_end("computation", retval)
    if ( retval .NE. PAPI_OK ) then
       write (*,*) "PAPIf_hl_region_end failed!"
    end if

    end program main
    ```

Events to be recorded are determined via the environment variable `PAPI_EVENTS` that lists comma
separated events for any component (see example below). The output is generated in the current
directory by default. However, it is recommended to specify an output directory for larger
measurements, especially for MPI applications via environment variable `PAPI_OUTPUT_DIRECTORY`.

!!! example "Setting performance events and output directory"

    ```bash
    export PAPI_EVENTS="PAPI_TOT_INS,PAPI_TOT_CYC"
    export PAPI_OUTPUT_DIRECTORY="/data/horse/measurement"
    ```

This will generate a directory called `papi_hl_output` in `/data/horse/measurement` that contains
one or more output files in JSON format.

### Low-Level API

The low-level API manages hardware events in user-defined groups called Event Sets. It is meant for
experienced application programmers and tool developers wanting fine-grained measurement and
control of the PAPI interface. It provides access to both PAPI preset and native events, and
supports all installed components. The PAPI wiki contains also a page with more details on the
[low-level API](https://bitbucket.org/icl/papi/wiki/PAPI-LL.md).

## Usage on ZIH Systems

Before you start a PAPI measurement, check which events are available on the desired architecture.
For this purpose, PAPI offers the tools `papi_avail` and `papi_native_avail`. If you want to measure
multiple events, please check which events can be measured concurrently using the tool
`papi_event_chooser`. The PAPI wiki contains more details on
[the PAPI tools](https://bitbucket.org/icl/papi/wiki/PAPI-Overview.md#markdown-header-papi-utilities)
.

!!! hint

    The PAPI tools must be run on the compute node, using an interactive shell or job.

!!! example "Example: Determine the events on the cluster `romeo` from a login node"
    Let us assume, that you are in project `p_number_crunch`. Then, use the following commands:

    ```console
    marie@login.romeo$ module load PAPI
    marie@login.romeo$ salloc --account=p_number_crunch
    [...]
    marie@romeo$ srun papi_avail
    marie@romeo$ srun papi_native_avail
    [...]
    # Exit with Ctrl+D
    ```

Instrument your application with either the high-level or low-level API. Load the PAPI module and
compile your application against the  PAPI library.

!!! example
    Assuming that you are in project `p_number_crunch`, use the following commands:

    ```console
    marie@login.romeo$ module load PAPI
    marie@login.romeo$ gcc app.c -o app -lpapi
    marie@login.romeo$ salloc --account=p_number_crunch
    marie@romeo$ srun ./app
    [...]
    # Exit with Ctrl+D
    ```

!!! hint

    The PAPI modules on ZIH systems are only installed with the default `perf_event` component. If you
    want to measure, e.g., GPU events, you have to install your own PAPI. Please see the
    [external instructions on how to download and install PAPI](https://bitbucket.org/icl/papi/wiki/Downloading-and-Installing-PAPI.md).
    To install PAPI with additional components, you have to specify them during configure as
    described for the [Installation of Components](https://bitbucket.org/icl/papi/wiki/PAPI-Overview.md#markdown-header-components).


---
search:
  boost: 450.0
---

# Performance Engineering Overview

!!! cite "Walter J. Doherty, 1970 [^1]"

    Fundamentally, performance is the degree to which a computing system meets the expectations of
    the person involved with it.

Performance engineering encompasses the techniques applied during a systems development life cycle
to ensure the non-functional requirements for performance (such as throughput, latency, or memory
usage) will be met.
Often, it is also referred to as systems performance engineering within systems engineering, and
software performance engineering or application performance engineering within software engineering
[[Wikipedia]](https://en.wikipedia.org/wiki/Performance_engineering).

[^1]: Scheduling TSS/360 for responsiveness. In: AFIPS '70 (Fall): Proceedings of the November
  17-19, 1970, fall joint computer conference, November 1970, Pages 97–111

## Objectives

??? hint "Some good reasons to think about performance in HPC"

    - Increase research output by ensuring the system can process transactions within the requisite time
      frame
    - Eliminate system failure requiring scrapping and writing off the system development effort due to
      performance objective failure
    - Eliminate avoidable system tuning efforts
    - Avoid additional and unnecessary hardware acquisition costs
    - Reduce increased software maintenance costs due to performance problems in production
    - Reduce additional operational overhead for handling system issues due to performance problems
    - Identify future bottlenecks by simulation over prototype

## Installed Tools in a Nutshell

| Tool                 | Task                                         | Easiness    | Details  | Overhead  | Re-compilation |
|----------------------|----------------------------------------------|-------------|----------|-----------|-----------------
| [lo2s](#lo2s)        | Create performance [trace](#trace)           | easy        | medium   | low       | (no)[^2]       |
| [MUST](#must)        | Check MPI correctness                        | medium      | medium   | variable  | no             |
| [PAPI](#papi)        | Read portable CPU counters                   | advanced    | medium   | variable  | yes            |
| [Perf](#perf-tools)  | Produce and visualize [profile](#profile)    | easy        | medium   | low       | (no)[^2]       |
| [PIKA](#pika)        | Show performance [profile](#profile) and [trace](#trace) | very easy | low | very low | no         |
| [Score-P](#score-p)  | Create performance [trace](#trace)           | complex     | high     | variable  | yes            |
| [Vampir](#vampir)    | Visualize performance [trace](#trace)        | complex     | high     | n.a.      | n.a.           |

[^2]: Re-compilation is not required. Yet, to obtain more details it is recommended to re-compile with the `-g` compiler option, which adds debugging information to the executable of an application.

## Approach and Terminology

Performance engineering typically is a cyclic process.
The following figure shows such a process and its potential stages.

![Performance engineering cycle](misc/performance_engineering_cycle.svg)

### Instrumentation

!!! hint "Instrumentation is a common term for preparing the performance measurement"

The engineering process typically begins with the original application in its unmodified state.
First, this application needs to be instrumented, i.e. it must be prepared to enable the
measurement of the performance properties.
There are different ways to do this, including manual instrumentation of the source code by the
user, automatic instrumentation by the compiler, linking against pre-instrumented libraries, or
interrupt-driven sampling during run time.

### Measurement

!!! note "During measurement, raw performance data is collected"

When an instrumented application is executed, the additional instructions introduced during the
instrumentation phase collect and record the data required to evaluate the performance properties
of the code.
Unfortunately, the measurement itself has a certain influence on the performance of the instrumented
code.
Whether the perturbations introduced have a significant effect on the behavior depends on the
specific structure of the code to be investigated.
In many cases, the perturbations will be rather small, so that the overall results can be considered
to be a realistic approximation of the corresponding properties of the non-instrumented code.
Yet, it is always advisable to compare the runtime of instrumented applications with their original
non-instrumented counterpart.

#### Profile

!!! hint "Performance profiles hold aggregated data (e.g. total time spent in function `foo()`)"

A performance profile provides aggregated metrics like _time_ or _number of calls_ for a list of
functions, loops or similar as depicted in the following table:

| Function | Total Time | Calls | Percentage |
|----------|-----------:|------:|-----------:|
| `main()` |        2 s |     1 |       1%   |
| `foo()`  |       80 s |   100 |      40%   |
| `bar()`  |      118 s |  9000 |      59%   |

#### Trace

<!-- markdownlint-disable-next-line line-length -->
!!! hint "Traces consist of a sorted list of timed application events/samples (e.g. enter function `foo()` at 0.11 s)."

In contrast to performance [profiles](#profile), performance traces consist of individual
application samples or events that are recorded with a timestamp.
A trace that corresponds to the profile recording above could look as follows:

| Timestamp | Data Type      | Parameter       |
|----------:|----------------|-----------------|
|    0.10 s | Enter Function | `main()`        |
|    0.11 s | Enter Function | `foo()`         |
|    0.12 s | Enter Function | `bar()`         |
|    0.15 s | Exit Function  | `bar()`         |
|    0.16 s | Enter Function | `bar()`         |
|    0.17 s | Exit Function  | `bar()`         |
|           | _many more events..._  |         |
|  200.00 s | Exit Function  | `main()`        |

<!-- markdownlint-disable-next-line line-length -->
!!! hint "Traces enable more sophisticated analysis at the cost of potentially very large amounts of raw data."

Apparently, the size of a performance trace depends on the recorded time whereas a profile does not.
Likewise, a trace can tell you when a specific action in your application happened whereas a profile
will tell you how much time in total a class of actions takes.

### Analysis

!!! note "Well defined performance metrics are derived from raw performance data during analysis"

The collected raw data is typically processed by a analysis tool (profiler, consistency checker, you
name it) to derive meaningful, well-defined performance metrics like data rates, data dependencies,
performance events of interest, etc.
This step is typically hidden to the user and taken care of automatically once the raw data was
collected.
Some tools, however, provide an independent analysis front-end that allows specifying the type of
analysis to carry out on the raw data.

### Presentation

!!! note "Presenting performance metrics graphically fosters human intuition"

After processing the raw performance data, the resulting metrics are usually presented in the form
of a report that makes use of tables or charts known from programs like Excel.
In this step, the reduction of the data complexity simplifies the evaluation of the data by software
developers.
Yet, data reductions have the potential to hide important facts or details.

### Evaluation

!!! note "The evaluation of performance metrics requires tools and lots of thinking"

During the evaluation phase, the metrics and findings in a performance report are compared to the
behavior/performance as expected by software developers.
This step typically requires a fair amount of knowledge about the application under test or software
performance in general.
The application is considered to behave sufficiently well or weaknesses have been identified which
potentially can be improved.
An application or its configuration is changed in the later case.
After evaluating an application's performance, the cyclic engineering process is either completed or
restarted from beginning.

## Installed Tools Summary

At ZIH, the following performance engineering tools are installed and maintained:

### lo2s

!!! hint "Easy to use application and system performance trace recorder supporting Vampir"

[lo2s](lo2s.md) records the status of an application at fixed intervals (statistical sampling).
It does not require any [instrumentation](#instrumentation).
The [measurement](#measurement) of a given application is done by pre-fixing the application's
executable with `lo2s`.
The data analysis of the fixed metrics is fully integrated and does not require any user actions.
Performance data is written to a [traces](#trace) repository at the current directory.
See [lo2s](lo2s.md) for further details.
Once the data have been recorded, the tool [Vampir](vampir.md) needs to be invoked to study the data
graphically.

### MUST

<!-- markdownlint-disable-next-line line-length -->
!!! hint "Advanced communication error detection for applications using the Message Passing Interface (MPI) standard."

[MUST](mpi_usage_error_detection.md) checks your application for communication errors if the MPI
library is used.
It does not require any [instrumentation](#instrumentation).
The checks of a given MPI application are done by simply replacing `srun` with `mustrun` when the
application is started.
The data analysis of the fixed metrics is fully integrated and does not require any user actions.
The correctness results are written to an HTML-formatted output file, which can be inspected with a
web browser.

### PAPI

!!! hint "Portable reading of CPU performance metrics like FLOPS"

The [PAPI](papi.md) library allows software developers to read CPU performance counters in a
platform-independent way.
Native usage of the library requires to manually [instrument](#instrumentation) an application by
adding library calls to the source code of the application under investigation.
Data [measurement](#measurement) happens whenever the PAPI library is called.
The data obtained is raw data.
Software developers have to process the data by themselves to obtain meaningful metrics.
Tools like [Score-P](#score-p) have built-in support for PAPI.
Therefore, native usage of the PAPI library is usually not needed.

### Perf Tools

!!! hint "Easy to use Linux-integrated performance data recording and analysis"

[Linux perf](perf_tools.md) reads and analyses CPU performance counters for any given application.
It does not require any [instrumentation](#instrumentation).
The [measurement](#measurement) of a given application is done by simply prefixing the application
executable with `perf`.
Perf has two modes of operation (`perf stat`, `perf record`), which both record [profile](#profile)
raw data.
While the first mode is very basic, the second mode records more data.
Use `perf report` to analyze the raw output data of `perf record` and produce a performance report.
See [Linux perf](perf_tools.md) for further details.

### PIKA

!!! hint "Very easy to use performance visualization of entire batch jobs"

[PIKA](pika.md) allows users to study their active and completed
[batch jobs](../jobs_and_resources/slurm.md).
It does not require any [instrumentation](#instrumentation).
The [measurement](#measurement) of batch jobs happens automatically in the background for all batch
jobs.
The data analysis of the given set of metrics is fully integrated and does not require any user
actions.
Performance metrics are accessible via the
[PIKA web service](https://pika.zih.tu-dresden.de/).

### Score-P

!!! hint "Complex and powerful performance data recording and analysis of parallel applications"

[Score-P](scorep.md) is an advanced tool that measures configurable performance event data.
It generates both [profiles](#profile) and detailed [traces](#trace) for subsequent analysis.
It supports automated [instrumentation](#instrumentation) of an application (involves
re-compilation) prior to the [measurement](#measurement) step.
The data analysis of the raw performance data can be carried out with the tools `scalasca`
(advanced MPI metrics), `cube` ([profile](#profile) viewer), `scorep-score` ([profile](#profile)
command line viewer), or [Vampir](#vampir) ([trace](#trace) viewer).
Many raw data sources are supported by Score-P.
It requires some time, training, and practice to fully benefit from the tool's features.
See [Score-P](scorep.md) for further details.

### Vampir

!!! hint "Complex and powerful performance data visualization of parallel applications"

[Vampir](vampir.md) is a graphical analysis tool that provides a large set of different chart
representations for performance data [traces](#trace) generated by tools such as
[Score-P](scorep.md) or [lo2s](lo2s.md).
Complex statistics, timelines, and state diagrams can be used by software developers to obtain a
better understanding of the inner working of a parallel application.
The tool requires some time, training, and practice to fully benefit from its rich set of features.


# Produce Performance Overview with Perf

The Linux `perf` command provides support for sampling applications and reading performance
counters. `perf` consists of two parts: the kernel space implementation and the userland tools.
This compendium page focusses on the latter.

For detailed information, please refer to the [perf
documentation](https://perf.wiki.kernel.org/index.php/Main_Page) and the comprehensive
[perf examples page](https://www.brendangregg.com/perf.html) of Brendan Gregg.

## Configuration

Admins can change the behaviour of the perf tools kernel part via the
following interfaces

| File Name                                   | Description                                                                                                                       |
|---------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|
| `/proc/sys/kernel/perf_event_max_sample_rate` | Describes the maximal sample rate for perf record and native access. This is used to limit the performance influence of sampling. |
| `/proc/sys/kernel/perf_event_mlock_kb`        | Defines the number of pages that can be used for sampling via perf record or the native interface                                 |
| `/proc/sys/kernel/perf_event_paranoid`        | Defines access rights:                                                                                                            |
|                                             | -1 - Not paranoid at all                                                                                                          |
|                                             | 0 - Disallow raw tracepoint access for unpriv                                                                                     |
|                                             | 1 - Disallow cpu events for unpriv                                                                                                |
|                                             | 2 - Disallow kernel profiling for unpriv                                                                                          |
| `/proc/sys/kernel/kptr_restrict`              | Defines whether the kernel address maps are restricted                                                                            |

## Perf Stat

`perf stat` provides a general performance statistic for a program. You
can attach to a running (own) process, monitor a new process or monitor
the whole system. The latter is only available for root user, as the
performance data can provide hints on the internals of the application.

### For Users

Run `perf stat <Your application>`. This will provide you with a general
overview on some counters. The following listing holds an exemplary output for sampling the `ls`
command.

```console
marie@compute$ perf stat ls
[...]
Performance counter stats for 'ls':=
          2,524235 task-clock                #    0,352 CPUs utilized
                15 context-switches          #    0,006 M/sec
                 0 CPU-migrations            #    0,000 M/sec
               292 page-faults               #    0,116 M/sec
         6.431.241 cycles                    #    2,548 GHz
         3.537.620 stalled-cycles-frontend   #   55,01% frontend cycles idle
         2.634.293 stalled-cycles-backend    #   40,96% backend  cycles idle
         6.157.440 instructions              #    0,96  insns per cycle
                                             #    0,57  stalled cycles per insn
         1.248.527 branches                  #  494,616 M/sec
            34.044 branch-misses             #    2,73% of all branches
       0,007167707 seconds time elapsed
```

- Generally speaking **task clock** tells you how parallel your job
  has been/how many cpus were used.
- [Context switches](http://en.wikipedia.org/wiki/Context_switch)
  are an information about how the scheduler treated the application.  Also interrupts cause context
  switches. Lower is better.
- **CPU migrations** are an information on whether the scheduler moved
  the application between cores. Lower is better. Please pin your programs to CPUs to avoid
  migrations. This can be done with environment variables for OpenMP and MPI, with `likwid-pin`,
  `numactl` and `taskset`.
- [Page faults](http://en.wikipedia.org/wiki/Page_fault) describe
  how well the Translation Lookaside Buffers fit for the program.  Lower is better.
- **Cycles** tells you how many CPU cycles have been spent in
  executing the program. The normalized value tells you the actual average frequency of the CPU(s)
  running the application.
- **stalled-cycles-...** tell you how well the processor can execute
  your code. Every stall cycle is a waste of CPU time and energy. The reason for such stalls can be
  numerous. It can be wrong branch predictions, cache misses, occupation of CPU resources by long
  running instructions and so on. If these stall cycles are to high you might want to review your
  code.
- The normalized **instructions** number tells you how well your code
  is running. More is better. Current x86 CPUs can run 3 to 5 instructions per cycle, depending on
  the instruction mix. A count of less then 1 is not favorable. In such a case you might want to
  review your code.
- **branches** and **branch-misses** tell you how many jumps and loops
  are performed in your code. Correctly [predicted](http://en.wikipedia.org/wiki/Branch_prediction)
  branches should not hurt your performance, **branch-misses** on the other hand hurt your
  performance very badly and lead to stall cycles.
- Other events can be passed with the `-e` flag. For a full list of
  predefined events run `perf list`
- PAPI runs on top of the same infrastructure as `perf stat`, so you
  might want to use their meaningful event names. Otherwise you can use raw events, listed in the
  processor manuals.

### For Admins

Administrators can run a system wide performance statistic, e.g., with `perf stat -a sleep 1` which
measures the performance counters for the whole computing node over one second.

## Perf Record

`perf record` provides the possibility to sample an application or a system. You can find
performance issues and hot parts of your code. By default `perf record` samples your program at 4000
Hz. It records CPU, Instruction Pointer and, if you specify it, the call chain. If your code runs
long (or often) enough, you can find hot spots in your application and external libraries.
Use [perf report](#perf-report) to evaluate the result. You should have debug symbols available,
otherwise you won't be able to see the name of the functions that are responsible for your load. You
can pass one or multiple events to define the **sampling event**.

!!! note "What is a sampling event?"

    Sampling reads values at a specific sampling frequency. This frequency is usually static and
    given in Hz, so you have for example 4000 events per second and a sampling frequency of 4000 Hz
    and a sampling rate of 250 microseconds. With the sampling event, the concept of a static
    sampling frequency in time is somewhat redefined. Instead of a constant factor in time (sampling
    rate) you define a constant factor in events. So instead of a sampling rate of 250 microseconds,
    you have a sampling rate of 10,000 floating point operations.

!!! note "Why would you need sampling events?"

    Passing an event allows you to find the functions that produce cache misses, floating point
    operations, ... Again, you can use events defined in `perf list` and raw events.

Use the `-g` flag to receive a call graph.

### For Users

Just run `perf record ./myapp` or attach to a running process.

#### Using Perf with MPI

Perf can also be used to record data for indivdual MPI processes. This requires a wrapper script
(`perfwrapper`) with the following content. Also make sure that the wrapper script is executable
(`chmod +x`).

```Bash
#!/bin/bash
perf record -o perf.data.$SLURM_JOB_ID.$SLURM_PROCID $@
```

To start the MPI program type `srun ./perfwrapper ./myapp` on your command line. The result will be
n independent `perf.data` files that can be analyzed individually using `perf report`.

### For Admins

This tool is very effective, if you want to help users find performance problems and hot-spots in
their code but also helps to find OS daemons that disturb such applications. You would start `perf
record -a -g` to monitor the whole node.

## Perf Report

`perf report` is a command line UI for evaluating the results from perf record. It creates something
like a profile from the recorded samplings.  These profiles show you what the most used have been.
If you added a callchain, it also gives you a callchain profile.

!!! note "Disclaimer"

    Sampling is not an appropriate way to gain exact numbers. So this is merely a rough overview and
    not guaranteed to be absolutely correct.

### On ZIH Systems

On ZIH systems, users are not allowed to see the kernel functions. If you have multiple events
defined, then the first thing you select in `perf report` is the type of event. Press the right
arrow key:

```Bash
Available samples
96 cycles
11 cache-misse
```

!!! hint

    * The more samples you have, the more exact is the profile. 96 or
    11 samples is not enough by far.
    * Repeat the measurement and set `-F 50000` to increase the sampling frequency.
    * The higher the frequency, the higher the influence on the measurement.

If you'd select cycles, you would get such a screen:

```Bash
Events: 96  cycles
+  49,13%  test_gcc_perf  test_gcc_perf      [.] main.omp_fn.0
+  34,48%  test_gcc_perf  test_gcc_perf      [.]
+   6,92%  test_gcc_perf  test_gcc_perf      [.] omp_get_thread_num@plt
+   5,20%  test_gcc_perf  libgomp.so.1.0.0   [.] omp_get_thread_num
+   2,25%  test_gcc_perf  test_gcc_perf      [.] main.omp_fn.1
+   2,02%  test_gcc_perf  [kernel.kallsyms]  [k] 0xffffffff8102e9ea
```

With increased sample frequency, it might look like this:

```Bash
Events: 7K cycles
+  42,61%  test_gcc_perf  test_gcc_perf      [.] p
+  40,28%  test_gcc_perf  test_gcc_perf      [.] main.omp_fn.0
+   6,07%  test_gcc_perf  test_gcc_perf      [.] omp_get_thread_num@plt
+   5,95%  test_gcc_perf  libgomp.so.1.0.0   [.] omp_get_thread_num
+   4,14%  test_gcc_perf  test_gcc_perf      [.] main.omp_fn.1
+   0,69%  test_gcc_perf  [kernel.kallsyms]  [k] 0xffffffff8102e9ea
+   0,04%  test_gcc_perf  ld-2.12.so         [.] check_match.12442
+   0,03%  test_gcc_perf  libc-2.12.so       [.] printf
+   0,03%  test_gcc_perf  libc-2.12.so       [.] vfprintf
+   0,03%  test_gcc_perf  libc-2.12.so       [.] __strchrnul
+   0,03%  test_gcc_perf  libc-2.12.so       [.] _dl_addr
+   0,02%  test_gcc_perf  ld-2.12.so         [.] do_lookup_x
+   0,01%  test_gcc_perf  libc-2.12.so       [.] _int_malloc
+   0,01%  test_gcc_perf  libc-2.12.so       [.] free
+   0,01%  test_gcc_perf  libc-2.12.so       [.] __sigprocmask
+   0,01%  test_gcc_perf  libgomp.so.1.0.0   [.] 0x87de
+   0,01%  test_gcc_perf  libc-2.12.so       [.] __sleep
+   0,01%  test_gcc_perf  ld-2.12.so         [.] _dl_check_map_versions
+   0,01%  test_gcc_perf  ld-2.12.so         [.] local_strdup
+   0,00%  test_gcc_perf  libc-2.12.so       [.] __execvpe
```

Now you select the most often sampled function and zoom into it by pressing the right arrow key. If
debug symbols are not available, `perf report` will show which assembly instruction is hit most often
when sampling. If debug symbols are available, it will also show you the source code lines for
these assembly instructions. You can also go back and check which instruction caused the cache
misses or whatever event you were passing to `perf record`.

## Perf Script

If you need a trace of the sampled data, you can use `perf script` command, which by default prints
all samples to stdout. You can use various interfaces (e.g., Python) to process such a trace.

## Perf Top

`perf top` is only available for admins, as long as the paranoid flag is not changed (see
configuration).

It behaves like the `top` command, but gives you not only an overview of the processes and the time
they are consuming but also on the functions that are processed by these.


---
search:
  boost: 4.0
---

# Track Slurm Jobs with PIKA

PIKA is a hardware performance monitoring stack to identify inefficient HPC jobs. Users of ZIH
systems have the possibility to visualize and analyze the efficiency of their jobs via the
[PIKA web interface](https://pika.zih.tu-dresden.de).

!!! hint

    To understand this guide, it is recommended that you open the
    [web interface](https://pika.zih.tu-dresden.de)
    in a separate window. Furthermore, you should have submitted at least one real HPC job at ZIH
    systems.

    If you are outside the TUD network, you will need to establish a VPN connection. For more
    information on our VPN and how to set it up, please visit the corresponding
    [ZIH service catalog
    page](https://tu-dresden.de/zih/dienste/service-katalog/arbeitsumgebung/zugang_datennetz/vpn).

## Overview

PIKA consists of several components and tools. It uses the collection daemon collectd, InfluxDB to
store time-series data and MariaDB to store job metadata. Furthermore, it provides a powerful
[web interface](https://pika.zih.tu-dresden.de)
for the visualization and analysis of job performance data.

## Table View and Job Search

The analysis of HPC jobs in PIKA is designed as a top-down approach. Starting from the table view,
you can either analyze running or completed jobs. You can navigate from groups of jobs with the
same name to the metadata of an individual job and finally investigate the job’s runtime metrics in
a timeline view.

To find jobs with specific properties, you can sort the table by any column, e.g., by consumed CPU
hours to find jobs where an optimization has a large impact on the system utilization. Additionally,
there is a filter mask to find jobs that match several properties. When a job has been selected, the
timeline view opens.

## Timeline Visualization

PIKA provides timeline charts to visualize the resource utilization of a job over time. After a job
is completed, timeline charts can help you to identify periods of inefficient resource usage.
However, they are also suitable for the live assessment of performance during the job’s runtime. In
case of unexpected performance behavior, you can cancel the job, thus avoiding long execution with
subpar performance.

The following timeline visualization shows a job with 840 cores, spread over 35 (dual-socket
Haswell) nodes that have been allocated for exclusive use.

![Timeline Visualization](misc/pika_timelines.png)
{: align="center"}

PIKA provides the following runtime metrics:

|Metric| Hardware Unit| Sampling Frequency|
|---|---|---:|
|CPU Usage|CPU core (average across hardware threads)|30s|
|IPC (instructions per cycle)|CPU core (sum over hardware threads)|60s|
|FLOPS (normalized to single precision) |CPU core (sum over hardware threads)|60s|
|Main Memory Bandwidth|CPU socket|60s|
|CPU Power|CPU socket|60s|
|Main Memory Utilization|node|30s|
|I/O Bandwidth (local, Lustre) |node|30s|
|I/O Metadata (local, Lustre) |node|30s|
|Network Bandwidth|node|30s|
|GPU Usage|GPU device|30s|
|GPU Memory Utilization|GPU device|30s|
|GPU Power Consumption|GPU device|30s|
|GPU Temperature|GPU device|30s|

Each monitored metric is represented by a timeline, whereby metrics with the same unit and data
source are displayed in a common chart, e.g., different Lustre metadata operations. Each metric is
measured with a certain granularity concerning the hardware, e.g. per hardware thread, per CPU
socket or per node.
Most metrics are recorded every 30 seconds except IPC, FLOPS, Main Memory Bandwidth and Power
Consumption. The latter are determined every 60 seconds, as they are a combination of different
hardware counters, which leads to a higher measurement overhead. Depending on the architecture,
metrics such as normalized FLOPS (2 x double-precision + 1 x single-precision) can require
multiplexing, since single and double precision FLOPS cannot be measured simultaneously.
The sampling frequency cannot be changed by the user.

!!! hint

    Be aware that CPU socket or node metrics can share the resources of other jobs running on the
    same CPU socket or node. This can result e.g., in cache perturbation and thus a sub-optimal
    performance. To get valid performance data for those metrics, it is recommended to submit an
    exclusive job (`--exclusive`)!

If the current partition supports simultaneous multithreading (SMT) the maximum number of hardware
threads per physical core is displayed in the SMT column. The Slurm configuration on ZIH systems
disables SMT by default. Therefore, in the example below, only a maximum CPU usage of 0.5 can be
achieved, as PIKA determines the average value over two hardware threads per physical core.
If you want to use SMT, you must set the Slurm environment variable `SLURM_HINT=multithread`.
In this case, `srun` distributes the tasks to all available hardware threads, thus a CPU usage of 1
can be reached. However, the SMT configuration only refers to the `srun` command. For single node
jobs without `srun` command the tasks are automatically distributed to all available hardware
threads.

![SMT Mode](misc/pika_smt_2.png)
{: align="center"}

!!! note

    To reduce the amount of recorded data, PIKA summarizes per hardware thread metrics to the
    corresponding physical core. In terms of simultaneous multithreading (SMT), PIKA only provides
    performance data per physical core. For CPU usage, the average value per measurement point across
    all hardware threads is calculated, while for IPC and FLOPS, the sum per measurement point is determined.

The following table explains different timeline visualization modes.
By default, each timeline shows the average value over all hardware units (HUs) per measured
interval.

|Visualization Mode| Description|
|---|---|
|Maximum |maximal value across all HUs per measured interval|
|Mean|mean value across all HUs per measured interval|
|Minimum |minimal value across all HUs per measured interval|
|Mean + Standard Deviation|mean value across all HUs including standard deviation per measured interval|
|Best|best average HU over time|
|Lowest|lowest average HU over time|

The visualization modes *Maximum*, *Mean*, and *Minimum* reveal the range in the utilization of
individual HUs per measured interval. A high deviation of the extrema from the mean value is a
reason for further investigation, since not all HUs are equally utilized.

To identify imbalances between HUs over time, the visualization modes *Best* and *Lowest* are a
first indicator how much the HUs differ in terms of resource usage. The timelines *Best* and
*Lowest* show the recorded performance data of the best/lowest average HU over time.

!!! note "More Details"

    If you want to conduct further analysis, you can download the job data as json-file(s) via the
    button in the top right section:
    
    ![Downlaod Jobdata](misc/pika_download_jobdata.png){ align=left}
    The options are

    - Metadata: Data shown in table (project, start, end, ...), jobscript, min/max/mean statistics
    - Performance Data: Data records of all metrics of every distinct device (CPU cores, GPUs, ...)
    - Cluster Data: Metadata of used partition

    <br>

    ??? example "Example: Visualize every CPU core that was allocated for the Job"

        ```python
        #in JupyterLab/Jupyter Notebook, using pandas and matplotlib
        #download the "Performance Data" and save as "jobdata.json"

        %pylab widget
        from pandas import read_json

        data = read_json('/tmp/jobdata.json', lines=True)
        for cpu in data['cpu_used'][0]['core']['series']:
            plot(cpu['data'], lw=0.5)
        ```

## Footprint Visualization

Complementary to the timeline visualization of one specific job, statistics on metadata and
footprints over multiple jobs or a group of jobs with the same name can be displayed with the
footprint view.  The performance footprint is a set of summarized run-time metrics that is generated
from the time series data for each job.  To limit the jobs displayed, a time period can be
specified.

To analyze the footprints of a larger number of jobs, a visualization with histograms and scatter
plots can be used. PIKA uses histograms to illustrate the number of jobs that fit into a category or
bin. For job states and job tags there is a fixed number of categories or values. For other
footprint metrics PIKA uses a binning with a user-configurable bin size, since the value range
usually contains an unlimited number of values.  A scatter plot enables the combined view of two
footprint metrics (except for job states and job tags), which is particularly useful for
investigating their correlation.

![Footprint](misc/pika_footprint.png)
{: align="center"}

## Hints

If you wish to perform your own measurement of performance counters using performance tools other
than PIKA, it is recommended to disable PIKA monitoring. This can be done using the following Slurm
flags in the job script:

```Bash
#SBATCH --exclusive
#SBATCH --constraint=no_monitoring
```

**Note:** Disabling PIKA monitoring is possible only for exclusive jobs!

## Case Studies

### Idle CPUs

![CPU Idle](misc/pika_cpu_idle.png)
{: align="center"}

### Blocking I/O Operations

![I/O Blocking](misc/pika_io_block.png)
{: align="center"}

### Memory Leaks

![Memory Leaking](misc/pika_mem_leak.png)
{: align="center"}


# Machine Learning with PowerAI

There are different documentation sources for users to learn more about
the PowerAI Framework for Machine Learning. In the following the links
are valid for PowerAI version 1.5.4.

!!! warning
    The information provided here is available from IBM and can be used on the `Power9` cluster only!

## General Overview

-   [PowerAI Introduction](https://www.ibm.com/support/knowledgecenter/en/SS5SF7_1.5.3/welcome/welcome.htm)
    (note that you can select different PowerAI versions with the drop down menu
    "Change Product or version")
-   [PowerAI Developer Portal](https://developer.ibm.com/linuxonpower/deep-learning-powerai/)
    (Some Use Cases and examples)
-   [Included Software Packages](https://www.ibm.com/support/knowledgecenter/en/SS5SF7_1.5.4/navigation/pai_software_pkgs.html)
    (note that you can select different PowerAI versions with the drop down menu "Change Product
    or version")

## Specific User Guides

- [Getting Started with PowerAI](https://www.ibm.com/support/knowledgecenter/SS5SF7_1.5.4/navigation/pai_getstarted.htm)
- [Caffe](https://www.ibm.com/support/knowledgecenter/SS5SF7_1.5.4/navigation/pai_getstarted_caffe.html)
- [TensorFlow](https://www.ibm.com/support/knowledgecenter/SS5SF7_1.5.4/navigation/pai_getstarted_tensorflow.html?view=kc)
- [TensorFlow Probability](https://www.ibm.com/support/knowledgecenter/SS5SF7_1.5.4/navigation/pai_getstarted_tensorflow_prob.html?view=kc)
  This release of PowerAI includes TensorFlow Probability. TensorFlow Probability is a library
  for probabilistic reasoning and statistical analysis in TensorFlow.
- [TensorBoard](https://www.ibm.com/support/knowledgecenter/SS5SF7_1.5.4/navigation/pai_getstarted_tensorboard.html?view=kc)
- [Snap ML](https://www.ibm.com/support/knowledgecenter/SS5SF7_1.5.4/navigation/pai_getstarted_snapml.html)
  This release of PowerAI includes Snap Machine Learning (Snap ML). Snap ML is a library for
  training generalized linear models. It is being developed at IBM with the
  vision to remove training time as a bottleneck for machine learning
  applications. Snap ML supports many classical machine learning
  models and scales gracefully to data sets with billions of examples
  or features. It also offers distributed training, GPU acceleration,
  and supports sparse data structures.
- [PyTorch](https://www.ibm.com/support/knowledgecenter/SS5SF7_1.5.4/navigation/pai_getstarted_pytorch.html)
  This release of PowerAI includes
  the community development preview of PyTorch 1.0 (rc1). PowerAI's
  PyTorch includes support for IBM's Distributed Deep Learning (DDL)
  and Large Model Support (LMS).
- [Caffe2 and ONNX](https://www.ibm.com/support/knowledgecenter/SS5SF7_1.5.4/navigation/pai_getstarted_caffe2ONNX.html)
  This release of PowerAI includes a Technology Preview of Caffe2 and ONNX. Caffe2 is a
  companion to PyTorch. PyTorch is great for experimentation and rapid
  development, while Caffe2 is aimed at production environments. ONNX
  (Open Neural Network Exchange) provides support for moving models
  between those frameworks.
- [Distributed Deep Learning](https://www.ibm.com/support/knowledgecenter/SS5SF7_1.5.4/navigation/pai_getstarted_ddl.html?view=kc)
  Distributed Deep Learning (DDL). Works on up to 4 nodes on cluster `Power9`.

## PowerAI Container

We have converted the official Docker container to Singularity. Here is
a documentation about the Docker base container, including a table with
the individual software versions of the packages installed within the
container: [PowerAI Docker Container](https://hub.docker.com/r/ibmcom/powerai/).


# Private Modules

Private module files allow you to load your own installed software packages into your environment
and to handle different versions without getting into conflicts. Private modules can be setup for a
**single user** as well as **all users of a project group**. The workflow and settings for user
private as well as project private module files are described in the following.

## Setup

### 0. Build and Install Software

Obviously the first step is to build and install the software you'd like to use. Please follow the
instructions and tips provided on the page [building_software](building_software.md).
For consistency, we use the placeholder variable `<sw_name>` in this documentation. When following this
instructions, please substitute it with the actual software name within the commands.

### 1. Create Directory

Now, create the directory `privatemodules` to store all your private module files and the directory
`sw_name` therein. All module files for different versions or build options of `sw_name` should be
located in this directory.

```console
marie@compute$ cd $HOME
marie@compute$ mkdir --verbose --parents privatemodules/<sw_name>
marie@compute$ cd privatemodules/<sw_name>
```

Project private module files for software that can be used by all members of your group should be
located in your global projects directory, e.g., `/projects/p_number_crunch/privatemodules`. Thus, create
this directory:

```console
marie@compute$ mkdir --verbose --parents /projects/p_number_crunch/privatemodules/<sw_name>
marie@compute$ cd /projects/p_number_crunch/privatemodules/<sw_name>
```

!!! note

    Make sure, that the directory is group-readable.

### 2. Create Modulefile

Within the directory `<sw_name>` create the module file. The file can either be a TCL or a Lua. We
recommend to use Lua. The module file name should reflect the particular version of the software,
e.g., `1.4.lua`.

!!! note

    If you create a group private module file, make sure it is group-readable.

A template module file is:

```lua linenums="1"
help([[

Description
===========
<sw_name> is ...

More Information
================
For detailed instructions, go to:
   https://...

]])

whatis("Version: 1.4")
whatis("Keywords: [System, Utility, ...]")
whatis("URL: <...>")
whatis("Description: <...>")

conflict("<sw_name>")

local root = "</path/to/installation>"
prepend_path( "PATH",            pathJoin(root, "bin"))
prepend_path( "LD_LIBRARY_PATH", pathJoin(root, "lib"))
prepend_path( "LIBRARY_PATH", pathJoin(root, "lib"))

setenv(       "<SOME_ENV>",        "<value>")
```

The most important functions to adjust the environment are listed and described in the following
table.

| Function | Description |
|----------|-------------|
| `help([[ help string ]]) ` | Message when the help command is called. |
| `conflict(“name1”, “name2”)` | The current modulefile will only load if all listed modules are NOT loaded. |
| `depends_on(“pkgA”, “pkgB”, “pkgC”)` | Loads all modules. When unloading only dependent modules are unloaded. |
| `load(“pkgA”, “pkgB”, “pkgC”)` | Load all modules. Report error if unable to load.
| `prepend_path(”PATH”, “/path/to/pkg/bin”)` | Prepend the value to a path-like variable. |
| `setenv(“NAME”, “value”):` | Assign the value to the environment variable `NAME`. |

Please refer to the official documentation of Lmod on
[writing modules](https://lmod.readthedocs.io/en/latest/015_writing_modules.html) and
[Lua Modulefile functions](https://lmod.readthedocs.io/en/latest/050_lua_modulefiles.html)
for detailed information.
You can also have a look at present module files at the system.

## Usage

In order to use private module files and the corresponding software, you need to expand the module
search path. This is done by invoking the command

```console
marie@login$ module use $HOME/privatemodules
```

for your private module files and

```console
marie@login$ module use /projects/p_number_crunch/privatemodules
```

for group private module files, respectively.

Afterwards, you can use the [module commands](modules.md) to, e.g., load and unload your private modules
as usual.

## Caveats

An automated backup system provides security for the home directories on the cluster on a daily
basis. This is the reason why we urge users to store (large) temporary data (like checkpoint files)
on the `/scratch` filesystem or at local scratch disks.

This is also why we have set `ulimit -c 0` as a default setting to prevent users from filling the
home directories with dumps of crashed programs. In particular, `ulimit -c 0` sets the core file
size (blocks) to 0, which disables creation of core dumps in case an application crashes.

!!! note "Enable core files for debugging"

    If you use `bash` as shell and you need these core files for analysis, set `ulimit -c
    unlimited`.


# Python Virtual Environments

Virtual environments allow users to install additional Python packages and
create an isolated run-time environment. We recommend using `virtualenv` for
this purpose. In your virtual environment, you can use packages from the
[modules list](modules.md) or if you didn't find what you need you can install
required packages with the command: `pip install`. With the command
`pip list`, you can see a list of all installed packages and their versions.

!!! warning

    Note that you cannot install additional Python packages with `pip`
    without an activated virtual environment on our systems.
    Doing so will abort with the following error error:
    > ERROR: Could not find an activated virtualenv (required).

There are two methods of how to work with virtual environments on ZIH systems:

1. **virtualenv** is a standard Python tool to create isolated Python
environments. It is the preferred interface for managing installations and
virtual environments on ZIH system and part of the Python modules.

2. **conda** is an alternative method for managing installations and
virtual environments on ZIH system. conda is an open-source package
management system and environment management system from Anaconda. The
conda manager is included in all versions of Anaconda and Miniconda.

!!! warning

    Keep in mind that you **cannot** use virtualenv for working
    with the virtual environments previously created with conda tool and
    vice versa! Prefer virtualenv whenever possible.

## Python Virtual Environment

This example shows how to start working with **virtualenv** and Python virtual
environment (using the module system).

!!! hint

    We recommend to use [workspaces](../data_lifecycle/workspaces.md) for your
    virtual environments.

At first, we check available Python modules and load the preferred version:

```console
[marie@login.barnard ~]$ module load release/23.10 GCCcore/11.3.0
Module GCCcore/11.3.0 loaded.
[marie@login.barnard ~]$ module avail Python # check available Python modules

---------------------------- Software build with Compiler GCCcore version 11.3.0 (HMNS Level Two) -----------------------------
   flatbuffers-python/2.0    pkgconfig/1.5.5-python    protobuf-python/4.21.9 (D)    Python/3.10.4-bare
   IPython/8.5.0             protobuf-python/3.19.4    Python/2.7.18-bare            Python/3.10.4      (D)

  Where:
   D:  Default Module
   *Module:  Some Toolchain, load to access other modules that depend on it
   >Module:  Recommended toolchain version, load to access other modules that depend on it

[marie@login.barnard ~]$ module load Python # load default Python
Module Python/3.10.4 and 11 dependencies loaded.
[marie@login.barnard ~]$ which python # check with version you are Python version you are using
/software/rapids/r23.10/Python/3.10.4-GCCcore-11.3.0/bin/python
```

Then create the virtual environment and activate it.

```console
[marie@login.barnard ~]$ ws_allocate python_virtual_environment 1
Info: creating workspace.
/data/horse/ws/marie-python_virtual_environment
remaining extensions  : 10
remaining time in days: 1
[marie@login.barnard ~]$ python3 -m venv --system-site-package /data/horse/ws/marie-python_virtual_environment # create a Python virtual environment
[marie@login.barnard ~]$ source /data/horse/ws/marie-python_virtual_environment/bin/activate
(marie-python_virtual_environment) [marie@login.barnard ~]$ python --version
Python 3.10.4
```

Now you can work in this isolated environment, without interfering with other
tasks running on the system. Note that the inscription (env) at the beginning of
each line represents that you are in the virtual environment. You can deactivate
the environment as follows:

```console
(env) marie@compute$ deactivate    #Leave the virtual environment
```

??? example

    This is an example on cluster `Alpha`. The example creates a python virtual environment, and
    installs the package `torchvision` with pip.
    ```console
    marie@login.alpha$ srun --nodes=1 --gres=gpu:1 --time=01:00:00 --pty bash
    marie@alpha$ ws_allocate my_python_virtualenv 100    # use a workspace for the environment
    marie@alpha$ cd /data/horse/ws/marie-my_python_virtualenv
    [marie@alpha ]$ module load release/23.04 GCC/11.3.0 OpenMPI/4.1.4 CUDAcore/11.5.1 PyTorch/1.12.1
    Module GCC/11.3.0, OpenMPI/4.1.4, CUDAcore/11.5.1, PyTorch/1.12.1 and 58 dependencies loaded.
    [marie@alpha ]$ which python
    /software/rome/r23.04/Python/3.10.4-GCCcore-11.3.0/bin/python
    [marie@alpha ]$ pip list
    [...]
    [marie@alpha ]$ python -m venv --system-site-packages my-torch-env
    created virtual environment CPython3.10.4.final.0-64 in 3621ms
      creator CPython3Posix(dest=/data/horse/ws/marie/marie-my_python_virtualenv/my-torch-env, clear=False, no_vcs_ignore=False, global=True)
      seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/h0/marie/.local/share/virtualenv)
        added seed packages: pip==23.3.2, setuptools==69.0.3, wheel==0.42.0
      activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
    [marie@alpha ]$ source my-torch-env/bin/activate
    (my-torch-env) [marie@alpha ]$ pip install torchvision==0.12.0
    Collecting torchvision==0.12.0
    [...]
    Successfully installed torch-1.11.0 torchvision-0.12.0
    [...]
    (my-torch-env) marie@alpha$ python -c "import torchvision; print(torchvision.__version__)"
    0.10.0+cu102
    (my-torch-env) marie@alpha$ deactivate
    ```

### Persistence of Python Virtual Environment

To persist a virtualenv, you can store the names and versions of installed
packages in a file. Then you can restore this virtualenv by installing the
packages from this file. Use the `pip freeze` command for storing:

```console
(env) marie@compute$ pip freeze > requirements.txt    #Store the currently installed packages
```

In order to recreate python virtual environment, use the `pip install` command to install the
packages from the file:

```console
marie@compute$ module load Python    #Load default Python
[...]
marie@compute$ python -m venv --system-site-packages /data/horse/ws/marie-python_virtual_environment/env_post  #Create virtual environment
[...]
marie@compute$ source /data/horse/ws/marie-python_virtual_environment/env/bin/activate    #Activate virtual environment. Example output: (env_post) bash-4.2$
(env_post) marie@compute$ pip install -r requirements.txt    #Install packages from the created requirements.txt file
```

## Conda Virtual Environment

!!!
    We were informed that the manufacturer of Anaconda has changed its license conditions
    and that the use of Anaconda/conda is also subject to licensing at universities
    with more than 200 employees.
    https://legal.anaconda.com/policies/en/?name=terms-of-service#anaconda-terms-of-service
    The TU Dresden does not plan to procure a license centrally.

**Prerequisite:** Before working with conda, your shell needs to be configured
initially. Therefore login to the ZIH system, load the Anaconda module and run
`source $EBROOTANACONDA3/etc/profile.d/conda.sh`. Note that you must run the
previous command each time you want to activate your virtual environment and
they are not automatically loaded after re-opening your shell.

!!! warning
    We recommend to **not** use the `conda init` command, since it may cause unexpected behavior
    when working with the ZIH system.

??? example

    ```console
    marie@compute$ module load Anaconda3    #load Anaconda module
    Module Anaconda3/2019.03 loaded.
    marie@compute$ sh $EBROOTANACONDA3/etc/profile.d/conda.sh    #init conda
    [...]
    ```

This example shows how to start working with **conda** and virtual environment
(with using module system). At first, we use an interactive job and create a
directory for the conda virtual environment:

```console
marie@compute$ ws_allocate conda_virtual_environment 1
Info: creating workspace.
/data/horse/ws/marie-conda_virtual_environment
[...]
```

Then, we load Anaconda, create an environment in our directory and activate the
environment:

```console
marie@compute$ module load Anaconda3    #load Anaconda module
marie@compute$ conda create --prefix /data/horse/ws/marie-conda_virtual_environment/conda-env python=3.6    #create virtual environment with Python version 3.6
marie@compute$ conda activate /data/horse/ws/marie-conda_virtual_environment/conda-env    #activate conda-env virtual environment
```

Now you can work in this isolated environment, without interfering with other
tasks running on the system. Note that the inscription (conda-env) at the
beginning of each line represents that you are in the virtual environment. You
can deactivate the conda environment as follows:

```console
(conda-env) marie@compute$ conda deactivate    #Leave the virtual environment
```

!!! warning
    When installing conda packages via `conda install`, ensure to have enough main memory requested
    in your job allocation.

!!! hint
    We do not recommend to use conda environments together with EasyBuild modules due to
    dependency conflicts. Nevertheless, if you need EasyBuild modules, consider installing conda
    packages via `conda install --no-deps [...]` to prevent conda from installing dependencies.

??? example

    This is an example on cluster `Alpha`. The example creates a conda virtual environment, and
    installs the package `torchvision` with conda.
    ```console
    marie@login.alpha$ srun --nodes=1 --gres=gpu:1 --time=01:00:00 --pty bash
    marie@alpha$ ws_allocate my_conda_virtualenv 100    # use a workspace for the environment
    marie@alpha$ cd /data/horse/ws/marie-my_conda_virtualenv
    marie@alpha$ module load Anaconda3
    Module Anaconda3/2022.05 loaded.
    marie@alpha$ conda create --prefix my-torch-env python=3.8
    Collecting package metadata (current_repodata.json): done
    Solving environment: done
    [...]
    Proceed ([y]/n)? y
    [...]
    marie@alpha$ source $EBROOTANACONDA3/etc/profile.d/conda.sh
    marie@alpha$ conda activate my-torch-env
    (my-torch-env) marie@alpha$ conda install -c pytorch torchvision
    Collecting package metadata (current_repodata.json): done
    [...]
    Preparing transaction: done
    Verifying transaction: done
    (my-torch-env) marie@alpha$ which python    # ensure to use the correct Python
    (my-torch-env) marie@alpha$ python -c "import torchvision; print(torchvision.__version__)"
    0.12.0
    (my-torch-env) marie@alpha$ conda deactivate
    ```

### Persistence of Conda Virtual Environment

To persist a conda virtual environment, you can define an `environments.yml`
file. Have a look a the [conda docs](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html?highlight=environment.yml#create-env-file-manually)
for a description of the syntax. See an example for the `environments.yml` file
below.

??? example
    ```yml
    name: workshop_env
    channels:
    - conda-forge
    - defaults
    dependencies:
    - python>=3.7
    - pip
    - colorcet
    - 'geoviews-core=1.8.1'
    - 'ipywidgets=7.6.*'
    - geopandas
    - hvplot
    - pyepsg
    - python-dotenv
    - 'shapely=1.7.1'
    - pip:
        - python-hll
    ```

After specifying the `name`, the conda [channel priority](https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/channels.html)
is defined. In the example above, packages will be first installed from the
`conda-forge` channel, and if not found, from the `default` Anaconda channel.

Below, dependencies can be specified. Optionally, <abbr title="Pinning is a
process that allows you to remain on a stable release while grabbing packages
from a more recent version."> pinning</abbr> can be used to delimit the packages
installed to compatible package versions.

Finally, packages not available on conda can be specified (indented) below
`- pip:`

Recreate the conda virtual environment with the packages from the created
`environment.yml` file:

```console
marie@compute$ mkdir /data/horse/ws/marie-conda_virtual_environment/conda-env    #Create directory for environment
marie@compute$ module load Anaconda3    #Load Anaconda
marie@compute$ conda config --set channel_priority strict
marie@compute$ conda env create --prefix /data/horse/ws/marie-conda_virtual_environment/conda-env --file environment.yml    #Create conda env in directory with packages from environment.yml file
```


# Neural Networks with PyTorch

[PyTorch](https://pytorch.org/) is an open-source machine learning framework.
It is an optimized tensor library for deep learning using GPUs and CPUs.
PyTorch is a machine learning tool developed by Facebook's AI division to process large-scale
object detection, segmentation, classification, etc.
PyTorch provides a core data structure, the tensor, a multi-dimensional array that shares many
similarities with NumPy arrays.

Please check the software modules list via

```console
marie@login$ module spider pytorch
```

to find out, which PyTorch modules are available.

We recommend using the cluster `Alpha`, `Capella` and/or `Power9` when
 working with machine learning workflows and the PyTorch library.
You can find detailed hardware specification in our
[hardware documentation](../jobs_and_resources/hardware_overview.md).

## PyTorch Console

On the cluster `alpha`, load the module environment:

```console
# Job submission on alpha nodes with 1 gpu on 1 node with 800 Mb per CPU

marie@login.alpha$ srun --gres=gpu:1 -n 1 -c 7 --pty --mem-per-cpu=800 bash
marie@alpha$ module load release/23.04 GCCcore/11.3.0 GCC/11.3.0 OpenMPI/4.1.4 Python/3.10.4
Module GCC/11.3.0, OpenMPI/4.1.4, Python/3.10.4 and 21 dependencies loaded.
marie@alpha$ module load PyTorch/1.12.1-CUDA-11.7.0
Module PyTorch/1.12.1-CUDA-11.7.0 and 42 dependencies loaded.
```

??? hint "Torchvision on the cluster `alpha`"

    On the cluster `alpha`, the module torchvision is not yet available within the module
    system. (19.08.2021)
    Torchvision can be made available by using a virtual environment:

    ```console
    marie@alpha$ virtualenv --system-site-packages python-environments/torchvision_env
    marie@alpha$ source python-environments/torchvision_env/bin/activate
    marie@alpha$ pip install torchvision --no-deps
    ```

    Using the **--no-deps** option for "pip install" is necessary here as otherwise the PyTorch
    version might be replaced and you will run into trouble with the CUDA drivers.

On the cluster `Power9`:

```console
# Job submission in power nodes with 1 gpu on 1 node with 800 Mb per CPU
marie@login.power9$ srun --gres=gpu:1 -n 1 -c 7 --pty --mem-per-cpu=800 bash
```

After calling

```console
marie@login.power9$ module spider pytorch
```

we know that we can load PyTorch (including torchvision) with

```console
marie@power9$ module load release/23.04  GCC/11.3.0  OpenMPI/4.1.4 torchvision/0.13.1
Modules GCC/11.3.0, OpenMPI/4.1.4, torchvision/0.13.1 and 62 dependencies loaded.
```

Now, we check that we can access PyTorch:

```console
marie@power9 python -c "import torch; print(torch.__version__)"
```

The following example shows how to create a python virtual environment and import PyTorch.

```console
# Create folder
marie@power9$ mkdir python-environments
# Check which python are you using
marie@power9$ which python
/sw/installed/Python/3.7.4-GCCcore-8.3.0/bin/python
# Create virtual environment "env" which inheriting with global site packages
marie@power9$ virtualenv --system-site-packages python-environments/env
[...]
# Activate virtual environment "env". Example output: (env) bash-4.2$
marie@power9$ source python-environments/env/bin/activate
marie@power9$ python -c "import torch; print(torch.__version__)"
```

## PyTorch in JupyterHub

In addition to using interactive and batch jobs, it is possible to work with PyTorch using
JupyterHub.  The production and test environments of JupyterHub contain Python kernels, that come
with a PyTorch support.

![PyTorch module in JupyterHub](misc/Pytorch_jupyter_module.png)
{: align="center"}

## Distributed PyTorch

For details on how to run PyTorch with multiple GPUs and/or multiple nodes, see
[distributed training](distributed_training.md).

## Migrate PyTorch-script from CPU to GPU

It is recommended to use GPUs when using large training data sets. While TensorFlow automatically
uses GPUs if they are available, in PyTorch you have to move your tensors manually.

First, you need to import `torch.CUDA`:

```python3
import torch.CUDA
```

Then you define a `device`-variable, which is set to 'CUDA' automatically when a GPU is available
with this code:

```python3
device = torch.device('CUDA' if torch.CUDA.is_available() else 'cpu')
```

You then have to move all of your tensors to the selected device. This looks like this:

```python3
x_train = torch.FloatTensor(x_train).to(device)
y_train = torch.FloatTensor(y_train).to(device)
```

Remember that this does not break backward compatibility when you port the script back to a computer
without GPU, because without GPU, `device` is set to 'cpu'.

### Caveats

#### Moving Data Back to the CPU-Memory

The CPU cannot directly access variables stored on the GPU. If you want to use the variables, e.g.,
in a `print` statement or when editing with NumPy or anything that is not PyTorch, you have to move
them back to the CPU-memory again. This then may look like this:

```python3
cpu_x_train = x_train.cpu()
print(cpu_x_train)
...
error_train = np.sqrt(metrics.mean_squared_error(y_train[:,1].cpu(), y_prediction_train[:,1]))
```

Remember that, without `.detach()` before the CPU, if you change `cpu_x_train`, `x_train` will also
be changed.  If you want to treat them independently, use

```python3
cpu_x_train = x_train.detach().cpu()
```

Now you can change `cpu_x_train` without `x_train` being affected.

#### Speed Improvements and Batch Size

When you have a lot of very small data points, the speed may actually decrease when you try to train
them on the GPU.  This is because moving data from the CPU-memory to the GPU-memory takes time. If
this occurs, please try using a very large batch size. This way, copying back and forth only takes
places a few times and the bottleneck may be reduced.


# Record Course of Events with Score-P

The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for
profiling, event tracing, and online analysis of HPC applications. Currently, it works with the
analysis tools Vampir, Scalasca, and Tau. Score-P supports lots of features e.g.

* MPI, SHMEM, OpenMP, Pthreads, and hybrid programs
* Manual source code instrumentation
* Monitoring of CUDA, OpenCL, and OpenACC applications
* Recording hardware counter by using PAPI library
* Function filtering and grouping

Only the basic usage is shown in this Wiki. For a comprehensive Score-P user manual refer to the
[Score-P website](https://score-p.org/).

Before using Score-P, set up the correct environment with

```console
marie@login$ module load Score-P
```

To make measurements with Score-P, the user's application program needs to be instrumented, i.e., at
specific important points ("events") Score-P measurement calls have to be activated. By default,
Score-P handles this automatically. In order to enable instrumentation of function calls, MPI as
well as OpenMP events, the user only needs to prepend the Score-P wrapper to the usual compile and
link commands. The following sections show some examples depending on the parallelization type of
the program.

## Serial Programs

Original:

```console
marie@login$ ifort a.f90 b.f90 -o myprog
```

With instrumentation:

```console
marie@login$ scorep ifort a.f90 b.f90 -o myprog
```

This will instrument user functions (if supported by the compiler) and link the Score-P library.

## MPI Parallel Programs

If your MPI implementation uses MPI compilers, Score-P will detect MPI parallelization
automatically:

Original:

```console
marie@login$ mpicc hello.c -o hello
```

With instrumentation:

```console
marie@login$ scorep mpicc hello.c -o hello
```

MPI implementations without own compilers require the user to link the MPI library
manually. Even in this case, Score-P will detect MPI parallelization automatically:

Original:

```console
marie@login$ icc hello.c -o hello -lmpi
```

With instrumentation:

```console
marie@login$ scorep icc hello.c -o hello -lmpi
```

However, if Score-P fails to detect MPI parallelization automatically you can manually select MPI
instrumentation:

Original:

```console
marie@login$ icc hello.c -o hello -lmpi
```

With instrumentation:

```console
marie@login$ scorep --mpp=mpi icc hello.c -o hello -lmpi
```

If you want to instrument MPI events only (creates less overhead and smaller trace files) use the
option `--nocompiler` to disable automatic instrumentation of user functions.

## OpenMP Parallel Programs

When Score-P detects OpenMP flags on the command line, OPARI2 is invoked for automatic source code
instrumentation of OpenMP events:

Original:

```console
marie@login$ ifort -openmp pi.f -o pi
```

With instrumentation:

```console
marie@login$ scorep ifort -openmp pi.f -o pi
```

## Hybrid MPI/OpenMP Parallel Programs

With a combination of the above mentioned approaches, hybrid applications can be instrumented:

Original:

```console
marie@login$ mpif90 -openmp hybrid.F90 -o hybrid
```

With instrumentation:

```console
marie@login$ scorep mpif90 -openmp hybrid.F90 -o hybrid
```

## Score-P Instrumenter Option Overview

| Type of instrumentation | Instrumenter switch | Default value | Runtime measurement control |
| --- | --- | --- | --- |
| MPI | `--mpp=mpi` | (auto) | (see Sec. Selection of MPI Groups ) |
| SHMEM | `--mpp=shmem` | (auto) | - |
| OpenMP | `--thread=omp` | (auto) | - |
| Pthread | `--thread=pthread` | (auto) | - |
| Compiler (see Sec. Automatic Compiler Instrumentation ) | `--compiler/--nocompiler` | enabled | Filtering (see Sec. Filtering ) |
| PDT instrumentation (see Sec. Source-Code Instrumentation Using PDT ) | `--pdt/--nopdt` | disabled | Filtering (see Sec. Filtering)|
| POMP2 user regions (see Sec. Semi-Automatic Instrumentation of POMP2 User Regions ) | `--pomp/--nopomp` | depends on OpenMP usage | Filtering (see Sec. Filtering ) |
| Manual (see Sec. Manual Region Instrumentation ) | `--user/--nouser` | disabled | Filtering (see Sec. Filtering ) and selective recording (see Sec. Selective Recording ) |

## Application Measurement

After the application run, you will find an experiment directory in your current working directory,
which contains all recorded data.  In general, you can record a profile and/or a event trace.
Whether a profile and/or a trace is recorded, is specified by the environment variables
`SCOREP_ENABLE_PROFILING` and `SCOREP_ENABLE_TRACING` (see
[official Score-P documentation](https://perftools.pages.jsc.fz-juelich.de/cicd/scorep/tags/latest/html/measurement.html)).
If the value of this variables is zero or false, profiling/tracing is disabled. Otherwise Score-P
will record a profile and/or trace. By default, profiling is enabled and tracing is disabled. For
more information please see the list of Score-P measurement
[configuration variables](https://perftools.pages.jsc.fz-juelich.de/cicd/scorep/tags/latest/html/scorepmeasurementconfig.html).

You may start with a profiling run, because of its lower space requirements. According to profiling
results, you may configure the trace buffer limits, filtering or selective recording for recording
traces.  Score-P allows to configure several parameters via environment variables. After the
measurement run you can find a `scorep.cfg` file in your experiment directory which contains the
configuration of the measurement run. If you had not set configuration values explicitly, the file
will contain the default values.


# Singularity for Power9 Architecture

!!! note "Root privileges"

    Building Singularity containers from a recipe on ZIH system is normally not possible due to the
    requirement of root (administrator) rights, see [Containers](containers.md). For obvious reasons
    users cannot be granted root permissions.

The solution is to build your container on your local Linux workstation using Singularity and copy
it to ZIH systems for execution.

**This does not work on the cluster `power`** as it uses the Power9 architecture which your
workstation likely doesn't.

For this, we provide a Virtual Machine (VM) on the cluster `power` which allows users to gain root
permissions in an isolated environment. The workflow to use this manually is described for
[virtual machines](virtual_machines.md) but is quite cumbersome.

To make this easier, two programs are provided: `buildSingularityImage` and `startInVM`, which do
what they say. The latter is for more advanced use cases, so you should be fine using
`buildSingularityImage`, see the following section.

!!! note "SSH key without password"

    You need to have your default SSH key without a password for the scripts to work as
    entering a password through the scripts is not supported.

**The recommended workflow** is to create and test a definition file locally. You usually start from
a base Docker container. Those typically exist for different architectures but with a common name
(e.g.  `ubuntu:18.04`). Singularity automatically uses the correct Docker container for your current
architecture when building. So, in most cases, you can write your definition file, build it and test
it locally, then move it to ZIH systems and build it on Power9 (cluster `power`) without any further
changes. However, sometimes Docker containers for different architectures have different suffixes,
in which case you'd need to change that when moving to ZIH systems.

## Build a Singularity Container in a Job

To build a Singularity container for the Power9 architecture on ZIH systems simply run:

```console
marie@login.power$ buildSingularityImage --arch=power9 myContainer.sif myDefinition.def
```

To build a singularity image on the x86-architecture, run:

```console
marie@login.power$ buildSingularityImage --arch=x86 myContainer.sif myDefinition.def
```

These commands will submit a batch job and immediately return. If you want it to block while the
image is built and see live output, add the option `--interactive`:

```console
marie@login.power$ buildSingularityImage --arch=power9 --interactive myContainer.sif myDefinition.def
```

There are more options available, which can be shown by running `buildSingularityImage --help`. All
have reasonable defaults. The most important ones are:

* `--time <time>`: Set a higher job time if the default time is not
  enough to build your image and your job is canceled before completing. The format is the same as
  for Slurm.
* `--tmp-size=<size in GB>`: Set a size used for the temporary
  location of the Singularity container, basically the size of the extracted container.
* `--output=<file>`: Path to a file used for (log) output generated
  while building your container.
* Various Singularity options are passed through. E.g.
  `--notest, --force, --update`. See, e.g., `singularity --help` for details.

For **advanced users**, it is also possible to manually request a job with a VM (`srun -p power9
--cloud=kvm ...`) and then use this script to build a Singularity container from within the job. In
this case, the `--arch` and other Slurm related parameters are not required. The advantage of using
this script is that it automates the waiting for the VM and mounting of host directories into it
(can also be done with `startInVM`) and creates a temporary directory usable with Singularity inside
the VM controlled by the `--tmp-size` parameter.

## Filesystem

**Read here if you have problems like "File not found".**

As the build starts in a VM, you may not have access to all your files. It is usually bad practice
to refer to local files from inside a definition file anyway as this reduces reproducibility.
However, common directories are available by default. For others, care must be taken. In short:

* `/home/$USER`, `/data/horse/$USER` are available and should be used `/data/horse/<group>` also
   works for all groups the users is in
* `/projects/<group>` similar, but is read-only! So don't use this to store your generated
   container directly, but rather move it here afterwards
* `/tmp` is the VM local temporary directory. All files put here will be lost!

If the current directory is inside (or equal to) one of the above (except `/tmp`), then relative
paths for container and definition work as the script changes to the VM equivalent of the current
directory.  Otherwise, you need to use absolute paths. Using `~` in place of `$HOME` does work too.

Under the hood, the filesystem of ZIH systems is mounted via SSHFS at `/host_data`. So if you need
any other files, they can be found there.

There is also a new SSH key named `kvm` which is created by the scripts and authorized inside the VM
to allow for password-less access to SSHFS. This is stored at `~/.ssh/kvm` and regenerated if it
does not exist. It is also added to `~/.ssh/authorized_keys`. Note that removing the key file does
not remove it from `authorized_keys`, so remove it manually if you need to. It can be easily
identified by the comment on the key. However, removing this key is **NOT** recommended, as it
needs to be re-generated on every script run.

## Start a Job in a VM

Especially when developing a Singularity definition file, it might be useful to get a shell directly
on a VM. To do so on the Power9 architecture, simply run:

```console
startInVM --arch=power9
```

To do so on the x86-architecture, run:

```console
startInVM --arch=x86
```

This will execute an `srun` command with the `--cloud=kvm` parameter, wait till the VM is ready,
mount all folders (just like `buildSingularityImage`, see the Filesystem section above) and come
back with a bash inside the VM. Inside that you are root, so you can directly execute `singularity
build` commands.

As usual, more options can be shown by running `startInVM --help`, the most important one being
`--time`.

There are two special use cases for this script:

1. Execute an arbitrary command inside the VM instead of getting a bash by appending the command to
   the script.
   Example: `startInVM --arch=power9 singularity build ~/myContainer.sif  ~/myDefinition.de`
1. Use the script in a job manually allocated via srun/sbatch. This will work the same as when
   running outside a job but will **not** start a new job. This is useful for using it inside batch
   scripts, when you already have an allocation or need special arguments for the job system. Again,
   you can run an arbitrary command by passing it to the script.


# Singularity Recipes and Hints

## Example Definitions

### Basic Example

A usual workflow to create Singularity Definition consists of the following steps:

* Start from base image
* Install dependencies
    * Package manager
    * Other sources
* Build and install own binaries
* Provide entry points and metadata

An example doing all this:

```bash
Bootstrap: docker
From: alpine

%post
  . /.singularity.d/env/10-docker*.sh

  apk add g++ gcc make wget cmake

  wget https://github.com/fmtlib/fmt/archive/5.3.0.tar.gz
  tar -xf 5.3.0.tar.gz
  mkdir build && cd build
  cmake ../fmt-5.3.0 -DFMT_TEST=OFF
  make -j$(nproc) install
  cd ..
  rm -r fmt-5.3.0*

  cat <<'EOF' >>  hello.cpp

#include <fmt/format.h>  // literal

int main(int argc, char** argv){
  if(argc == 1) fmt::print("No arguments passed!\n");
  else fmt::print("Hello {}!\n", argv[1]);
}
EOF

  g++ hello.cpp -o hello -lfmt
  mv hello /usr/bin/hello

%runscript
  hello "$@"

%labels
  Author Alexander Grund
  Version 1.0.0

%help
  Display a greeting using the fmt library

  Usage:
    ./hello
```

### Distributed memory

#### MPICH

Ubuntu+MPICH definition file:

```bash
Bootstrap: docker
From: ubuntu:20.04

%files
    mpitest.c /opt

%environment
    export MPICH_DIR=/opt/mpich
    export SINGULARITY_MPICH_DIR=${MPICH_DIR}
    export SINGULARITYENV_APPEND_PATH=${MPICH_DIR}/bin
    export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH=${MPICH_DIR}/lib

%post
    echo "Installing required packages..."
    apt-get update && apt-get install -y wget git bash gcc gfortran g++ make file

    # required for F90 bindings
    apt-get install -y python3

    echo "Installing MPICH"
    export MPICH_DIR=/opt/mpich
    export MPICH_VERSION=4.1
    export MPICH_URL="https://www.mpich.org/static/downloads/${MPICH_VERSION}/mpich-${MPICH_VERSION}.tar.gz"
    mkdir -p /tmp/mpich
    mkdir -p /opt
    # Download
    cd /tmp/mpich && wget -O mpich-${MPICH_VERSION}.tar.gz ${MPICH_URL} && tar -xf mpich-${MPICH_VERSION}.tar.gz

    # Configure and compile/install
    cd /tmp/mpich/mpich-${MPICH_VERSION}
    ./configure --prefix=${MPICH_DIR} && make install


    # Set env variables so we can compile our application
    export PATH=${MPICH_DIR}/bin:${PATH}
    export LD_LIBRARY_PATH=${MPICH_DIR}/lib:${LD_LIBRARY_PATH}
    export MANPATH=${MPICH_DIR}/share/man:${MANPATH}


    echo "Compiling the MPI application..."
    cd /opt && mpicc -o mpitest mpitest.c
```

At your local machine:

```console
marie@local$ sudo singularity build ubuntu_mpich.sif ubuntu_mpich.def
```

This will create the `ubuntu_mpich.sif` file that you have to copy to HPC system.

At the HPC system run as following:

```console
marie@login$ srun ---nodes=4 --ntasks-per-node=2 --time=00:10:00 singularity exec ubuntu_mpich.sif /opt/mpitest
```

### CUDA + CuDNN + Open MPI

* Chosen CUDA version depends on installed driver of host
* Open MPI needs PMI for Slurm integration
* Open MPI needs CUDA for GPU copy-support
* Open MPI needs `ibverbs` library for InfiniBand
* `openmpi-mca-params.conf` required to avoid warnings on fork (OK on ZIH systems)
* Environment variables `SLURM_VERSION` and `OPENMPI_VERSION` can be set to  choose different
  version when building the container

```bash
Bootstrap: docker
From: nvidia/cuda-ppc64le:10.1-cudnn7-devel-ubuntu18.04

%labels
    Author ZIH
    Requires CUDA driver 418.39+.

%post
    . /.singularity.d/env/10-docker*.sh

    apt-get update
    apt-get install -y cuda-compat-10.1
    apt-get install -y libibverbs-dev ibverbs-utils
    # Install basic development tools
    apt-get install -y gcc g++ make wget python
    apt-get autoremove; apt-get clean

    cd /tmp

    : ${SLURM_VERSION:=17-02-11-1}
    wget https://github.com/SchedMD/slurm/archive/slurm-${SLURM_VERSION}.tar.gz
    tar -xf slurm-${SLURM_VERSION}.tar.gz
        cd slurm-slurm-${SLURM_VERSION}
        ./configure --prefix=/usr/ --sysconfdir=/etc/slurm --localstatedir=/var --disable-debug
        make -C contribs/pmi2 -j$(nproc) install
    cd ..
    rm -rf slurm-*

    : ${OPENMPI_VERSION:=3.1.4}
    wget https://download.open-mpi.org/release/open-mpi/v${OPENMPI_VERSION%.*}/openmpi-${OPENMPI_VERSION}.tar.gz
    tar -xf openmpi-${OPENMPI_VERSION}.tar.gz
    cd openmpi-${OPENMPI_VERSION}/
    ./configure --prefix=/usr/ --with-pmi --with-verbs --with-cuda
    make -j$(nproc) install
    echo "mpi_warn_on_fork = 0" >> /usr/etc/openmpi-mca-params.conf
    echo "btl_openib_warn_default_gid_prefix = 0" >> /usr/etc/openmpi-mca-params.conf
    cd ..
    rm -rf openmpi-*
```

## Hints

### GUI (X11) Applications

Running GUI applications inside a singularity container is possible out of the box. Check the
following definition:

```Bash
Bootstrap: docker
From: centos:7

%post
yum install -y xeyes
```

This image may be run with

```console
singularity exec xeyes.sif xeyes.
```

This works because all the magic is done by Singularity already like setting `${DISPLAY}` to the
outside display and mounting `${HOME}` so `${HOME}/.Xauthority` (X11 authentication cookie) is
found. When you are using `--contain` or `--no-home` you have to set that cookie yourself or
mount/copy it inside the container. Similar for `--cleanenv` you have to set `${DISPLAY}`, e.g., via

```console
export SINGULARITY_DISPLAY=${DISPLAY}
```

When you run a container as root (via `sudo`) you may need to allow root for your local display
port: `xhost +local:root\`

### Hardware Acceleration

If you want hardware acceleration, you **may** need [VirtualGL](https://virtualgl.org). An example
definition file is as follows:

```Bash
Bootstrap: docker
From: centos:7

%post
yum install -y glx-utils # for glxgears example app

yum install -y curl
VIRTUALGL_VERSION=2.6.2 # Replace by required (e.g. latest) version

curl -sSL https://downloads.sourceforge.net/project/virtualgl/"${VIRTUALGL_VERSION}"/VirtualGL-"${VIRTUALGL_VERSION}".x86_64.rpm -o VirtualGL-"${VIRTUALGL_VERSION}".x86_64.rpm
yum install -y VirtualGL*.rpm
/opt/VirtualGL/bin/vglserver_config -config +s +f -t
rm VirtualGL-*.rpm

# Install video drivers AFTER VirtualGL to avoid them being overwritten
yum install -y mesa-dri-drivers # for e.g. intel integrated GPU drivers. Replace by your driver
```

You can now run the application with `vglrun`:

```console
singularity exec vgl.sif vglrun glxgears
```

!!! warning

    Using VirtualGL may not be required at all and could even decrease the performance.

To check install, e.g., `glxgears` as above and your graphics driver (or use the VirtualGL image
from above) and disable `vsync`:

```console
vblank_mode=0 singularity exec vgl.sif glxgears
```

Compare the FPS output with the `glxgears` prefixed by `vglrun` (see above) to see which produces more
FPS (or runs at all).

**NVIDIA GPUs** need the `--nv` parameter for the Singularity command:

```console
singularity exec --nv vgl.sif glxgears
```

### Singularity Temporary and Cache Directories

Singularity uses `/tmp` to store temporary data, e. g., when you use `pull` or `build`. Because
there are also diskless nodes, you may use the environment variable `SINGULARITY_TMPDIR` to change
the storage place for temporary data. You can use the following line to indicate a different
location for temporary data:

```console
export SINGULARITY_TMPDIR=$SOME_WORKSPACE/singularity_tmp
```

Singularity commands invoked afterwards store temporary data at the indicated location.

!!! warning "Singularity caches data in your home directory by default"

    Another location where Singularity stores data is your home directory! This is problematic when
    Singularity caches too many containers, because you cannot start Slurm jobs if you are over a
    certain limit. To prevent that from happening, indicate a different location for cache space,
    e. g., a [workspace](../data_lifecycle/workspaces.md):

    ```console
    export SINGULARITY_CACHEDIR=$SOME_WORKSPACE/singularity_cache
    ```

Further information on both environment variables can be found on
[the official Singularity documentation](https://docs.sylabs.io/guides/latest/user-guide/build_env.html#build-environment).


# Software Development and Tools

This section provides you with the basic knowledge and tools for software development
on the ZIH systems.
It will tell you:

- How to compile your code
    - [General advises for building software](building_software.md)
    - [Using compilers](compilers.md)
    - [GPU programming](gpu_programming.md)
- How to use libraries
    - [Using mathematical libraries](math_libraries.md)
- How to deal with (or even prevent) bugs
    - [Find caveats and hidden errors in MPI application codes](mpi_usage_error_detection.md)
    - [Using debuggers](debuggers.md)
- [How to investigate the performance and efficiency of your code](performance_engineering_overview.md)

!!! hint "Some general, helpful hints"

    - Stick to standards wherever possible, e.g. use the `-std` flag
      for CLANG, GNU and Intel C/C++ compilers. Computers are short living
      creatures, migrating between platforms can be painful. In addition,
      running your code on different platforms greatly increases the
      reliably. You will find many bugs on one platform that never will be
      revealed on another.
    - Compile your code with optimization, e.g. `-O2` will turn on a moderate level of optimization
      where most optimization algorithms are applied. Please refer to the specific documentation of
      your compiler of choice for detailed information.
    - Before and during performance tuning: Make sure that your code delivers the correct results.

!!! questions "Some questions you should ask yourself"

    - Given that a code is parallel, are the results independent from the
      numbers of threads or processes?
    - Have you ever run your Fortran code with array bound and subroutine argument checking (the
      `-check all` and `-traceback` flags for the Intel compilers)?
    - Have you checked that your code is not causing floating point exceptions?
    - Does your code work with a different link order of objects?
    - Have you made any assumptions regarding storage of data objects in memory?


# Compare System Performance with SPEChpc

SPEChpc 2021 is a benchmark suite developed by the Standard Performance Evaluation Corporation
(SPEC) for the evaluation of various, heterogeneous HPC systems. Documentation and released
benchmark results can be found on their [web page](https://www.spec.org/hpc2021/). In fact, our
system *Taurus* (partition `haswell`) is the benchmark's reference system and thus represents
the baseline score.

The tool includes nine real-world scientific applications (see
[benchmark table](https://www.spec.org/hpc2021/docs/result-fields.html#benchmarks))
with different workload sizes ranging from tiny, small, medium to large, and different
parallelization models including MPI only, MPI+OpenACC, MPI+OpenMP and MPI+OpenMP with target
offloading. With this benchmark suite you can compare the performance of different HPC systems and
furthermore, evaluate parallel strategies for applications on a target HPC system. When you e.g.
want to implement an algorithm, port an application to another platform or integrate acceleration
into your code, you can determine from which target system and parallelization model your
application performance could benefit most. Or this way you can check whether an acceleration scheme
can be deployed and run on a given system, since there could be software issues restricting a
capable hardware (see this [CUDA issue](#cuda-reduction-operation-error)).

Since TU Dresden is a member of the SPEC consortium, the HPC benchmarks can be requested by anyone
interested. Please contact
[Holger Brunst](https://tu-dresden.de/zih/die-einrichtung/struktur/holger-brunst) for access.

## Installation

The target partition determines which of the parallelization models can be used, and vice versa.
For example, if you want to run a model including acceleration, you would have to use a partition
with GPUs.

Once the target partition is determined, follow SPEC's
[Installation Guide](https://www.spec.org/hpg/hpc2021/Docs/install-guide-linux.html).
It is straight-forward and easy to use.

???+ tip "Building for partition `ml`"

    The partition `ml` is a Power9 architecture. Thus, you need to provide the `-e ppc64le` switch
    when installing.

???+ tip "Building with NVHPC for partition `alpha`"

    To build the benchmark for partition `alpha`, you don't need an interactive session
    on the target architecture. You can stay on the login nodes as long as you set the
    flag `-tp=zen`. You can add this compiler flag to the configuration file.

If you are facing errors during the installation process, check the [solved](#solved-issues) and
[unresolved issues](#unresolved-issues) sections for our systems. The problem might already be
listed there.

## Configuration

The behavior in terms of how to build, run and report the benchmark in a particular environment is
controlled by a configuration file. There are a few examples included in the source code.
Here you can apply compiler tuning and porting, specify the runtime environment and describe the
system under test. SPEChpc 2021 has been deployed on the partitions `haswell`, `ml` and
`alpha`. Configurations are available, respectively:

- [gnu-taurus.cfg](misc/spec_gnu-taurus.cfg)
- [nvhpc-ppc.cfg](misc/spec_nvhpc-ppc.cfg)
- [nvhpc-alpha.cfg](misc/spec_nvhpc-alpha.cfg)

No matter which one you choose as a starting point,
double-check the line that defines the submit command and make sure it says `srun [...]`, e.g.

```bash
submit = srun $command
```

Otherwise this can cause trouble (see [Slurm Bug](#slurm-bug)).
You can also put Slurm options in the configuration but it is recommended to do this in a job
script (see chapter [Execution](#execution)). Use the following to apply your configuration to the
benchmark run:

```
runhpc --config <configfile.cfg> [...]
```

For more details about configuration settings check out the following links:

- [Config Files Description](https://www.spec.org/hpc2021/Docs/config.html)
- [Flag Description](https://www.spec.org/hpc2021/results/res2021q4/hpc2021-20210917-00050.flags.html)
- [Result File Fields Description](https://www.spec.org/hpc2021/docs/result-fields.html)

## Execution

The SPEChpc 2021 benchmark suite is executed with the `runhpc` command, which also sets it's
configuration and controls it's runtime behavior. For all options, see SPEC's documentation about
[`runhpc` options](https://www.spec.org/hpc2021/Docs/runhpc.html).
First, execute `source shrc` in your SPEC installation directory. Then use a job script to submit a
job with the benchmark or parts of it.

In the following there are job scripts shown for partitions `haswell`, `ml` and `alpha`,
respectively. You can use them as a template in order to reproduce results or to transfer the
execution to a different partition.

- Replace `<p_number_crunch>` (line 2) with your project name
- Replace `ws=</scratch/ws/spec/installation>` (line 15/18) with your SPEC installation path

### Submit SPEChpc Benchmarks with a Job File

=== "submit_spec_haswell_mpi.sh"
    ```bash linenums="1"
    #!/bin/bash
    #SBATCH --account=<p_number_crunch>
    #SBATCH --partition=haswell64
    #SBATCH --exclusive
    #SBATCH --nodes=1
    #SBATCH --ntasks=24
    #SBATCH --cpus-per-task=1
    #SBATCH --mem-per-cpu=2541M
    #SBATCH --time=16:00:00
    #SBATCH --constraint=DA

    module purge
    module load gompi/2019a

    ws=</scratch/ws/spec/installation>
    cd ${ws}
    source shrc

    # reportable run with all benchmarks
    BENCH="tiny"

    runhpc --config gnu-taurus --define model=mpi --ranks=24 --reportable --tune=base --flagsurl=$SPEC/config/flags/gcc_flags.xml ${BENCH}
    ```

=== "submit_spec_ml_openacc.sh"
    ```bash linenums="1"
    #!/bin/bash
    #SBATCH --account=<p_number_crunch>
    #SBATCH --partition=ml
    #SBATCH --exclusive
    #SBATCH --nodes=1
    #SBATCH --ntasks=6
    #SBATCH --cpus-per-task=7
    #SBATCH --gpus-per-task=1
    #SBATCH --gres=gpu:6
    #SBATCH --mem-per-cpu=5772M
    #SBATCH --time=00:45:00
    #SBATCH --export=ALL
    #SBATCH --hint=nomultithread

    module --force purge
    module load modenv/ml NVHPC OpenMPI/4.0.5-NVHPC-21.2-CUDA-11.2.1

    ws=</scratch/ws/spec/installation>
    cd ${ws}
    source shrc

    export OMPI_CC=nvc
    export OMPI_CXX=nvc++
    export OMPI_FC=nvfortran

    suite='tiny ^pot3d_t'
    cfg=nvhpc_ppc.cfg

    # test run
    runhpc -I --config ${cfg} --ranks ${SLURM_NTASKS} --define pmodel=acc --size=test --noreportable --tune=base --iterations=1 ${suite}

    # reference run
    runhpc --config ${cfg} --ranks ${SLURM_NTASKS} --define pmodel=acc --rebuild --tune=base --iterations=3 ${suite}
    ```

=== "submit_spec_alpha_openacc.sh"
    ```bash linenums="1"
    #!/bin/bash
    #SBATCH --account=<p_number_crunch>
    #SBATCH --partition=alpha
    #SBATCH --exclusive
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=8
    #SBATCH --cpus-per-task=6
    #SBATCH --gpus-per-task=1
    #SBATCH --gres=gpu:8
    #SBATCH --mem-per-cpu=20624M
    #SBATCH --time=00:45:00
    #SBATCH --export=ALL
    #SBATCH --hint=nomultithread

    module --force purge
    module load modenv/hiera NVHPC OpenMPI

    ws=</scratch/ws/spec/installation>
    cd ${ws}
    source shrc

    suite='tiny'
    cfg=nvhpc_alpha.cfg

    # test run
    runhpc -I --config ${cfg} --ranks ${SLURM_NTASKS} --define pmodel=acc --size=test --noreportable --tune=base --iterations=1 ${suite}

    # reference workload
    runhpc --config ${cfg} --ranks ${SLURM_NTASKS} --define pmodel=acc --tune=base --iterations=3 ${suite}
    ```

## Solved Issues

### Fortran Compilation Error

!!! failure "PGF90-F-0004-Corrupt or Old Module file"

!!! note "Explanation"

    If this error arises during runtime, it means that the benchmark binaries and the MPI module
    do not fit together. This happens when you have built the benchmarks written in Fortran with a
    different compiler than which was used to build the MPI module that was loaded for the run.

!!! success "Solution"

    1. Use the correct MPI module
        - The MPI module in use must be compiled with the same compiler that was used to build the
        benchmark binaries. Check the results of `module avail` and choose a corresponding module.
    1. Rebuild the binaries
        - Rebuild the binaries using the same compiler as for the compilation of the MPI module of
        choice.
    1. Request a new module
        - Ask the HPC support to install a compatible MPI module.
    1. Build your own MPI module (as a last resort)
        - Download and build a private MPI module using the same compiler as for building the
        benchmark binaries.

### pmix Error

!!! failure "PMIX ERROR"

    ```bash
    It looks like the function `pmix_init` failed for some reason; your parallel process is
    likely to abort. There are many reasons that a parallel process can
    fail during pmix_init; some of which are due to configuration or
    environment problems. This failure appears to be an internal failure;

    mix_progress_thread_start failed
    --> Returned value -1 instead of PMIX_SUCCESS

    *** An error occurred in MPI_Init_thread
    *** on a NULL communicator
    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
    ***    and potentially your MPI job)
    ```

!!! note "Explanation"

    This is most probably a MPI related issue. If you built your own MPI module, PMIX support might
    be configured wrong.

!!! success "Solution"

    Use `configure --with-pmix=internal` during the `cmake` configuration routine.

### ORTE Error (too many processes)

!!! failure "Error: system limit exceeded on number of processes that can be started"

    ORTE_ERROR_LOG: The system limit on number of children a process can have was reached.

!!! note "Explanation"

    There are too many processes spawned, probably due to a wrong job allocation and/or invocation.

!!! success "Solution"

    Check the invocation command line in your job script. It must not say `srun runhpc [...]`
    there, but only `runhpc [...]`. The submit command in the [configuration](#configuration) file
    already contains `srun`. When `srun` is called in both places, too many parallel processes are
    spawned.

### Error with OpenFabrics Device

!!! warning "There was an error initializing an OpenFabrics device"

!!! note "Explanation"

    "I think it’s just trying to find the InfiniBand libraries, which aren’t used, but can’t.
    It’s probably safe to ignore."
    <p style='text-align: right;'> Matthew Colgrove, Nvidia </p>

!!! success "Solution"

    This is just a warning which cannot be suppressed, but can be ignored.

### Out of Memory

!!! failure "Out of memory"

    ```
    Out of memory allocating [...] bytes of device memory
    call to cuMemAlloc returned error 2: Out of memory
    ```

!!! note "Explanation"

    - When running on a single node with all of its memory allocated, there is not enough memory
    for the benchmark.
    - When running on multiple nodes, this might be a wrong resource distribution caused by Slurm.
    Check the `$SLURM_NTASKS_PER_NODE` environment variable. If it says something like `15,1` when
    you requested 8 processes per node, Slurm was not able to hand over the resource distribution
    to `mpirun`.

!!! success "Solution"

    - Expand your job from single node to multiple nodes.
    - Reduce the workload (e.g. form small to tiny).
    - Make sure to use `srun` instead of `mpirun` as the submit command in your
    [configuration](#configuration) file.

## Unresolved Issues

### CUDA Reduction Operation Error

!!! failure "There was a problem while initializing support for the CUDA reduction operations."

!!! note "Explanation"

    For OpenACC, NVHPC was in the process of adding OpenMP array reduction support which is needed
    for the `pot3d` benchmark. An Nvidia driver version of 450.80.00 or higher is required. Since
    the driver version on partiton `ml` is 440.64.00, it is not supported and not possible to run
    the `pot3d` benchmark in OpenACC mode here.

!!! note "Workaround"

    As for the partition `ml`, you can only wait until the OS update to CentOS 8 is carried out,
    as no driver update will be done beforehand. As a workaround, you can do one of the following:

    - Exclude the `pot3d` benchmark.
    - Switch the partition (e.g. to partition `alpha`).

### Slurm Bug

!!! warning "Wrong resource distribution"

    When working with multiple nodes on partition `ml` or `alpha`, the Slurm parameter
    `$SLURM_NTASKS_PER_NODE` does not work as intended when used in conjunction with `mpirun`.

!!! note "Explanation"

    In the described case, when setting e.g. `SLURM_NTASKS_PER_NODE=8` and calling `mpirun`, Slurm
    is not able to pass on the allocation settings correctly. With two nodes, this leads to a
    distribution of 15 processes on the first node and 1 process on the second node instead. In
    fact, none of the proposed methods of Slurm's man page (like `--distribution=plane=8`) will
    give the result as intended in this case.

!!! note "Workaround"

    - Use `srun` instead of `mpirun`.
    - Use `mpirun` along with a rank-binding perl script (like
    `mpirun -np <ranks> perl <bind.pl> <command>`) as seen on the bottom of the configurations
    [here](https://www.spec.org/hpc2021/results/res2021q4/hpc2021-20210908-00012.cfg) and
    [here](https://www.spec.org/hpc2021/results/res2021q4/hpc2021-20210917-00056.cfg)
    in order to enforce the correct distribution of ranks as it was intended.

### Benchmark Hangs Forever

!!! warning "The benchmark runs forever and produces a timeout."

!!! note "Explanation"

    The reason for this is not known, however, it is caused by the flag `-DSPEC_ACCEL_AWARE_MPI`.

!!! note "Workaround"

    Remove the flag `-DSPEC_ACCEL_AWARE_MPI` from the compiler options in your configuration file.

### Other Issues

For any further issues you can consult SPEC's
[FAQ page](https://www.spec.org/hpc2021/Docs/faq.html), search through their
[known issues](https://www.spec.org/hpc2021/Docs/known-problems.html) or contact their
[support](https://www.spec.org/hpc2021/Docs/techsupport.html).


# Inspect Model Training with TensorBoard

TensorBoard is a visualization toolkit for TensorFlow and offers a variety of functionalities such
as presentation of loss and accuracy, visualization of the model graph or profiling of the
application.

## Using JupyterHub

The easiest way to use TensorBoard is via [JupyterHub](../access/jupyterhub.md). By default,
TensorBoard is configured to read log data from `/tmp/<username>/tf-logs` on the compute node on
which the Jupyter session is running. In order to show your own log data from a different directory,
soft-link this directory with `/tmp/<username>/tf-logs` in order to make TensorBoard reading your
log data. Note, that the directory `/tmp/<username>/tf-logs` might not exist and you have to
create it first. Therefore, open a "New Launcher" (`Ctrl+Shift+L`) and select "Terminal" session.
It will start a new terminal on the respective compute node. Then you can create the directory
`/tmp/<username>/tf-logs` and link it with the directory where your own log data is located.
Assuming you use a line like the following in your code:

```python
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="/home/marie/logs")
```

You can then make the TensorBoard available from the Jupyter terminal with:

```bash
mkdir -p /tmp/${USER}/tf-logs
ln -s /home/marie/logs /tmp/${USER}/tf-logs
```

Update TensorBoard tab if needed with `F5`.

## Using TensorBoard from Module Environment

On ZIH systems, TensorBoard is also available as an extension of the TensorFlow module. To check
whether a specific TensorFlow module provides TensorBoard, use the following command:

```console hl_lines="9"
marie@compute$ module spider TensorFlow/2.3.1
[...]
        Included extensions
        ===================
        absl-py-0.10.0, astor-0.8.0, astunparse-1.6.3, cachetools-4.1.1, gast-0.3.3,
        google-auth-1.21.3, google-auth-oauthlib-0.4.1, google-pasta-0.2.0,
        grpcio-1.32.0, Keras-Preprocessing-1.1.2, Markdown-3.2.2, oauthlib-3.1.0, opt-
        einsum-3.3.0, pyasn1-modules-0.2.8, requests-oauthlib-1.3.0, rsa-4.6,
        tensorboard-2.3.0, tensorboard-plugin-wit-1.7.0, TensorFlow-2.3.1, tensorflow-
        estimator-2.3.0, termcolor-1.1.0, Werkzeug-1.0.1, wrapt-1.12.1
```

If TensorBoard occurs in the `Included extensions` section of the output, TensorBoard is available.

To use TensorBoard, you have to connect via ssh to the ZIH system as usual, schedule an interactive
job and load a TensorFlow module:

```console
marie@compute$ module load TensorFlow/2.3.1
Module TensorFlow/2.3.1-fosscuda-2019b-Python-3.7.4 and 47 dependencies loaded.
```

Then, create a workspace for the event data, that should be visualized in TensorBoard. If you
already have an event data directory, you can skip that step.

```console
marie@compute$ ws_allocate -F /data/horse tensorboard_logdata 1
Info: creating workspace.
/data/horse/ws/marie-tensorboard_logdata
[...]
```

Now, you can run your TensorFlow application. Note that you might have to adapt your code to make it
accessible for TensorBoard. Please find further information on the official [TensorBoard website](https://www.tensorflow.org/tensorboard/get_started)
Then, you can start TensorBoard and pass the directory of the event data:

```console
marie@compute$ tensorboard --logdir /data/horse/ws/marie-tensorboard_logdata --bind_all
[...]
TensorBoard 2.3.0 at http://taurusi8034.taurus.hrsk.tu-dresden.de:6006/
[...]
```

TensorBoard then returns a server address on Taurus, e.g. `taurusi8034.taurus.hrsk.tu-dresden.de:6006`

For accessing TensorBoard now, you have to set up some port forwarding via ssh to your local
machine:

```console
marie@local$ ssh -N -f -L 6006:taurusi8034:6006 taurus
```

!!! important "SSH command"

    The previous SSH command requires that you have already set up your [SSH configuration
    ](../access/ssh_login.md#configuring-default-parameters-for-ssh).

Now, you can see the TensorBoard in your browser at `http://localhost:6006/`.

Note that you can also use TensorBoard in an [sbatch file](../jobs_and_resources/slurm.md).


# Neural Networks with TensorFlow

[TensorFlow](https://www.tensorflow.org) is a free end-to-end open-source software library for data
flow and differentiable programming across many tasks. It is a symbolic math library, used primarily
for machine learning applications. It has a comprehensive, flexible ecosystem of tools, libraries
and community resources.

Please check the software modules list via

```console
marie@compute$ module spider TensorFlow
[...]
```

to find out, which TensorFlow modules are available on your cluster.

On ZIH systems, TensorFlow 2 is the default module version. For compatibility hints between
TensorFlow 2 and TensorFlow 1, see the corresponding [section below](#compatibility-tf2-and-tf1).

We recommend using the clusters `Alpha`, `Capella` and/or `Power9` when working with machine
learning workflows and the TensorFlow library. You can find detailed hardware specification in our
[Hardware](../jobs_and_resources/hardware_overview.md) documentation.
Available software may differ among the clusters.

## TensorFlow Console

On the cluster `Alpha`, load the module environment:

```console
marie@alpha$ module load release/23.04
```

Alternatively you can use `release/23.10` module environment, where the newest versions are
available

```console
[marie@alpha ]$ module load release/23.10  GCC/11.3.0  OpenMPI/4.1.4
Module GCC/11.3.0, OpenMPI/4.1.4 and 14 dependencies loaded.

[marie@alpha ]$ module load TensorFlow/2.9.1
Module TensorFlow/2.9.1 and 35 dependencies loaded.
[marie@alpha ]$ module avail TensorFlow

-------- /software/modules/rapids/r23.10/all/MPI/GCC/11.3.0/OpenMPI/4.1.4 --------
   TensorFlow/2.9.1 (L)

  Where:
   L:  Module is loaded
   *Module:  Some Toolchain, load to access other modules that depend on it
   >Module:  Recommended toolchain version, load to access other modules that depend on it
```

This example shows how to install and start working with TensorFlow using the modules system.

```console
marie@power$ module load TensorFlow
Module TensorFlow/2.3.1-fosscuda-2019b-Python-3.7.4 and 47 dependencies loaded.
```

Now we can use TensorFlow. Nevertheless when working with Python in an interactive job, we recommend
to use a virtual environment. In the following example, we create a python virtual environment and
import TensorFlow:

!!! example

    ```console
    marie@power$ ws_allocate -F horse python_virtual_environment 1
    Info: creating workspace.
    /data/horse/ws/python_virtual_environment
    [...]
    marie@power$ which python    #check which python are you using
    /sw/installed/Python/3.7.2-GCCcore-8.2.0
    marie@power$ virtualenv --system-site-packages /data/horse/ws/marie-python_virtual_environment/env
    [...]
    marie@power$ source /data/horse/ws/marie-python_virtual_environment/env/bin/activate
    marie@power$ python -c "import tensorflow as tf; print(tf.__version__)"
    [...]
    2.3.1
    ```

## TensorFlow in JupyterHub

In addition to interactive and batch jobs, it is possible to work with TensorFlow using
JupyterHub, which contains a kernel named `Python 3 ... TensorFlow`, that
come with TensorFlow support.

![TensorFlow module in JupyterHub](misc/tensorflow_jupyter_module.png)
{: align="center"}

!!! hint

    You can also define your own Jupyter kernel for more specific tasks. Please read about Jupyter
    kernels and virtual environments in our
    [JupyterHub](../access/jupyterhub_custom_environments.md) documentation.

## TensorFlow in Containers

Another option to use TensorFlow are containers. In the HPC domain, the
[Singularity](https://singularity.hpcng.org/) container system is a widely used tool. In the
following example, we use the tensorflow-test in a Singularity container:

```console
marie@power$ singularity shell --nv /data/horse/singularity/powerai-1.5.3-all-ubuntu16.04-py3.img
Singularity>$ export PATH=/opt/anaconda3/bin:$PATH
Singularity>$ source activate /opt/anaconda3    #activate conda environment
(base) Singularity>$ . /opt/DL/tensorflow/bin/tensorflow-activate
(base) Singularity>$ tensorflow-test
Basic test of tensorflow - A Hello World!!!...
[...]
```

!!! hint
    In the above example, we create a conda virtual environment. To use conda, it is be necessary to
    configure your shell as described in [Python virtual environments](python_virtual_environments.md#conda-virtual-environment)

## TensorFlow with Python or R

For further information on TensorFlow in combination with Python see
[data analytics with Python](data_analytics_with_python.md), for R see
[data analytics with R](data_analytics_with_r.md).

## Distributed TensorFlow

For details on how to run TensorFlow with multiple GPUs and/or multiple nodes, see
[distributed training](distributed_training.md).

## Compatibility TF2 and TF1

TensorFlow 2.0 includes many API changes, such as reordering arguments, renaming symbols, and
changing default values for parameters. Thus in some cases, it makes code written for the TensorFlow
1.X not compatible with TensorFlow 2.X. However, If you are using the high-level APIs (`tf.keras`)
there may be little or no action you need to take to make your code fully
[TensorFlow 2.0](https://www.tensorflow.org/guide/migrate) compatible. It is still possible to
run 1.X code, unmodified (except for `contrib`), in TensorFlow 2.0:

```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()    #instead of "import tensorflow as tf"
```

To make the transition to TensorFlow 2.0 as seamless as possible, the TensorFlow team has created
the tf_upgrade_v2 utility to help transition legacy code to the new API.

## Keras

[Keras](https://keras.io) is a high-level neural network API, written in Python and capable
of running on top of TensorFlow. Please check the software modules list via

```console
marie@compute$ module spider Keras
[...]
```

to find out, which Keras modules are available on your cluster. TensorFlow should be automatically
loaded as a dependency. After loading the module, you can use Keras as usual.


# Utilities

This page provides tools and utilities that make your life on ZIH systems more comfortable.

## Tmux

### Best Practices

Terminal multiplexers are particularly well-suited for aiding you as a computer scientist in your
daily trade. We generally favor *tmux* as it's newer than certain others and allows for better
customization.

As there is already plenty of documentation on how to use tmux, we won't repeat that here. But
instead, we would like to point you to those documents:

* [Tmux man page](https://manpages.org/tmux)
* [Tmux customization](https://tmuxguide.readthedocs.io/en/latest/tmux/tmux.html#tmux-conf)
* [Tao of Tmux](https://tao-of-tmux.readthedocs.io/en/latest/)
* [Tmux Cheat Sheet](https://tmuxcheatsheet.com/)

### Basic Usage

Tmux is a terminal multiplexer. It lets you switch easily between several programs in one
terminal, detach them (they keep running in the background), and reattach them to a different
terminal.

The huge advantage is, that as long as your tmux session is running, you can connect to it and your
settings (e.g., loaded modules, current working directory, ...) are in place. This is
beneficial when working within an unstable network with connection losses (e.g., traveling by the
train in Germany), but also speed-ups your workflow in the daily routine.

```console
marie@compute$ tmux new-session -s marie_is_testing -d
marie@compute$ tmux attach -t marie_is_testing
  echo "hello world"
  ls -l
Ctrl+b & d
```

!!! note

    If you want to jump out of your tmux session, hold the Control key and press 'b'. After that,
    release both keys and press 'd'. With the first key combination, you address tmux itself, whereas
    'd' is the tmux command to "detach" yourself from it. The tmux session will stay alive and
    running. You can jump into it any time later by just using the aforementioned "tmux attach"
    command again.

### Using a More Recent Version

More recent versions of tmux are available via the module system. Using the well know
[module commands](modules.md#module-commands), you can query all available versions, load and unload
certain versions from your environment, e.g.,

```console
marie@login$ module load tmux/3.2a
```

### Error: Protocol Version Mismatch

When trying to connect to tmux, you might encounter the following error message:

```console
marie@compute$ tmux a -t juhu
protocol version mismatch (client 7, server 8)
```

To solve this issue, make sure that the tmux version you invoke
is the same as the tmux server that is running.
In particular, you can determine your client's version with the command `tmux -V`.
Try to [load the appropriate tmux version](#using-a-more-recent-version) to match with your
client's tmux server like this:

```console
marie@compute$ tmux -V
tmux 1.8
marie@compute$ module load tmux/3.2a
Module tmux/3.2a-GCCcore-11.2.0 and 5 dependencies loaded.
marie@compute$ tmux -V
tmux 3.2a
```

!!! hint

    When your client's version is newer than the server version, the aforementioned approach
    won't help you. In that case, you need to unload the loaded tmux module to downgrade
    the client to the client version that is supplied with the operating system (which
    should have a lower version number).

### Using Tmux on Compute Nodes

At times it might be quite handy to have tmux sessions running inside your computation jobs,
such that you perform your computations within an interactive tmux session.
For this purpose, the following shorthand is to be placed inside the
[job file](../jobs_and_resources/slurm.md#job-files):

```bash
#!/bin/bash
#SBATCH [...]

module load tmux/3.2a
tmux new-session -s marie_is_computing -d
sleep 1;
tmux wait-for CHANNEL_NAME_MARIE

srun [...]
```

You can then connect to the tmux session like this:

```console
marie@login$ ssh -t "$(squeue --me --noheader --format="%N" 2>/dev/null | tail -n 1)" \
             "source /etc/profile.d/10_modules.sh; module load tmux/3.2a; tmux attach"
```

### Where Is My Tmux Session?

Please note that, as there are thousands of compute nodes available, there are also multiple login
nodes. Thus, try checking the other login nodes as well:

```console
marie@login3$ tmux ls
failed to connect to server
marie@login3$ ssh login4 tmux ls
marie_is_testing: 1 windows (created Tue Mar 29 19:06:26 2022) [105x32]
```

## Architecture Information (lstopo)

The page [HPC Resource Overview](../jobs_and_resources/overview.md) holds a general and fast
overview about the available HPC resources at ZIH.
Sometime a closer look and deeper understanding of a particular architecture is needed. This is
where the tool `lstopo` comes into play.

The tool [lstopo](https://linux.die.net/man/1/lstopo) displays the topology of a system in a variety
of output formats.

`lstopo` and `lstopo-no-graphics` are available from the `hwloc` modules, e.g.

```console
marie@login$ module load hwloc/2.5.0-GCCcore-11.2.0
marie@login$ lstopo
```

The topology map is displayed in a graphical window if the `DISPLAY` environment variable is set.
Otherwise, a text summary is displayed. The displayed topology levels and granularity can be
controlled using the various options of `lstopo`. Please refer to the corresponding man page and
help message (`lstopo --help`).

It is also possible to run this command using a job file to retrieve the topology of a compute nodes.

```bash
#!/bin/bash

#SBATCH --job-name=topo_node
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=300m
#SBATCH --time=00:05:00
#SBATCH --output=get_topo.out
#SBATCH --error=get_topo.err

module purge
module load hwloc/2.5.0-GCCcore-11.2.0

srun lstopo
```

## Working with Large Archives and Compressed Files

### Parallel Gzip Decompression

There is a plethora of `gzip` tools but none of them can fully utilize multiple cores.
The fastest single-core decoder is `igzip` from the
[Intelligent Storage Acceleration Library](https://github.com/intel/isa-l.git).
In tests, it can reach ~500 MB/s compared to ~200 MB/s for the system-default `gzip`.
If you have very large files and need to decompress them even faster, you can use
[rapidgzip](https://github.com/mxmlnkn/rapidgzip).
Currently, it can reach ~1.5 GB/s using a 12-core processor in the above-mentioned tests.

[rapidgzip](https://github.com/mxmlnkn/rapidgzip) is available on PyPI and can be installed via pip.
It is recommended to install it inside a
[Python virtual environment](python_virtual_environments.md).

```console
marie@compute$ pip install rapidgzip
```

It can also be installed from its C++ source code.
If you prefer that over the version on PyPI, then you can build it like this:

```console
marie@compute$ git clone https://github.com/mxmlnkn/rapidgzip.git
marie@compute$ cd rapidgzip
marie@compute$ mkdir build
marie@compute$ cd build
marie@compute$ cmake ..
marie@compute$ cmake --build . rapidgzip
marie@compute$ src/tools/rapidgzip --help
```

The built binary can then be used directly or copied inside a folder that is available in your
`PATH` environment variable.

Rapidgzip can be used like this:

```bash
marie@compute$ rapidgzip -d <file_to_decompress>
```

For example, if you want to decompress a file called `data.gz`, use:

```console
marie@compute$ rapidgzip -d data.gz
```

Furthermore, you can use it to speed up extraction of a file `my-archive.tar.gz` like this:

```console
marie@compute$ tar --use-compress-program=rapidgzip -xf my-archive.tar.gz
```

Rapidgzip is still in development, so if it crashes or if it is slower than the system `gzip`,
please [open an issue](https://github.com/mxmlnkn/rapidgzip/issues) on GitHub.

### Direct Archive Access Without Extraction Using Ratarmount

In some cases of archives with millions of small files, it might not be feasible to extract the
whole archive to a filesystem.
The known `archivemount` tool has performance problems with such archives even if they are simply
uncompressed TAR files.
Furthermore, with `archivemount` the archive would have to be reanalyzed whenever a new job is started.

`Ratarmount` is an alternative that solves these performance issues.
The archive will be analyzed and then can be accessed via a FUSE mountpoint showing the internal
folder hierarchy.
Access to files is consistently fast no matter the archive size while `archivemount` might take
minutes per file access.
Furthermore, the analysis results of the archive will be stored in a sidecar file alongside the
archive or in your home directory if the archive is in a non-writable location.
Subsequent mounts instantly load that sidecar file instead of reanalyzing the archive.
You will find further information on the [Ratarmount GitHub page](https://github.com/mxmlnkn/ratarmount).

#### Example Workflow

The software Ratarmount is installed system-wide on the HPC system.

The first step is to create a tar archive to bundle your small files in a single file.

```bash
# On your local machine
marie@local$ tar cf dataset.tar folder_containing_my_small_files

# If your small files are already on the HPC system
marie@login$ dttar cf dataset.tar folder_containing_my_small_files
```

For the latter, please make sure that you are on a [Datamover node](../data_transfer/datamover.md)
and **not** on a login node.
Depending on the number of files, the tar bundle process may take some time.

We do not recommend to compress (e.g. Gzip) the archive, as this can decrease the read performance substantially
e.g. for images, audio and video files.

Once the tar archive has been created, you can mount it on the compute node using `ratarmount'.
All files in the mount points can be accessed as normal files or directories
in the filesystem without any special treatment.
Note that the tar archive must be mounted on every compute node in your job.

!!! note

    Mounting an archive for the first time can take some time  because Ratarmount has to create an index of its contents to access it efficiently.
    The index, named `.<name_of_the_archive>.index.sqlite`, will be placed
    in the same directory as the archive if the directory is writable,
    otherwise ratarmount will try to place the index in your home directory.
    This indexing step could be done in a separate job to save resources.
    It also prevents conflicting indexing by more than one process at the same time.

    ```bash
    # create index
    sbatch --ntasks=1 --mem=10G --time=5:00:00 ratarmount dataset.tar
    ```

!!! example "Example job script using Ratarmount"

    ```bash
    #!/bin/bash

    #SBATCH --ntasks=3
    #SBATCH --nodes=2
    #SBATCH --time=00:05:00


    # mount the dataset on every node one time
    DATASET=/tmp/${SLURM_JOB_ID}
    srun --ntasks-per-node=1 mkdir ${DATASET}
    srun --ntasks-per-node=1 ratarmount dataset.tar ${DATASET}

    # now it can be accessed like a normal directory
    srun --ntasks=1 ls ${DATASET}

    # start the application
    srun ./my_application --input-directory ${DATASET}

    # unmount it after all work is done
    srun --ntasks-per-node=1 ratarmount -u ${DATASET}
    ```

!!! hint

    If you are starting many processes per node, Ratarmount could benefit from
    having individual mount points for each process, rather than just one per node.

In case of Ratarmount issues
please [open an issue](https://github.com/mxmlnkn/ratarmount/issues) on GitHub.

There also is a library interface called
[ratarmountcore](https://github.com/mxmlnkn/ratarmount/tree/master/core#example) that works
fully without FUSE, which might make access to files from Python even faster.


# Study Course of Events with Vampir

## Introduction

Vampir is a graphical analysis framework that provides a large set of different chart
representations of event based performance data generated through program instrumentation. These
graphical displays, including state diagrams, statistics, and timelines, can be used by developers
to obtain a better understanding of their parallel program inner working and to subsequently
optimize it. Vampir allows to focus on appropriate levels of detail, which allows the detection and
explanation of various performance bottlenecks such as load imbalances and communication
deficiencies. [ZIH's Vampir overview page
](http://tu-dresden.de/die_tu_dresden/zentrale_einrichtungen/zih/forschung/projekte/vampir) gives
further information.

[Score-P](scorep.md) is the primary code instrumentation and run-time measurement framework for
Vampir and supports various instrumentation methods, including instrumentation at source level and
at compile/link time. The tool supports trace files in Open Trace Format (OTF, OTF2) that is
developed by ZIH and its partners and is especially designed for massively parallel programs.

![Vampir Framework](misc/vampir-framework.png)
{: align="center"}

## Starting Vampir

Prior to using Vampir you need to set up the correct environment on one
the HPC systems with:

```console
marie@login$ module load Vampir
```

For members of TU Dresden the Vampir tool is also available as
[download](http://tu-dresden.de/die_tu_dresden/zentrale_einrichtungen/zih/forschung/projekte/vampir/vampir_download_tu)
for installation on your personal computer.

Make sure, that compressed display forwarding (e.g., `ssh -YC taurus`) is
enabled. Start the GUI by typing

```console
marie@login$ vampir
```

on your command line or by double-clicking the Vampir icon on your personal computer.

Please consult the
[Vampir user manual](http://tu-dresden.de/die_tu_dresden/zentrale_einrichtungen/zih/forschung/projekte/vampir/dateien/Vampir-User-Manual.pdf)
for a tutorial on using the tool.

## Using VampirServer

VampirServer provides additional scalable analysis capabilities to the Vampir GUI mentioned above.
To use VampirServer on the ZIH Systems proceed as follows: start the Vampir GUI as
described above and use the *Open Remote* dialog with the parameters indicated in the following
figure to start and connect a VampirServer already instance running on the HPC system. Make sure
to fill in your personal ZIH login name.

![Vampir Open Remote dialog](misc/vampir-open-remote-dialog.png)
{: align="center"}

Click on the *Connect* button and wait until the connection is established. Enter your password when
requested. Depending on the available resources on the target system, this setup can take some time.
Please be patient and take a look at available resources beforehand.

## Advanced Usage

### Manual Server Startup

VampirServer is a parallel MPI program, which should be started by typing:

```console
marie@login$ vampirserver start
Launching VampirServer...
Submitting slurm 30 minutes job (this might take a while)...
```

This way, a job with a timelimit of 30 minutes and default resources is submitted. This might fit
your needs. If not, please feel free to request a **customized job** running VampirServer, e.g.

```console
marie@login$ vampirserver start --ntasks=8 -- --time=01:00:00 -- --mem-per-cpu=3000M 
Launching VampirServer...
Submitting slurm 01:00:00 minutes job (this might take a while)...
```

The above `vampirserver` command automatically allocates its resources via the respective batch
system (, i.e. [Slurm](../jobs_and_resources/slurm.md) on ZIH systems). As shown, you can customize
the resources requirements and time limit. This is especially useful, if you run into performance
issues handling very large trace files. Please refer to `vampirserver --help` for further options
and usage.

If you want to start

VampirServer without a batch allocation or from inside an interactive allocation, use

```console
marie@compute$ vampirserver start srun
```

After scheduling this job the server prints out the port number it is serving on, like `Listen port:
30088`.

Connecting to the most recently started server can be achieved by entering `auto-detect` as *Setup
name* in the *Open Remote* dialog of Vampir.

![Vampir Open Remote Dialog (auto start)](misc/vampir-open-remote-dialog-auto-start.png)
{: align="center"}

Please make sure you stop VampirServer after finishing your work with
the front-end (*File* → *Shutdown Server...*) or with

```console
marie@login$ vampirserver stop
```

Type

```console
marie@login$ vampirserver help
```

for further information. The [user manual](http://tu-dresden.de/die_tu_dresden/zentrale_einrichtungen/zih/forschung/projekte/vampir/dateien/VampirServer-User-Manual.pdf)
of VampirServer can be found at `doc/vampirserver-manual.pdf` in the installation directory.
Type

```console
marie@login$ which vampirserver
```

to find the revision dependent *installation directory*.

### Port Forwarding

VampirServer listens to a given socket port. It is possible to forward
this port (SSH tunnel) to a remote machine. This procedure is not
recommended and not needed at ZIH. However, the following example shows
the tunneling to a VampirServer on a compute node.

Start VampirServer on the ZIH system and wait for its scheduling:

```console
marie@login$ vampirserver start
Launching VampirServer...
Submitting slurm 30 minutes job (this might take a while)...
salloc: Granted job allocation 2753510
VampirServer 8.1.0 (r8451)
Licensed to ZIH, TU Dresden
Running 4 analysis processes... (abort with vampirserver stop 594)
VampirServer <594> listens on: taurusi1253:30055
```

Or choose from an already running VampirServer:

```console
marie@login$ vampirserver list
594 taurusi1253:30055 [4x, slurm]
```

Open a second console on your local computer and establish an SSH tunnel to the compute node with:

```console
marie@local$ ssh -L 30000:taurusi1253:30055 taurus
```

!!! important "SSH command"

    The previous SSH command requires that you have already set up your [SSH configuration
    ](../access/ssh_login.md#configuring-default-parameters-for-ssh).

Now, the port 30000 on your desktop is connected to the VampirServer port 30055 at the compute node
`taurusi1253` of the ZIH system. Finally, start your local Vampir client and establish a remote
connection to `localhost`, port 30000 as described in the manual.

```console
marie@local$ vampir
```

**Remark:** Please substitute the ports given in this example with appropriate numbers and available
ports based on the output from `vampirserver start` or `vampirserver list`.

### Nightly Builds (unstable)

Expert users who subscribed to the development program can test new, unstable tool features. The
corresponding Vampir and VampirServer software releases are provided as nightly builds. Unstable
versions of VampirServer are also installed on the HPC systems. The most recent version can be
launched/connected by entering `unstable` as *Setup name* in the *Open Remote* dialog of Vampir.

![Vampir Open Remote Dialog (unstable)](misc/vampir-open-remote-dialog-unstable.png)
{: align="center"}


# Virtual Desktops

Use WebVNC or DCV to run GUI applications on HPC resources.

|                | WebVNC*                                               | DCV                                        |
|----------------|-------------------------------------------------------|--------------------------------------------|
| **use case**   | all GUI applications that do **not need** OpenGL      | only GUI applications that **need** OpenGL |
| **clusters**   | all\*                                                 | Visualization (Vis)                        |

!!! note

    WebVNC has GL support at the visualization nodes on cluster named Visualization (Vis).

## Launch a Virtual Desktop

| Step | WebVNC         | DCV                    |
|------|:--------------:|:----------------------:|
| 1  <td colspan=2 align="center"> Navigate to [https://jupyterhub.hpc.tu-dresden.de](https://jupyterhub.hpc.tu-dresden.de). There is our [JupyterHub](../access/jupyterhub.md) instance.
| 2  <td colspan=2 align="center"> Click on the "advanced" tab and choose a preset:
| 3  <td colspan=2 align="center"> Optional: Fine tune your session with the available Slurm job parameters or assign a certain project or reservation. Then save your settings in a new preset for future use.
| 4  <td colspan=2 align="center"> Click on `Spawn`. JupyterHub starts now a Slurm job for you. If everything is ready the JupyterLab interface will appear to you.
| 5    | Click on `WebVNC` to start a virtual desktop. | Click on `DCV` to start a virtual desktop. |
| 6  <td colspan=2 align="center"> The virtual desktop starts in a new tab or window.

### Demonstration

![type:video](./misc/start-virtual-desktop-dcv.mp4)

### Using the Quickstart Feature

JupyterHub can start a job automatically if the URL contains certain
parameters.

|              | WebVNC       | DCV          |
|--------------|:------------:|:------------:|
| Examples     | [WebVNC](https://jupyterhub.hpc.tu-dresden.de/hub/spawn#/~(cluster~'vis~nodes~'1~ntasks~'1~cpuspertask~'1~mempercpu~'1536)) | [DCV](https://jupyterhub.hpc.tu-dresden.de/hub/spawn#/~(cluster~'vis~nodes~'1~ntasks~'1~cpuspertask~'2~mempercpu~'2048)) |
| Description  | cluster `vis`, 1 Node, 1 CPUs with 1,5 GB RAM, 1 GPU | cluster `vis`, 1 Node, 2 CPUs with 2 GB RAM per core, 1 GPU |
| Link creator <td colspan=2 align="center"> Use the spawn form to set your preferred options. The browser URL will be updated with the corresponding parameters.

If you close the browser tabs or windows or log out from your local
machine, you are able to open the virtual desktop later again - as long
as the session runs. But please remember that a Slurm job is running in
the background which has a certain time limit.

## Reconnecting to a Session

In order to reconnect to an active instance of WebVNC, simply repeat the
steps required to start a session, beginning - if required - with the
login, then clicking `My Server`, then by pressing the `+` sign on the
upper left corner. Provided your server is still running and you simply
closed the window or logged out without stopping your server, you will
find your WebVNC desktop the way you left it.

## Terminate a Remote Session

| Step | Description |
|------|-------------|
| 1    | Close the VNC viewer tab or window. |
| 2    | Click on File \> Log Out in the JupyterLab main menu. Now you get redirected to the JupyterLab control panel. If you don't have your JupyterLab tab or window anymore, navigate directly to [https://jupyterhub.hpc.tu-dresden.de/hub/home](https://jupyterhub.hpc.tu-dresden.de/hub/home) |
| 3    | Click on `Stop My Server`. This cancels the Slurm job and terminates your session. |

### Demonstration

![type:video](./misc/terminate-virtual-desktop-dcv.mp4)

!!! note

    This does not work if you click on the button `Logout` in your
    virtual desktop. Instead this will just close your DCV session or cause
    a black screen in your WebVNC window without a possibility to recover a
    virtual desktop in the same Jupyter session. The solution for now would
    be to terminate the whole Jupyter session and start a new one like
    mentioned above.


# Virtual Machines

The following instructions are primarily aimed at users who want to build their own
[Singularity](containers.md) containers on ZIH systems.

The Singularity container setup requires a Linux machine with root privileges, the same architecture
and a compatible kernel. If some of these requirements cannot be fulfilled, then there is also the
option of using the provided virtual machines (VM) on ZIH systems.

Currently, starting VMs is only possible on the cluster `power` (and `hpdlf`?). The VMs on the power
nodes are used to build Singularity containers for the Power9 architecture and the HPDLF nodes to
build Singularity containers for the x86 architecture.

## Create a Virtual Machine

The Slurm parameter `--cloud=kvm` specifies that a virtual machine should be started.

### On Power9 Architecture

```console
marie@login.power$ srun --nodes=1 --cpus-per-task=4 --hint=nomultithread --cloud=kvm --pty /bin/bash
srun: job 6969616 queued and waiting for resources
srun: job 6969616 has been allocated resources
bash-4.2$
```

### On x86 Architecture

_to be updated...._

```console
marie@login$ srun --partition=hpdlf --nodes=1 --cpus-per-task=4 --hint=nomultithread --cloud=kvm --pty /bin/bash
srun: job 2969732 queued and waiting for resources
srun: job 2969732 has been allocated resources
bash-4.2$
```

## Access a Virtual Machine

After a security issue on ZIH systems, we restricted the filesystem permissions. Now, you have to
wait until the file `/tmp/${SLURM_JOB_USER}_${SLURM_JOB_ID}/activate` is created. Then, you can try
to connect via `ssh` into the virtual machine, but it could be that the virtual machine needs some
more seconds to boot and accept the connection. So you may need to try the `ssh` command multiple
times till it succeeds.

```console
bash-4.2$ cat /tmp/marie_2759627/activate
#!/bin/bash

if ! grep -q -- "Key for the VM on the cluster power" "/home/marie/.ssh/authorized_keys" > /dev/null; then
  cat "/tmp/marie_2759627/kvm.pub" >> "/home/marie/.ssh/authorized_keys"
else
  sed -i "s|.*Key for the VM on the cluster power.*|ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC3siZfQ6vQ6PtXPG0RPZwtJXYYFY73TwGYgM6mhKoWHvg+ZzclbBWVU0OoU42B3Ddofld7TFE8sqkHM6M+9jh8u+pYH4rPZte0irw5/27yM73M93q1FyQLQ8Rbi2hurYl5gihCEqomda7NQVQUjdUNVc6fDAvF72giaoOxNYfvqAkw8lFyStpqTHSpcOIL7pm6f76Jx+DJg98sXAXkuf9QK8MurezYVj1qFMho570tY+83ukA04qQSMEY5QeZ+MJDhF0gh8NXjX/6+YQrdh8TklPgOCmcIOI8lwnPTUUieK109ndLsUFB5H0vKL27dA2LZ3ZK+XRCENdUbpdoG2Czz Key for the VM on the cluster power|" "/home/marie/.ssh/authorized_keys"
fi

ssh -i /tmp/marie_2759627/kvm root@192.168.0.6
bash-4.2$ source /tmp/marie_2759627/activate
Last login: Fri Jul 24 13:53:48 2020 from gateway
[root@marie_2759627 ~]#
```

## Example Usage

## Automation

We provide [tools](singularity_power9.md) to automate these steps. You may just type `startInVM
--arch=power9` on a login node and you will be inside the VM with everything mounted.

## Known Issues

### Temporary Memory

The available space inside the VM can be queried with `df -h`. Currently the whole VM has 8 GB and
with the installed operating system, 6.6 GB of available space.

Sometimes, the Singularity build might fail because of a disk out-of-memory error. In this case, it
might be enough to delete leftover temporary files from Singularity:

```console
rm -rf /tmp/sbuild-*
```

If that does not help, e.g., because one build alone needs more than the available disk memory, then
it will be necessary to use the `tmp` folder on `/data/horse`. In order to ensure that the files in the
temporary folder will be owned by root, it is necessary to set up an image inside `/data/horse/tmp`
instead of using it directly. E.g., to create a 25 GB of temporary memory image:

```console
tmpDir="$( mktemp -d --tmpdir=/host_data/tmp )" && tmpImg="$tmpDir/singularity-build-temp-dir"
export LANG_BACKUP=$LANG
unset LANG
truncate -s 25G "$tmpImg.ext4" && echo yes | mkfs.ext4 "$tmpImg.ext4"
export LANG=$LANG_BACKUP
```

The image can now be mounted and with the `SINGULARITY_TMPDIR` environment variable can be
specified as the temporary directory for Singularity builds. Unfortunately, because of an open
Singularity [bug](https://github.com/sylabs/singularity/issues/32) it is should be avoided to mount
the image using `/dev/loop0`.

```console
mkdir -p "$tmpImg" && i=1 && while test -e "/dev/loop$i"; do (( ++i )); done && mknod -m 0660 "/dev/loop$i" b 7 "$i"
mount -o loop="/dev/loop$i" "$tmpImg"{.ext4,}

export SINGULARITY_TMPDIR="$tmpImg"
singularity build my-container.{sif,def}
```

The architecture of the base image is automatically chosen when you use an image from DockerHub.
This may not work for Singularity Hub, so in order to build for the power architecture the
Bootstraps **shub** and **library** should be avoided.

### Transport Endpoint is not Connected

This happens when the SSHFS mount gets unmounted because it is not very stable. It is sufficient to
run `~/mount_host_data.sh` again or just the SSHFS command inside that script.


# Visualization

## ParaView

[ParaView](https://paraview.org) is an open-source, multi-platform data analysis and visualization
application. The ParaView package comprises different tools which are designed to meet interactive,
batch and in-situ workflows.

ParaView can be used in [interactive mode](#interactive-mode) as well as in
[batch mode](#batch-mode-pvbatch). Both modes are documented in more details in the following
subsections.

!!! warning WLOG ParaView module and cluster

    Without loss of generality, we stick to a certain ParaView module (from a certain module
    release) in the following documentation and provided examples. **Do not blind copy the
    examples.**
    Furthermore, please **adopt the commands to your needs**, e.g., the concrete ParaView module you
    want to use.

    The same holds for the cluster used in the documentation and examples. The documentation refers
    to the cluster [`Barnard`](../jobs_and_resources/hardware_overview.md#barnard). If you want to
    use ParaView on [one of the other clusters](../jobs_and_resources/hardware_overview.md), this
    documentation should hold too.

### ParaView Modules

ParaView is available through the [module system](modules.md#module-environments). The
following command lists the available versions

```console
marie@login$ module spider ParaView
    [...]
        Versions:
            ParaView/5.10.1-mpi
            ParaView/5.11.1-mpi
            ParaView/5.11.2
    [...]
```

Please note, that not all ParaView modules are available in all
[module environments](modules.md#module-environments).
The command `module spider <module-name>` will show you, how to load a certain ParaView module.

??? example "Example on how to load a ParaView module"

    For example, to obtain information on how to properly load the module `ParaView/5.10.1-mpi`, you
    need to invoke the `module spider` command as follows:

    ```console
    marie@login$ module spider ParaView/5.10.1-mpi
    [...]
    You will need to load all module(s) on any one of the lines below before the "ParaView/5.10.1-mpi" module is available to load.

      release/23.04  GCC/11.3.0  OpenMPI/4.1.4
      release/23.10  GCC/11.3.0  OpenMPI/4.1.4
    [...]
    ```

    Obviously, the `ParaView/5.10.1-mpi` module is available within two releases and depends in
    both cases on the two modules `GCC/11.3.0` and `OpenMPI/4.1.4`. Without loss of generality, a
    valid command to load `ParaView/5.10.1-mpi` is

    ```console
    marie@login$ module load release/23.10 GCC/11.3.0 OpenMPI/4.1.4
    ```

### Interactive Mode

There are two different ways of using ParaView interactively on ZIH systems, which are described
in more details in the following subsections:

- [GUI via NICE DCV](#using-the-gui-via-nice-dcv)
- [Client-Server mode with MPI-parallel off-screen-rendering](#using-client-server-mode-with-mpi-parallel-offscreen-rendering)

#### Using the GUI via NICE DCV

This option provides hardware accelerated OpenGL and might provide the best performance and smooth
handling. First, you need to open a DCV session on the Visualization cluster (use the
[Jupyter spawner](https://jupyterhub.hpc.tu-dresden.de/hub/spawn) and choose one of the *VIS* job
profiles, then click on the *DCV* tile in the lower section named *Other*). Please
find further instructions on how to start DCV on the [virtual desktops page](virtual_desktops.md).
In your virtual desktop session, start a terminal (right-click on desktop ->
Terminal or *Activities -> Terminal*), then load the ParaView module as usual and start the GUI:

```console
marie@dcv$ module load release/23.10 GCC/11.3.0 OpenMPI/4.1.4 ParaView/5.11.1-mpi
marie@dcv$ paraview
```

Since your DCV session already runs inside a job, which has been scheduled to a compute node, no
`srun` command is necessary here.

#### Using Client-Server Mode with MPI-parallel Offscreen-Rendering

ParaView has a built-in client-server architecture, where you run the GUI locally on your local
workstation and connect to a ParaView server instance (so-called *pvserver*) on a cluster. The
*pvserver* performs the computationally intensive rendering.

!!! note

    The ParaView version of the client, i.e., your workstation, needs to be of the same version as
    the server.

    Otherwise, you will encounter the error message "paraview client/server version hash mismatch"
    when connection.

The *pvserver* can be run in parallel using MPI. To do so, load the desired ParaView module and
start the `pvserver` executable in offscreen rendering mode within an interactive allocation via
`srun`.

???+ example "Start `pvserver`"

    Here, we ask for 8 MPI tasks on one node for 4 hours within an interactive allocation. Please
    adopt the time limit and ressources to your needs.

    ```console
    marie@login$ module load release/23.10 GCC/11.3.0 OpenMPI/4.1.4 ParaView/5.11.1-mpi
    marie@login$ srun --nodes=1 --ntasks=8 --mem-per-cpu=2500 --time=04:00:00 --pty pvserver --force-offscreen-rendering
    srun: job 1730359 queued and waiting for resources
    srun: job 1730359 has been allocated resources
    Waiting for client...
    Connection URL: cs://n1179:11111
    Accepting connection(s): n1179:11111
    ```

Once the resources are allocated, the `pvserver` is started in parallel and connection information
are printed.

!!! tip "Custom port"

    If the default port `11111` is already in use, an alternative or custom port can be specified
    via the commandline option `-sp=<PORT>` to `pvserver`.

The output from `pvserver` contains the node name which your job and server runs on. However, since
the node names of the cluster are not present in the public domain name system (only
cluster-internally), you cannot just use this line as-is for connection with your client. Instead,
you need to establish a so-called forward SSH tunnel to your local host. You first have to resolve
the name to an IP address on ZIH systems using `host` in another SSH session. Then, the SSH tunnel
can be created from your workstation. The following example will
depict both steps: Resolve the IP of the compute node and finally create a
forward SSH tunnel to local host on port 22222 (or what ever port is preferred).

???+ example "SSH tunnel"

    ```console
    marie@login$ host n1179
    n1179.barnard.hpc.tu-dresden.de has address 172.24.64.189
    marie@login$ exit
    Connection to login2.barnard.hpc.tu-dresden.de closed.
    marie@local$ ssh -L 22222:172.24.64.189:11111 barnard
    ```

!!! important "SSH command"

    The previous SSH command requires that you have already set up your SSH configuration to
    `Barnard` as documented on our
    [SSH configuration page](../access/ssh_login.md#configuring-default-parameters-for-ssh).

The **final step** is to start ParaView locally on your own machine and configure the connection to
the remote server. Click `File > Connect` to bring up the `Choose Server Configuration` dialog. When
you open this dialog for the first time, you need to add a server. Click on `Add Server` and
configure as follows:

![ParaView Edit Server Dialog](misc/paraview-edit-server.png)
{: align="center"}

In the `Configure` dialog, choose `Startup Type: Manual` and `Save`. Then, you can connect to the
remote `pvserver` via `Connect` button.

A successful connection is displayed by a *client connected* message displayed on the `pvserver`
process terminal, and within ParaView's Pipeline Browser (instead of it saying builtin). You now are
connected to the `pvserver` running on a compute node at ZIH systems and can open files from its
filesystems.

##### Caveats

Connecting to the compute nodes will only work when you are **inside the TU Dresden campus network**,
because otherwise, the private networks 172.24.\* will not be routed. That's why you either need to
use [VPN](https://tu-dresden.de/zih/dienste/service-katalog/arbeitsumgebung/zugang_datennetz/vpn)
(recommended) or an SSH tunnel.

??? tip "SSH Tunnel"

    When coming via the ZIH login gateway (`login1.zih.tu-dresden.de`), you can use SSH tunneling.
    For the example IP address from above, this could look like the following:

    ```console
    marie@local$ ssh -f -N -L11111:172.24.64.189:11111 <zihlogin>@login1.zih.tu-dresden.de
    ```

    This command opens the port 11111 locally and tunnels it via `login1` to the `pvserver` running on
    the compute node. Note that you then must instruct your local ParaView client to connect to host
    `localhost` instead. The recommendation, though, is to use VPN, which makes this extra step
    unnecessary.

### Batch Mode (`pvbatch`)

ParaView can run in batch mode, i.e., without opening the ParaView GUI, executing a Python script.
This way, common visualization tasks can be automated. There are two Python interfaces: `pvpython`
and `pvbatch`. The interface `pvbatch` only accepts commands from input scripts, and it will run in
parallel, if it was built using MPI.

!!! note

    ParaView is shipped with a prebuild MPI library and **pvbatch has to be
    invoked using this very mpiexec** command. Make sure to not use `srun`
    or `mpiexec` from another MPI module, i.e., check what `mpiexec` is in
    the path:

    ```console
    marie@login$ module load release/23.10 GCC/11.3.0 OpenMPI/4.1.4 ParaView/5.11.1-mpi
    marie@login$ which mpiexec
    /software/rapids/r23.10/OpenMPI/4.1.4-GCC-11.3.0/bin/mpiexec
    ```

The resources for the MPI processes have to be allocated via the
[batch system](../jobs_and_resources/slurm.md) option `--cpus-per-task=<NUM>` (not `--ntasks=<NUM>`,
as it would be usual for MPI processes). It might be valuable in terms of runtime to bind/pin the
MPI processes to hardware. A convenient option is `--bind-to core`. All other options can be
obtained by

```console
marie@login$ mpiexec --help binding
```

or from
[mpich wiki](https://wiki.mpich.org/mpich/index.php/Using_the_Hydra_Process_Manager#Process-core_Binding%7Cwiki.mpich.org).

In the following, we provide two examples on how to use `pvbatch` from within a job file and an
interactive allocation.

??? example "Example job file"

    ```Bash
    #!/bin/bash

    #SBATCH --nodes=1
    #SBATCH --cpus-per-task=12
    #SBATCH --time=01:00:00

    # Make sure to only use ParaView
    module purge
    module load release/23.10 GCC/11.3.0 OpenMPI/4.1.4 ParaView/5.11.1-mpi

    pvbatch --mpi --force-offscreen-rendering pvbatch-script.py
    ```

??? example "Example of interactive allocation using `salloc`"

    ```console
    marie@login$ salloc --nodes=1 --cpus-per-task=16 --time=01:00:00 bash
    salloc: Pending job allocation 336202
    salloc: job 336202 queued and waiting for resources
    salloc: job 336202 has been allocated resources
    [...]

    # Make sure to only use ParaView
    marie@compute$ module purge
    marie@compute$ module load release/23.10 GCC/11.3.0 OpenMPI/4.1.4 ParaView/5.11.1-mpi

    # Go to working directory, e.g., your workspace
    marie@compute$ cd /path/to/workspace

    # Execute pvbatch using 16 MPI processes in parallel on allocated resources
    marie@compute$ pvbatch --mpi --force-offscreen-rendering pvbatch-script.py
    ```

#### Using GPUs

ParaView `pvbatch` can render offscreen through the Native Platform Interface (EGL) on the graphics
cards (GPUs) specified by the device index. For that, make sure to use a cluster with GPUs
(e.g. `Alpha`, `Power9`; see [hardware overview](../jobs_and_resources/hardware_overview.md)),
and pass the option `--displays $CUDA_VISIBLE_DEVICES` to `pvbatch`.

??? example "Example job file"

    ```Bash
    #!/bin/bash

    #SBATCH --nodes=1
    #SBATCH --cpus-per-task=12
    #SBATCH --gres=gpu:2
    #SBATCH --time=01:00:00

    # Make sure to only use ParaView
    module purge
    module load release/23.10 GCC/11.3.0 OpenMPI/4.1.4 ParaView/5.11.1-mpi

    pvbatch --mpi --displays $CUDA_VISIBLE_DEVICES --force-offscreen-rendering pvbatch-script.py
    ```


# ZSH as Alternative Shell

!!! warning
    Though all efforts have been made to ensure the accuracy and
    currency of the content on this website, please be advised that
    some content might be out of date and there is no continuous
    website support available. In case of any ambiguity or doubts,
    users are advised to do their own research on the content's
    accuracy and currency.

The [ZSH](https://www.zsh.org), short for `z-shell`, is an alternative shell for Linux that offers
many convenience features for productive use that `bash`, the default shell, does not offer.

This should be a short introduction to `zsh` and offer some examples that are especially useful
on ZIH systems.

## `oh-my-zsh`

`oh-my-zsh` is a plugin that adds many features to the `zsh` with a very simple install. Simply run:

```
marie@login$ sh -c "$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)"
```

and then, if it is not already your login shell, run `zsh` or re-login.

The rest of this document assumes that you have `oh-my-zsh` installed and running.

## Features

### Themes

There are many different themes for the `zsh`. See the
[GitHub-page of `oh-my-zsh`](https://github.com/ohmyzsh/ohmyzsh) for more details.

### Auto-completion

`zsh` offers more auto-completion features than `bash`. You can auto-complete programs, filenames, parameters,
`man`-pages and a lot more, and you can cycle through the suggestions with `TAB`-button.

![Cycling through auto-completion for parameter names](misc/zsh_autocomplete_parameters.png)

### Syntax-highlighting

When you add this line to your `~/.zshrc` with `oh-my-zsh` installed, you get syntax-highlighting directly
in the shell:

```bash
plugins+=(
  zsh-syntax-highlighting
)
```

![Syntax-highlighting directly in the shell](misc/zsh_syntax_highlighting.png)

### Typo-correction

With

```bash
setopt correct_all
ENABLE_CORRECTION="true"
```

in `~/.zshrc` you get correction suggestions when the shell thinks
that it might be what you want, e.g. when a command
is expected to be handed an existing file.

![Correction suggestion](misc/zsh_typo.png)

### Automatic `cd`

Adding `AUTO_CD` to `~/.zshrc` file allows to leave out the `cd` when a folder name is provided.

```bash
setopt AUTO_CD
```

![Automatic cd](misc/zsh_autocd.png)

### `fish`-like auto-suggestions

Install [`zsh-autosuggestions`](https://github.com/zsh-users/zsh-autosuggestions) to get `fish`-shell-like
auto-suggestions of previous commands that start with the same letters and that you can complete with
the right arrow key.

![Auto-suggestion](misc/zsh_autosuggestion.png)

??? example "Addons for your shell"
    === "`bash`"
        ```bash
        # Create a new directory and directly `cd` into it
        mcd () {
            mkdir -p $1
            cd $1
        }

        # Find the largest files in the current directory easily
        function treesizethis {
            du -k --max-depth=1 | sort -nr | awk '
             BEGIN {
            split("KB,MB,GB,TB", Units, ",");
             }
             {
            u = 1;
            while ($1 >= 1024) {
               $1 = $1 / 1024;
               u += 1
            }
            $1 = sprintf("%.1f %s", $1, Units[u]);
            print $0;
             }
            '
        }

        #This allows you to run `slurmlogpath $SLURM_ID` and get the log-path directly in stdout:
        function slurmlogpath {
            scontrol show job $1 | sed -n -e 's/^\s*StdOut=//p'
        }

        # `ftails` follow-tails a slurm-log. Call it without parameters to tail the only running job or
        # get a list of running jobs or use `ftails $JOBID` to tail a specific job
        function ftails {
            JOBID=$1
            if [[ -z $JOBID ]]; then
                 JOBS=$(squeue --format="%i \\'%j\\' " --me | grep -v JOBID)
                 NUMBER_OF_JOBS=$(echo "$JOBS" | wc -l)
                 JOBID=
                 if [[ "$NUMBER_OF_JOBS" -eq 1 ]]; then
                     JOBID=$(echo $JOBS | sed -e "s/'//g" | sed -e 's/ .*//')
                 else
                     JOBS=$(echo $JOBS | tr -d '\n')
                     JOBID=$(eval "whiptail --title 'Choose jobs to tail' --menu 'Choose Job to tail' 25 78 16 $JOBS" 3>&1 1>&2 2>&3)
                 fi
            fi
            SLURMLOGPATH=$(slurmlogpath $JOBID)
            if [[ -e $SLURMLOGPATH ]]; then
                tail -n100 -f $SLURMLOGPATH
            else
                echo "No slurm-log-file found"
            fi
        }

        #With this, you only need to type `sq` instead of `squeue -u $USER`.
        alias sq="squeue --me"
        ```
    === "`zsh`"
        ```bash
        # Create a new directory and directly `cd` into it
        mcd () {
            mkdir -p $1
            cd $1
        }

        # Find the largest files in the current directory easily
        function treesizethis {
            du -k --max-depth=1 | sort -nr | awk '
             BEGIN {
            split("KB,MB,GB,TB", Units, ",");
             }
             {
            u = 1;
            while ($1 >= 1024) {
               $1 = $1 / 1024;
               u += 1
            }
            $1 = sprintf("%.1f %s", $1, Units[u]);
            print $0;
             }
            '
        }

        #This allows you to run `slurmlogpath $SLURM_ID` and get the log-path directly in stdout:
        function slurmlogpath {
            scontrol show job $1 | sed -n -e 's/^\s*StdOut=//p'
        }

        # `ftails` follow-tails a slurm-log. Call it without parameters to tail the only running job or
        # get a list of running jobs or use `ftails $JOBID` to tail a specific job
        function ftails {
            JOBID=$1
            if [[ -z $JOBID ]]; then
                 JOBS=$(squeue --format="%i \\'%j\\' " --me | grep -v JOBID)
                 NUMBER_OF_JOBS=$(echo "$JOBS" | wc -l)
                 JOBID=
                 if [[ "$NUMBER_OF_JOBS" -eq 1 ]]; then
                     JOBID=$(echo $JOBS | sed -e "s/'//g" | sed -e 's/ .*//')
                 else
                     JOBS=$(echo $JOBS | tr -d '\n')
                     JOBID=$(eval "whiptail --title 'Choose jobs to tail' --menu 'Choose Job to tail' 25 78 16 $JOBS" 3>&1 1>&2 2>&3)
                 fi
            fi
            SLURMLOGPATH=$(slurmlogpath $JOBID)
            if [[ -e $SLURMLOGPATH ]]; then
                tail -n100 -f $SLURMLOGPATH
            else
                echo "No slurm-log-file found"
            fi
        }

        #With this, you only need to type `sq` instead of `squeue -u $USER`.
        alias sq="squeue --me"

        #This will automatically replace `...` with `../..` and `....` with `../../..`
        # and so on (each additional `.` adding another `/..`) when typing commands:
        rationalise-dot() {
            if [[ $LBUFFER = *.. ]]; then
                LBUFFER+=/..
            else
                LBUFFER+=.
            fi
        }
        zle -N rationalise-dot
        bindkey . rationalise-dot

        # This allows auto-completion for `module load`:
        function _module {
            MODULE_COMMANDS=(
                '-t:Show computer parsable output'
                'load:Load a module'
                'unload:Unload a module'
                'spider:Search for a module'
                'avail:Show available modules'
                'list:List loaded modules'
            )

            MODULE_COMMANDS_STR=$(printf "\n'%s'" "${MODULE_COMMANDS[@]}")

            eval "_describe 'command' \"($MODULE_COMMANDS_STR)\""
            _values -s ' ' 'flags' $(ml -t avail | sed -e 's#/$##' | tr '\n' ' ')
        }

        compdef _module "module"
        ```

## Setting `zsh` as default-shell

Please ask HPC support if you want to set the `zsh` as your default login shell.


# User Support

## Create a Ticket

The best way to ask for help send a message to
[hpc-support@tu-dresden.de](mailto:hpc-support@tu-dresden.de) with a
detailed description of your problem.

It should include:

- Who is reporting? (login name)
- Where have you seen the problem? (name of the HPC system and/or of the node)
- When has the issue occurred? Maybe, when did it work last?
- What exactly happened?

If possible include

- job ID,
- batch script,
- filesystem path,
- loaded modules and environment,
- output and error logs,
- steps to reproduce the error.

This email automatically opens a trouble ticket which will be tracked by the HPC team. Please
always keep the ticket number in the subject on your answers so that our system can keep track
on our communication.

For a new request, please simply send a new email (without any ticket number).

!!! hint "Please try to find an answer in this documentation first."

## Open Q&A Sessions

We invite you to join our public Q&A sessions with any questions you may have:

* Open Q&A session for users of the NHR@TUD HPC systems.  
   Biweekly on Mondays from 1.30 - 2.30 pm.

   See [ZIH calendar](https://tu-dresden.de/zih/die-einrichtung/termine)
   for the next event or download the
   [Q&A event series](https://tu-dresden.de/zih/die-einrichtung/termine/termine/qa-session-nhr-at-tud/ics_view)
   to your calendar.

* Open AI Q&A session as a joint initiative for users of NHR and GCS centers.  
   Every Thursday from 2:00 - 3:00 pm.

   See [NHR - AI on High Performance Computers](https://www.nhr-verein.de/ki-auf-hochleistungsrechnern)
   for further details.  
   Join the [zoom session](https://www.nhr-verein.de/en/ai-supercomputers) now.
