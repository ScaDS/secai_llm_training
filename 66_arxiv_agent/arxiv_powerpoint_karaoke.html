
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>PowerPoint Karaoke about Arxiv papers &#8212; SECAI LLM Training</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '66_arxiv_agent/arxiv_powerpoint_karaoke';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Imprint" href="../imprint.html" />
    <link rel="prev" title="Function calling using ScaDS.AI’s LLM service" href="../50_function_calling/12_function_calling_scadsai_llm.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../intro.html">

  
  
  
  
  
  
  

  
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../00_setup/readme.html">
                        Setting up your computer
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../15_endpoint_apis/01_scadsai_llm_endpoint.html">
                        ScaDS.AI LLM endpoint
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../15_endpoint_apis/02_ollama_endpoint.html">
                        Ollama
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../15_endpoint_apis/06_kisski_endpoint.html">
                        KISSKI / GWDG endpoint
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../15_endpoint_apis/03_blablador_endpoint.html">
                        Blablador endpoint
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../20_chatbots/readme.html">
                        Chatbots
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../33_chat_with_docs/readme.html">
                        Retreivela Augmented Generation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../33_chat_with_docs/chat-with-docs.html">
                        Chat with your PDFs!
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../33_chat_with_docs/hpc-compendium-prepare-data.html">
                        Chat with documentation about HPC systems
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../33_chat_with_docs/long-context-prompting.html">
                        Long context prompting
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../33_chat_with_docs/chat-with-hpc-compendium.html">
                        Ask about the HPC compendium
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../33_chat_with_docs/llm-based-rag.html">
                        LLM-based Retrieval Augmented Generation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../47_vision/22_vision_qwen.html">
                        Qwen2-VL
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../47_vision/20_vision_ollama.html">
                        LLAVA
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../50_function_calling/12_function_calling_scadsai_llm.html">
                        Function calling using ScaDS.AI’s LLM service
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="#">
                        PowerPoint Karaoke about Arxiv papers
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../imprint.html">
                        Imprint
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../00_setup/readme.html">
                        Setting up your computer
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../15_endpoint_apis/01_scadsai_llm_endpoint.html">
                        ScaDS.AI LLM endpoint
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../15_endpoint_apis/02_ollama_endpoint.html">
                        Ollama
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../15_endpoint_apis/06_kisski_endpoint.html">
                        KISSKI / GWDG endpoint
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../15_endpoint_apis/03_blablador_endpoint.html">
                        Blablador endpoint
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../20_chatbots/readme.html">
                        Chatbots
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../33_chat_with_docs/readme.html">
                        Retreivela Augmented Generation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../33_chat_with_docs/chat-with-docs.html">
                        Chat with your PDFs!
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../33_chat_with_docs/hpc-compendium-prepare-data.html">
                        Chat with documentation about HPC systems
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../33_chat_with_docs/long-context-prompting.html">
                        Long context prompting
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../33_chat_with_docs/chat-with-hpc-compendium.html">
                        Ask about the HPC compendium
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../33_chat_with_docs/llm-based-rag.html">
                        LLM-based Retrieval Augmented Generation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../47_vision/22_vision_qwen.html">
                        Qwen2-VL
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../47_vision/20_vision_ollama.html">
                        LLAVA
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../50_function_calling/12_function_calling_scadsai_llm.html">
                        Function calling using ScaDS.AI’s LLM service
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="#">
                        PowerPoint Karaoke about Arxiv papers
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../imprint.html">
                        Imprint
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item">
  


<a class="navbar-brand logo" href="../intro.html">

  
  
  
  
  
  
  

  
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    </div>
    <div class="sidebar-start-items__item">
<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    SECAI LLM Training
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Setup</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../00_setup/readme.html">Setting up your computer</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../15_endpoint_apis/01_scadsai_llm_endpoint.html">ScaDS.AI LLM endpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15_endpoint_apis/02_ollama_endpoint.html">Ollama</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15_endpoint_apis/06_kisski_endpoint.html">KISSKI / GWDG endpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../15_endpoint_apis/03_blablador_endpoint.html">Blablador endpoint</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../20_chatbots/readme.html">Chatbots</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../20_chatbots/gui.html">A Chatbot GUI</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Retrieval Augmented Generation (RAG)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../33_chat_with_docs/readme.html">Retreivela Augmented Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../33_chat_with_docs/chat-with-docs.html">Chat with your PDFs!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../33_chat_with_docs/hpc-compendium-prepare-data.html">Chat with documentation about HPC systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../33_chat_with_docs/long-context-prompting.html">Long context prompting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../33_chat_with_docs/chat-with-hpc-compendium.html">Ask about the HPC compendium</a></li>
<li class="toctree-l1"><a class="reference internal" href="../33_chat_with_docs/llm-based-rag.html">LLM-based Retrieval Augmented Generation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Multi-modal LLMs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../47_vision/22_vision_qwen.html">Qwen2-VL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../47_vision/20_vision_ollama.html">LLAVA</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Agents</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../50_function_calling/12_function_calling_scadsai_llm.html">Function calling using ScaDS.AI’s LLM service</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">PowerPoint Karaoke about Arxiv papers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Links</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../imprint.html">Imprint</a></li>

</ul>

    </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        <label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" data-toggle="tooltip" data-placement="right" title="Toggle primary sidebar">
            <span class="fa-solid fa-bars"></span>
        </label>
    </div>
    <div class="header-article__right">


<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
  </ul>
</div>

<button onclick="toggleFullScreen()"
  class="btn btn-sm"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<div class="dropdown dropdown-repository-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="https://github.com/ScaDS/secai_llm_training" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">repository</span>
</a>
</a>
      
      <li><a href="https://github.com/ScaDS/secai_llm_training/issues/new?title=Issue%20on%20page%20%2F66_arxiv_agent/arxiv_powerpoint_karaoke.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">open issue</span>
</a>
</a>
      
  </ul>
</div>



<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="../_sources/66_arxiv_agent/arxiv_powerpoint_karaoke.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</a>
      
      <li>
<button onclick="printPdf(this)"
  class="btn btn-sm dropdown-item"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</a>
      
  </ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary" data-toggle="tooltip" data-placement="left" title="Toggle secondary sidebar">
            <span class="fa-solid fa-list"></span>
        </label>
    </div>
</div>
            </div>
            
            

<div id="jb-print-docs-body" class="onlyprint">
    <h1>PowerPoint Karaoke about Arxiv papers</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise">
   Exercise
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Exercise
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>

            <article class="bd-article" role="main">
              
  <section class="tex2jax_ignore mathjax_ignore" id="powerpoint-karaoke-about-arxiv-papers">
<h1>PowerPoint Karaoke about Arxiv papers<a class="headerlink" href="#powerpoint-karaoke-about-arxiv-papers" title="Permalink to this heading">#</a></h1>
<p>In this notebook we program an agent that is capable of generating PowerPoint slide decks out of Arxiv papers. We will use the <a class="reference external" href="https://github.com/huggingface/smolagents">smolagents</a> framework for programming the agent.</p>
<p>We will use the <a class="reference external" href="https://llm.scads.ai/">ScaDS.AI LLM infrastructure</a> infrastructure at the <a class="reference external" href="https://tu-dresden.de/zih">Center for Information Services and High Performance Computing (ZIH) of TU Dresden</a>. To use it, you must be connected via <a class="reference external" href="https://tu-dresden.de/zih/dienste/service-katalog/arbeitsumgebung/zugang_datennetz/vpn">TU Dresden VPN</a> and have your API key stored in a <code class="docutils literal notranslate"><span class="pre">SCADSAI_API_KEY</span></code> environment variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Markdown</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utilities</span><span class="w"> </span><span class="kn">import</span> <span class="n">convert_to_markdown</span><span class="p">,</span> <span class="n">search_arxiv</span><span class="p">,</span> <span class="n">download_pdf</span><span class="p">,</span> <span class="n">pdf_to_markdown</span><span class="p">,</span> <span class="n">make_powerpoint_slides</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">smolagents.agents</span><span class="w"> </span><span class="kn">import</span> <span class="n">ToolCallingAgent</span><span class="p">,</span> <span class="n">CodeAgent</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">smolagents</span><span class="w"> </span><span class="kn">import</span> <span class="n">tool</span><span class="p">,</span> <span class="n">LiteLLMModel</span>
</pre></div>
</div>
</div>
</div>
<p>First, we initialize the LLM. The server supports the OpenAI-API.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LiteLLMModel</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="s2">&quot;openai/meta-llama/Llama-3.3-70B-Instruct&quot;</span><span class="p">,</span> 
                         <span class="n">api_base</span><span class="o">=</span><span class="s2">&quot;https://llm.scads.ai/v1&quot;</span><span class="p">,</span> 
                         <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;SCADSAI_API_KEY&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we specify tools. The actual functionality is programmed in <span class="xref myst">arxiv_utilities.py</span>. Note: To make these functions work, they require detailed docstrings describing precisely what parameters the functions require.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tools</span> <span class="o">=</span> <span class="p">[]</span>

<span class="nd">@tools</span><span class="o">.</span><span class="n">append</span>
<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">search_publications</span><span class="p">(</span><span class="n">query</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">author</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">year</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_results</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Searches the arxiv for papers using a query, selects papers from given authors and/or by year.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        query: Search terms</span>
<span class="sd">        author: Author(s) of the searched items</span>
<span class="sd">        year: publication year</span>
<span class="sd">        max_results: maximum number of results</span>

<span class="sd">    Returns:</span>
<span class="sd">        Found paper(s)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Searching...&quot;</span><span class="p">)</span>
    <span class="n">papers</span> <span class="o">=</span> <span class="n">search_arxiv</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="n">author</span><span class="p">,</span> <span class="n">year</span><span class="o">=</span><span class="n">year</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="n">max_results</span><span class="p">)</span>
    <span class="n">markdown</span> <span class="o">=</span> <span class="n">convert_to_markdown</span><span class="p">(</span><span class="n">papers</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">markdown</span>

<span class="nd">@tools</span><span class="o">.</span><span class="n">append</span>
<span class="nd">@tool</span>
<span class="k">def</span><span class="w"> </span><span class="nf">download_paper</span><span class="p">(</span><span class="n">paper_link</span><span class="p">:</span><span class="nb">str</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Downloads a paper and return its contents as markdown.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        paper_link: url of the paper to be downloaded</span>

<span class="sd">    Returns:</span>
<span class="sd">        Content of the paper</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Downloading...&quot;</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>

    <span class="n">filename</span> <span class="o">=</span> <span class="n">download_pdf</span><span class="p">(</span><span class="n">paper_link</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">filename</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pdf_to_markdown</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>

<span class="c1"># You can also add external tools like this.</span>
<span class="n">tools</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tool</span><span class="p">(</span><span class="n">make_powerpoint_slides</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We can then initialize the agent.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span> <span class="o">=</span> <span class="n">CodeAgent</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                 <span class="n">additional_authorized_imports</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;json&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># comment this to show detailed output</span>
<span class="c1">#from smolagents.utils import console</span>
<span class="c1">#console.quiet = True</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">I need to give a presentation about the latest arxiv paper from the year 2022 that was about LLMs.</span>
<span class="s2">Please make a powerpoint slide deck about this paper.</span>
<span class="s2">The first slide should have the same title as the paper, and mention the authors, and give a link to the paper.</span>
<span class="s2">The following slides are about the individual chapters of the paper.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">response</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #d4b702; text-decoration-color: #d4b702">╭──────────────────────────────────────────────────── </span><span style="color: #d4b702; text-decoration-color: #d4b702; font-weight: bold">New run</span><span style="color: #d4b702; text-decoration-color: #d4b702"> ────────────────────────────────────────────────────╮</span>
<span style="color: #d4b702; text-decoration-color: #d4b702">│</span>                                                                                                                 <span style="color: #d4b702; text-decoration-color: #d4b702">│</span>
<span style="color: #d4b702; text-decoration-color: #d4b702">│</span> <span style="font-weight: bold">I need to give a presentation about the latest arxiv paper from the year 2022 that was about LLMs.</span>              <span style="color: #d4b702; text-decoration-color: #d4b702">│</span>
<span style="color: #d4b702; text-decoration-color: #d4b702">│</span> <span style="font-weight: bold">Please make a powerpoint slide deck about this paper.</span>                                                           <span style="color: #d4b702; text-decoration-color: #d4b702">│</span>
<span style="color: #d4b702; text-decoration-color: #d4b702">│</span> <span style="font-weight: bold">The first slide should have the same title as the paper, and mention the authors, and give a link to the paper.</span> <span style="color: #d4b702; text-decoration-color: #d4b702">│</span>
<span style="color: #d4b702; text-decoration-color: #d4b702">│</span> <span style="font-weight: bold">The following slides are about the individual chapters of the paper.</span>                                            <span style="color: #d4b702; text-decoration-color: #d4b702">│</span>
<span style="color: #d4b702; text-decoration-color: #d4b702">│</span>                                                                                                                 <span style="color: #d4b702; text-decoration-color: #d4b702">│</span>
<span style="color: #d4b702; text-decoration-color: #d4b702">╰─ LiteLLMModel - openai/meta-llama/Llama-3.3-70B-Instruct ───────────────────────────────────────────────────────╯</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #d4b702; text-decoration-color: #d4b702">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style="font-weight: bold">Step </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span><span style="color: #d4b702; text-decoration-color: #d4b702"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"> ─ <span style="font-weight: bold">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── 
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">paper_link </span><span style="color: #ff4689; text-decoration-color: #ff4689; background-color: #272822">=</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822"> search_publications(query</span><span style="color: #ff4689; text-decoration-color: #ff4689; background-color: #272822">=</span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"LLMs"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">, year</span><span style="color: #ff4689; text-decoration-color: #ff4689; background-color: #272822">=</span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"2022"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">, max_results</span><span style="color: #ff4689; text-decoration-color: #ff4689; background-color: #272822">=</span><span style="color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822">1</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">)</span><span style="background-color: #272822">                                     </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">print(paper_link)</span><span style="background-color: #272822">                                                                                              </span>  
 ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── 
</pre>
</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Search
Searched # Search Results

## Paper 1
**Title:** Rethinking with Retrieval: Faithful Large Language Model Inference

**Authors:** Hangfeng He, Hongming Zhang, Dan Roth

**Published:** 2022-12-31T22:35:34Z

**Summary:** Despite the success of large language models (LLMs) in various natural
language processing (NLP) tasks, the stored knowledge in these models may
inevitably be incomplete, out-of-date, or incorrect. This motivates the need to
utilize external knowledge to assist LLMs. Unfortunately, current methods for
incorporating external knowledge often require additional training or
fine-tuning, which can be costly and may not be feasible for LLMs. To address
this issue, we propose a novel post-processing approach, rethinking with
retrieval (RR), which retrieves relevant external knowledge based on the
decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting.
This lightweight approach does not require additional training or fine-tuning
and is not limited by the input length of LLMs. We evaluate the effectiveness
of RR through extensive experiments with GPT-3 on three complex reasoning
tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our
results show that RR can produce more faithful explanations and improve the
performance of LLMs.

**Link:** [Read More](http://arxiv.org/abs/2301.00303v1)
</pre></div>
</div>
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Execution logs:</span>
# Search Results

## Paper 1
**Title:** Rethinking with Retrieval: Faithful Large Language Model Inference

**Authors:** Hangfeng He, Hongming Zhang, Dan Roth

**Published:** 2022-12-31T22:35:34Z

**Summary:** Despite the success of large language models (LLMs) in various natural
language processing (NLP) tasks, the stored knowledge in these models may
inevitably be incomplete, out-of-date, or incorrect. This motivates the need to
utilize external knowledge to assist LLMs. Unfortunately, current methods for
incorporating external knowledge often require additional training or
fine-tuning, which can be costly and may not be feasible for LLMs. To address
this issue, we propose a novel post-processing approach, rethinking with
retrieval (RR), which retrieves relevant external knowledge based on the
decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting.
This lightweight approach does not require additional training or fine-tuning
and is not limited by the input length of LLMs. We evaluate the effectiveness
of RR through extensive experiments with GPT-3 on three complex reasoning
tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our
results show that RR can produce more faithful explanations and improve the
performance of LLMs.

**Link:** [Read More](http://arxiv.org/abs/2301.00303v1)



Out: None
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">[Step 1: Duration 5.66 seconds| Input tokens: 2,633 | Output tokens: 109]</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #d4b702; text-decoration-color: #d4b702">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style="font-weight: bold">Step </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span><span style="color: #d4b702; text-decoration-color: #d4b702"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"> ─ <span style="font-weight: bold">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── 
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">paper_contents </span><span style="color: #ff4689; text-decoration-color: #ff4689; background-color: #272822">=</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822"> download_paper(paper_link</span><span style="color: #ff4689; text-decoration-color: #ff4689; background-color: #272822">=</span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"http://arxiv.org/abs/2301.00303v1"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">)</span><span style="background-color: #272822">                                </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">print(paper_contents)</span><span style="background-color: #272822">                                                                                          </span>  
 ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── 
</pre>
</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Download
</pre></div>
</div>
<p>PDF downloaded: <a class="reference external" href="pdf_filename">http://arxiv.org/abs/2301.00303v1</a>, licensed CC-BY 4.0</p>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading 2301.00303v1.pdf
</pre></div>
</div>
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Execution logs:</span>

# Page 1

Rethinking with Retrieval: Faithful Large Language Model Inference

Hangfeng He†∗ Hongming Zhang‡ Dan Roth§

†University of Rochester ‡Tencent AI Lab, Seattle §University of Pennsylvania

hanfeng.he@rochester.edu, hongmzhang@global.tencent.com

danroth@seas.upenn.edu

Abstract

Despite the success of large language mod-

els (LLMs) in various natural language pro-

cessing (NLP) tasks, the stored knowledge

in these models may inevitably be incom-

plete, out-of-date, or incorrect. This mo-

tivates the need to utilize external knowl-

edge to assist LLMs. Unfortunately, current

methods for incorporating external knowl-

edge often require additional training or

ﬁne-tuning, which can be costly and may

not be feasible for LLMs. To address this

issue, we propose a novel post-processing

approach, rethinking with retrieval (RR),

which retrieves relevant external knowledge

based on the decomposed reasoning steps

obtained from the chain-of-thought (CoT)

prompting. This lightweight approach does

not require additional training or ﬁne-tuning

and is not limited by the input length of

LLMs. We evaluate the effectiveness of RR

through extensive experiments with GPT-3

on three complex reasoning tasks: common-

sense reasoning, temporal reasoning, and

tabular reasoning. Our results show that RR

can produce more faithful explanations and

improve the performance of LLMs.1

1 Introduction

Large language models (LLMs) have shown

exceptional performance across various tasks

through in-context learning without task-speciﬁc

training or ﬁne-tuning (Brown et al., 2020;

Chowdhery et al., 2022; Zhang et al., 2022;

Ouyang et al., 2022). Recent progress in prompt-

ing (Wei et al., 2022; Zhou et al., 2022; Kojima

et al., 2022) and decoding (Wang et al., 2022) has

made it feasible for LLMs to tackle tasks that de-

mand complex reasoning.

∗Part of this work was done while the author was at the

University of Pennsylvania.

1Our code is publicly available at https://github.

com/HornHehhf/RR.

Query PredictionLLM

Query Explanation + PredictionLLM

Query Explanation + PredictionLLM

(a)

(b)

(c)

Knowledge

Chain of thought

Chain of thoughtRetrievalRethinking

Figure 1: An overview of three approaches for using

LLMs: (a) Standard prompting for generating a pre-

diction in response to a query. (b) Chain-of-thought

prompting for generating both an explanation and a

prediction in response to a query. (c) Rethinking with

retrieval, our proposed approach for using the decom-

posed reasoning steps obtained from chain-of-thought

prompting to retrieve relevant external knowledge for

LLMs, leading to more faithful explanations and im-

proved predictions in response to a query.

However, the knowledge stored in LLMs might

inevitably be incomplete, out-of-date, or incorrect.

As a result, external sources of knowledge, such

as Wikipedia, may be essential for the success-

ful deployment of LLMs for real-world applica-

tions. Previously, people tried to utilize knowl-

edge for smaller language models (LMs), such

as T5 (Raffel et al., 2020), BERT (Devlin et al.,

2019), and RoBERTa (Liu et al., 2019). However,

these methods often require additional training or

ﬁne-tuning, which can be costly and thus imprac-

tical for LLMs.

In this paper, we present a post-processing

approach called rethinking with retrieval (RR)

for utilizing external knowledge in LLMs. Our

method begins by using the chain-of-thought

(CoT) prompting method (Wei et al., 2022) to gen-

erate a diverse set of reasoning paths, as described

in Wang et al. (2022). We then use each rea-

soning step in those paths to retrieve relevant ex-

ternal knowledge, which enables RR to provide

arXiv:2301.00303v1  [cs.CL]  31 Dec 2022


# Page 2

more faithful explanations and more accurate pre-

dictions, as illustrated in Figure 1.

We evaluate the effectiveness of our proposed

method, RR, on three complex reasoning tasks:

commonsense reasoning, temporal reasoning, and

tabular reasoning, using GPT-3 175B (Brown

et al., 2020) and different external knowledge

sources: Wikipedia, Wikidata (Vrande ˇci´c and

Krötzsch, 2014), WordNet (Miller, 1995), and

Conceptnet (Speer et al., 2017). The results

demonstrate that RR consistently outperforms all

baselines on all three tasks without requiring ad-

ditional training or ﬁne-tuning, indicating the su-

periority of our approach in leveraging external

knowledge to enhance the performance of LLMs.

2 Related Work

Enhancing LMs through retrieval. Retrieval-

enhanced LMs have received signiﬁcant attention

as a means of improving performance through the

incorporation of external knowledge. For exam-

ple, the k-most similar training contexts can be re-

trieved to improve the estimation of the next word

distribution in both the training stage (Borgeaud

et al., 2021) and the inference stage (Khandelwal

et al., 2020). Furthermore, search query genera-

tors have been adopted to generate search queries

for search engines to retrieve relevant documents

(Komeili et al., 2022; Shuster et al., 2022; Thop-

pilan et al., 2022). Other approaches have uti-

lized retrieved documents as the additional con-

text in generation tasks (Joshi et al., 2020; Guu

et al., 2020; Lewis et al., 2020). Nakano et al.

(2021) instead use human feedback in a text-based

web-browsing environment. Among these pre-

vious works, Khandelwal et al. (2020) is most

closely related to our approach. However, they

focus on improving local inference by using the

nearest neighbor datastore constructed from train-

ing data, whereas we focus on conducting faith-

ful inference using external knowledge. In con-

trast to other aforementioned approaches, which

require training or ﬁne-tuning to incorporate re-

trieved knowledge, we propose a post-processing

method for leveraging retrieved knowledge with-

out additional training or ﬁne-tuning.

Incorporating external knowledge into LMs.

Signiﬁcant effort has been devoted to leveraging

external knowledge to improve the reasoning abil-

ity of LMs. Previous work has incorporated exter-

nal knowledge sources such as WordNet (Miller,

1995) and ConceptNet (Speer et al., 2017) to en-

hance LMs for tabular reasoning tasks (Neeraja

et al., 2021; Varun et al., 2022). Explicit rules

have also been added to inputs to improve rea-

soning ability over implicit knowledge (Talmor

et al., 2020). In addition, explicit knowledge from

Wikidata (Vrandeˇci´c and Krötzsch, 2014) and im-

plicit knowledge in LLMs have been integrated

into a transformer (Vaswani et al., 2017) for vi-

sual question answering (Gui et al., 2021). Nye

et al. (2021) instead introduces a symbolic reason-

ing module to improve coherence and consistency

in LLMs. Among these previous works, Nye et al.

(2021) is the most relevant to our approach. Still,

they focus on incorporating logical constraints to

improve coherence and consistency, whereas we

aim to improve the faithfulness of explanations

through the use of external knowledge. In con-

trast to other aforementioned approaches that in-

corporate external knowledge before generation

and require additional training or ﬁne-tuning, our

proposal leverages external knowledge in a post-

processing manner to enhance LMs without addi-

tional training or ﬁne-tuning.

Uncovering latent Knowledge in LLMs. There

has been a line of work exploring the knowledge

hidden within LLMs for reasoning. This has in-

cluded the use of careful prompting to encourage

LLMs to generate explanations in the reasoning

process, such as through chain of thought prompt-

ing in few-shot (Wei et al., 2022) or zero-shot

(Kojima et al., 2022) learning, or through the use

of scratchpads for intermediate computation (Nye

et al., 2022). In addition, various methods based

on sampling a diverse set of reasoning paths in

LLMs have been proposed, including training ver-

iﬁers to judge the correctness of model comple-

tions (Cobbe et al., 2021), calibrating model pre-

dictions based on the reliability of the explana-

tions (Ye and Durrett, 2022), and promoting self-

consistency over diverse reasoning paths (Wang

et al., 2022). Zelikman et al. (2022) instead it-

eratively bootstrap the ability of LLMs to gener-

ate high-quality rationales from a few initial ex-

amples. Liu et al. (2022) further propose generat-

ing knowledge from LLMs, which is then used as

additional input to improve commonsense reason-

ing. In contrast to this line of work, our proposal

focuses on leveraging external knowledge to en-

hance LLMs, while they aim to explore the knowl-

edge hidden within LLMs.


# Page 3

3 Rethinking with Retrieval

LLMs have been shown to generate incorrect sup-

porting facts from time to time, even when they ac-

curately capture the perspective needed to answer

a question. This phenomenon highlights intrinsic

issues in the way LLMs store and retrieve knowl-

edge, including (1) the presence of out-of-date,

incorrect, or missing relevant knowledge in the

pre-training corpus; (2) incorrect memorization of

relevant knowledge during pre-training; and (3)

incorrect retrieval of relevant knowledge during

the inference stage. To address these issues, we

propose the use of RR, which leverages external

knowledge through the retrieval of relevant infor-

mation based on decomposed reasoning steps.

Overview. Given a queryQ, we utilize chain-of-

thought prompting to generate a diverse set of rea-

soning paths R1, R2, ···RN , where each reason-

ing path Ri consists of an explanationEi followed

by a prediction Pi. After that, we retrieve relevant

knowledge K1, ···KM from a suitable knowledge

base KBto support the explanation in each reason-

ing path, and select the prediction ˆP that is most

faithful to this knowledge. To better illustrate our

proposal, we use “Did Aristotle use a laptop?” as

a running example in this work.

Chain-of-thought prompting. In contrast to

standard prompting, CoT prompting (Wei et al.,

2022) includes demonstrations of step-by-step rea-

soning examples in the prompt to produce a series

of short sentences that capture the reasoning pro-

cess. For instance, given the question “ Did Aris-

totle use a laptop?”, CoT prompting aims to gen-

erate the complete reasoning path “Aristotle died

in 322 BC. The ﬁrst laptop was invented in 1980.

Thus, Aristotle did not use a laptop. So the answer

is no.” rather than simply outputs “No.” Empirical

results show that CoT prompting signiﬁcantly im-

proves the performance of LLMs on many multi-

step reasoning tasks. Therefore, we adopt CoT

prompting to obtain both explanation E and pre-

diction P for the query Q.

Sampling diverse reasoning paths. Similar to

Wang et al. (2022), we sample a diverse set of rea-

soning paths R1, R2, ···RN rather than only con-

sidering the greedy path as in Wei et al. (2022).

For the question “Did Aristotle use a laptop?”, the

potential reasoning paths can be as follows:

(R1) Aristotle died in 2000. The ﬁrst laptop was

invented in 1980. Thus, Aristotle used a lap-

top. So the answer is yes.

(R2) Aristotle died in 322BC. The ﬁrst laptop was

invented in 2000. Thus, Aristotle did not use

a laptop. So the answer is no.

(R3) Aristotle died in 322BC. The ﬁrst laptop was

invented in 1980. Thus, Aristotle did not use

a laptop. So the answer is no.

Knowledge retrieval. Different knowledge

bases can be used to address different tasks. For

example, to address the question “ Did Aristotle

use a laptop? ”, we can use Wikipedia as the ex-

ternal knowledge base KB. Information retrieval

techniques can be applied to retrieve the relevant

knowledge K1, ···KM from Wikipedia based

on the decomposed reasoning steps. Ideally, we

would obtain the following two paragraphs from

Wikipedia for this question:

(K1) Aristotle (384–322 BC) was a Greek philoso-

pher and polymath during the Classical pe-

riod in Ancient Greece. ...

(K2) The Epson HX-20, the ﬁrst laptop computer,

was invented in 1980. ...

Faithful inference. The faithfulness of each rea-

soning path Ri can be estimated using a function

fKB(Ri), which is based on relevant knowledge

K1, ··· , KM retrieved from the knowledge base

KB. The ﬁnal prediction is obtained through the

application of the following inference procedure2:

ˆP = arg max

Pi∈{P1,···,PN }

N∑

i=1

1 (Pi = P)fKB(Ri), (1)

where Pi denotes the corresponding prediction in

the reasoning path Ri. This inference procedure

is designed to identify the most faithful prediction

ˆP to the knowledge base among all predictions in

the N reasoning paths. For instance, in the run-

ning example, given reasoning paths R1, R2, R3

and the retrieved knowledgeK1, K2, the above in-

ference procedure would output the prediction “So

the answer is no.”, as it is supported by both R2

and R3 and has a higher faithfulness score com-

pared to the prediction “So the answer is yes.”,

which is only supported by R1.

2Note that this is the basic version of faithful inference,

and further variations can be found in Section 5.3.


# Page 4

4 Experiments

In this section, we present the evaluation of our

proposed method, RR, on three complex reason-

ing tasks: commonsense reasoning, temporal rea-

soning, and tabular reasoning.

4.1 Baselines

We compare with the following baselines.

Zero-shot/few-shot prompting. In our experi-

ments, we consider GPT-3 with standard zero-

shot/few-shot prompting as baselines, following

the approach described in Brown et al. (2020), in

which zero or few in-context exemplars of input-

output pairs are provided in the prompt.

Chain-of-thought prompting. In addition to

the standard zero-shot/few-shot prompting, we

also consider GPT-3 with the CoT prompting pro-

posed in (Wei et al., 2022) as a baseline in our ex-

periments. This approach involves feeding LLMs

step-by-step reasoning examples instead of stan-

dard input-output examples.

Self-consistency. In addition, we also consider

self-consistency (Wang et al., 2022) as a baseline

in our experiments. This approach, proposed as an

alternative to the naive greedy decoding used in

CoT prompting (Wei et al., 2022), involves sam-

pling a diverse set of reasoning paths and select-

ing the most consistent answer by marginalizing

the sampled paths.

4.2 Commonsense Reasoning

Dataset description. For commonsense reason-

ing, we consider the StrategyQA dataset (Geva

et al., 2021), which includes questions that require

implicit reasoning strategies. For example, the

question “Did Aristotle use a laptop? ” requires

implicit decomposition into reasoning steps, while

the question “Was Aristotle alive when the laptop

was invented?” explicitly speciﬁes the reasoning

process. The StrategyQA dataset includes 2, 290

training examples, each consisting of a question

(Q), a yes/no answer (A), a decomposition (D),

evidence paragraphs (E), and supporting facts (F).

On average, each question requires about2.93 rea-

soning steps and 2.33 evidence paragraphs. In ad-

dition, a development set is constructed by ran-

domly sampling 10% of the training examples

(i.e., 229 examples). The answer distribution is

roughly balanced, with approximately 47% "yes"

questions in both the training and development

sets. Unless otherwise speciﬁed, the models are

evaluated on the development set3 for StrategyQA.

Implementation details. In this part, we uti-

lize Wikipedia as the external knowledge base

KB. For each sentence in the explanation of ev-

ery reasoning path, we ﬁrst apply BM25 (Robert-

son et al., 2009) to retrieve the top 10 most rele-

vant paragraphs from Wikipedia. In particular, we

use the re-implementation of the sparse retrieval

BM254 in Karpukhin et al. (2020) from Pyserini

(Lin et al., 2021). Subsequently, we use the pre-

trained MPNet model (Song et al., 2020) to se-

lect the most similar paragraph based on the cosine

similarity between the sentence embeddings of the

retrieved paragraph and the sentence. We then

employ a pre-trained natural language inference

(NLI) model (Nie et al., 2020) to obtain the en-

tailment and contradiction scores for the sentence,

treating the most similar paragraph as the premise.

The faithfulness of each reasoning path is then

calculated using fKB(·) based on the entailment

scores, contradiction scores, and MPNet similari-

ties of all sentences in the explanation of the rea-

soning path. The ﬁnal prediction for each ques-

tion is obtained through faithful inference (Equa-

tion 1). More details about fKB(·) can be found in

Appendix A.2.

4.3 Temporal Reasoning

Dataset description. In this experiment, we use

the TempQuestions dataset (Jia et al., 2018) to

investigate temporal reasoning. This dataset in-

cludes 1, 271 temporal questions that are divided

into four classes: explicit temporal, implicit tem-

poral, temporal answer, and ordinal constraints.

The questions are paired with their answers from

Freebase (Bollacker et al., 2008). To examine the

most challenging aspect of temporal reasoning, we

focus on the set of implicit temporal questions,

which contain implicit temporal expressions, in-

cluding free-text temporal expressions. For ex-

ample, the question “who was governor of oregon

when shanghai noon was released?” is an implicit

temporal question. To facilitate our analysis, we

only consider questions with a single answer, re-

sulting in a total of 175 examples. Of these ex-

3As the annotations for the test set are not publicly avail-

able, we use the development set for evaluation. This allows

us to perform a more comprehensive analysis.

4We also experimented with DPR and BM25+DPR, and

found that BM25 outperformed these methods in our experi-

ments. More details can be found in Appendix A.3.


# Page 5

Methods Commonsense Temporal Tabular

GPT-3

Zero-shot prompting 58.08 28.40 82.00

Few-shot prompting 63.32 29.59 83.08

Chain-of-thought prompting 65.94 33.14 83.33

Self-consistency 73.36 37.28 84.00

Rethinking with retrieval 77.73 39.05 84.83

Table 1: Performance of different methods using GPT-3 on three reasoning tasks.

amples, the ﬁrst 6 are used for prompting, and the

remaining 169 are used for evaluation.

Implementation details. In this part, we utilize

Wikidata (Vrande ˇci´c and Krötzsch, 2014) as the

external knowledge base KB, as it is the largest

publicly available knowledge graph, and the data

from Freebase has been migrated to Wikidata. To

incorporate this knowledge into our system, we

apply an entity linking system 5 to each sentence

in the explanation of each reasoning path to iden-

tify the corresponding Wikidata pages for all enti-

ties in the sentence. Next, we extract all temporal

relations from these relevant Wikidata pages and

use templates to convert these temporal relations

into sentences. This step generates a set of rele-

vant knowledge sentences for each sentence in the

explanation of each reasoning path. The ﬁnal pre-

diction is then obtained by applying the procedure

described in Section 4.2, in which the retrieved

paragraphs are replaced with the relevant knowl-

edge sentences from the current part.

4.4 Tabular Reasoning

Dataset description. We consider the IN-

FOTABS dataset (Gupta et al., 2020) for tabu-

lar reasoning, which consists of 23, 738 human-

written textual hypotheses based on premises in

the form of tables extracted from 2, 540 unique

Wikipedia info-boxes. We focus on the develop-

ment set, which includes 1, 800 hypotheses based

on 200 tables, and only consider entailed and con-

tradictory hypotheses as it is tricky to write CoT

demonstrations for neutral hypotheses. This re-

sults in a total of 1, 200 hypotheses based on 200

tables for evaluation, with an equal number of en-

tailed and contradictory hypotheses.

Implementation details. In this part, we utilize

WordNet (Miller, 1995) and ConceptNet (Speer

5We use the spacy entity linker:https://pypi.org/

project/spacy-entity-linker/.

et al., 2017) as external knowledge bases. To con-

vert tables into textual premises, we follow the

same technique as in Varun et al. (2022). For each

premise-hypothesis pair, we follow the procedure

outlined in Varun et al. (2022) to retrieve rele-

vant word relation triples that connect the premise

and hypothesis words, such as “married” RelatedTo←−−−−→

“spouse”. These triples are then converted into

sentences using some simple templates. The re-

sulting sentences, along with the textual premises

from the tables, serve as relevant knowledge for

each sentence in the explanation of each reasoning

path. To obtain the ﬁnal prediction, the procedure

described in Section 4.2 is applied, whereby the

retrieved paragraphs in Section 4.2 are replaced

with the relevant knowledge from the current part.

4.5 Evaluation

Experimental settings. In all experiments, we

utilize GPT-3 text-davinci-002 unless oth-

erwise stated. The maximum number of tokens for

generation during completion is set to 256. For

zero-shot, few-shot, and chain-of-thought prompt-

ing, the temperature is ﬁxed at 0. For self-

consistency and rethinking with retrieval, we ran-

domly sample 10 outputs6 with temperature 0.7.

Detailed prompts can be found in Appendix A.1.

We evaluate the performance of different methods

on commonsense and tabular reasoning using ac-

curacy, and on temporal reasoning using the exact

match metric as deﬁned in Rajpurkar et al. (2016).

Results. As shown in Table 1, our proposed

method, rethinking with retrieval, consistently

outperforms all baselines on all three reasoning

tasks without requiring additional training or ﬁne-

tuning. The results highlight the effectiveness of

our approach in leveraging external knowledge to

improve the performance of LLMs.

6For commonsense reasoning, we sample9 outputs, as we

have found that odd numbers of outputs tend to yield better

voting performance for self-consistency on StrategyQA.


# Page 6

Wrong supporting facts Wrong inference

Question Was Lil Jon’s top ranked Billboard song a

collaboration with a member of The Lox?

Would the top of Mount Fuji stick out of

the Sea of Japan?

Answer No Yes

Facts

Lil Jon’s highest ranked billboard song

was Yeah; Yeah was a collaboration be-

tween Lil Jon, Usher, and Ludacris; The

Lox is a rap trio consisting of: Styles P,

Sheek Louch, and Jadakiss.

The average depth of the Sea of Japan is

5,748 feet (1,752 metres) and its maxi-

mum depth is 12,276 feet (3,742 metres);

Mount Fuji is 3,776.24 metres (12,389.2

ft) tall.

GPT-3

Lil Jon’s top ranked Billboard song was

"Get Low" with the East Side Boyz. The

Lox is not a member of the East Side

Boyz. Thus, Lil Jon’s top ranked Bill-

board song was not a collaboration with

a member of The Lox. So the answer is

no.

Mount Fuji is 3,776 meters tall. The

Sea of Japan is about 3,741 meters deep.

Thus, the top of Mount Fuji would not

stick out of the Sea of Japan. So the an-

swer is no.

Table 2: Examples of incorrect outputs from GPT-3 with CoT prompting.

5 Analysis

In this section, we perform a thorough analysis to

gain a deeper understanding of RR.

5.1 Limitations of LLMs in Reasoning

In this subsection, we present an analysis of GPT-

3 with CoT prompting on the StrategyQA dataset.

Upon closer examination of the outputs of GPT-

3, we observed that it can provide reasonable ex-

planations and correct predictions for a number

of questions. For example, when given the ques-

tion “Will the Albany in Georgia reach a hundred

thousand occupants before the one in New York?”,

GPT-3 produced the following output:

The Albany in New York has a pop-

ulation of about 98,000. The Albany

in Georgia has a population of about

77,000. Thus, the Albany in New York

is more populous than the Albany in

Georgia. So the answer is no.

The above output consists of three components:

(1) supporting facts (in cyan) that are based on a

particular perspective, (2) chaining arguments (in

orange), and (3) a prediction (in green). Com-

ponents (1) and (2) contribute to the explanation.

Overall, the output exhibits a high level of quality.

However, we also observed that GPT-3 may occa-

sionally produce incorrect supporting facts for its

explanations or make incorrect inferences for its

Retrieval Commonsense Tabular

Query-based 73.36 36.69

Decomposition-based 77.73 39.05

Table 3: Comparison of query-based and

decomposition-based retrieval on commonsense

and tabular reasoning.

predictions, despite generally being able to iden-

tify suitable perspectives.

Wrong supporting facts. As shown in Table 2,

GPT-3 provides the incorrect supporting fact for

Lil Jon’s top-ranked Billboard song, stating that

it was “Get Low” instead of the correct answer,

“Yeah”. However, it does have the correct per-

spective on how to answer the question, “ Was 
..._This content has been truncated to stay below 50000 characters_...
iﬁcantly outperform the voting baseline, self-

consistency (i.e., 73.36%), as shown in Table 1.

It is noteworthy that UniﬁedQA performs

poorly on StrategyQA, achieving an accuracy of

only 58.95%. However, when provided with

gold supporting facts in StrategyQA, UniﬁedQA

demonstrates excellent performance with an accu-

racy of 90.83%. This suggests that UniﬁedQA is

suitable for last-step inference, but not effective

for answering questions in StrategyQA.

5.4 Impact of the Size of LMs

In this subsection, we examine the effect of the

size of LMs on the performance of our proposed

method, speciﬁcally in the context of the fact gen-

eration variant. We compare the performance of

our method using various sizes of OPT models

(Zhang et al., 2022) in addition to GPT-3 (175B)

using the same experimental setup as in Sec-

tion 5.3. As shown in Figure 2, our proposed

method (Variant II) consistently outperforms CoT

prompting in terms of both prediction accuracy

and the faithfulness of explanations, even when

using smaller LMs.

6 Conclusion

In conclusion, the proposed approach is a promis-

ing solution for utilizing external knowledge to as-

sist LLMs. Unlike traditional methods, RR does

not require additional training or ﬁne-tuning, mak-

ing it a lightweight and feasible option for LLMs.

Through extensive experiments on three reason-

ing tasks using GPT-3, we have shown that RR is

able to produce more faithful explanations and im-

prove the performance of LLMs. In the future, we

plan to investigate various variations of RR to en-

hance its effectiveness and efﬁciency in augment-

ing LLMs with external knowledge.


# Page 9

References

Kurt Bollacker, Colin Evans, Praveen Paritosh,

Tim Sturge, and Jamie Taylor. 2008. Freebase:

a collaboratively created graph database for

structuring human knowledge. In Proceedings

of the 2008 ACM SIGMOD international con-

ference on Management of data , pages 1247–

1250.

Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-

mann, Trevor Cai, Eliza Rutherford, Katie Mil-

lican, George van den Driessche, Jean-Baptiste

Lespiau, Bogdan Damoc, Aidan Clark, et al.

2021. Improving language models by retriev-

ing from trillions of tokens. arXiv preprint

arXiv:2112.04426.

Tom Brown, Benjamin Mann, Nick Ryder,

Melanie Subbiah, Jared D Kaplan, Prafulla

Dhariwal, Arvind Neelakantan, Pranav Shyam,

Girish Sastry, Amanda Askell, et al. 2020.

Language models are few-shot learners. Ad-

vances in neural information processing sys-

tems, 33:1877–1901.

Aakanksha Chowdhery, Sharan Narang, Jacob De-

vlin, Maarten Bosma, Gaurav Mishra, Adam

Roberts, Paul Barham, Hyung Won Chung,

Charles Sutton, Sebastian Gehrmann, et al.

2022. Palm: Scaling language modeling with

pathways. arXiv preprint arXiv:2204.02311.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-

ian, Jacob Hilton, Reiichiro Nakano, Christo-

pher Hesse, and John Schulman. 2021. Training

veriﬁers to solve math word problems. arXiv

preprint arXiv:2110.14168.

Ido Dagan, Oren Glickman, and Bernardo

Magnini. 2005. The pascal recognising tex-

tual entailment challenge. In Machine learning

challenges workshop, pages 177–190. Springer.

Daniel Deutsch, Tania Bedrax-Weiss, and Dan

Roth. 2021. Towards question-answering as

an automatic metric for evaluating the content

quality of a summary. Transactions of the Asso-

ciation for Computational Linguistics , 9:774–

789.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and

Kristina Toutanova. 2019. BERT: Pre-training

of deep bidirectional transformers for language

understanding. In Proceedings of the 2019

Conference of the North American Chapter

of the Association for Computational Linguis-

tics: Human Language Technologies, Volume 1

(Long and Short Papers), pages 4171–4186.

Alexander R Fabbri, Chien-Sheng Wu, Wenhao

Liu, and Caiming Xiong. 2021. Qafacte-

val: Improved qa-based factual consistency

evaluation for summarization. arXiv preprint

arXiv:2112.08542.

Mor Geva, Daniel Khashabi, Elad Segal, Tushar

Khot, Dan Roth, and Jonathan Berant. 2021.

Did aristotle use a laptop? a question answer-

ing benchmark with implicit reasoning strate-

gies. Transactions of the Association for Com-

putational Linguistics, 9:346–361.

Liangke Gui, Borui Wang, Qiuyuan Huang, Alex

Hauptmann, Yonatan Bisk, and Jianfeng Gao.

2021. Kat: A knowledge augmented trans-

former for vision-and-language. arXiv preprint

arXiv:2112.08614.

Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and

Vivek Srikumar. 2020. Infotabs: Inference on

tables as semi-structured data. In Proceed-

ings of the 58th Annual Meeting of the As-

sociation for Computational Linguistics , pages

2309–2324.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong

Pasupat, and Mingwei Chang. 2020. Retrieval

augmented language model pre-training. In In-

ternational Conference on Machine Learning ,

pages 3929–3938. PMLR.

Or Honovich, Roee Aharoni, Jonathan Herzig,

Hagai Taitelbaum, Doron Kukliansy, Vered Co-

hen, Thomas Scialom, Idan Szpektor, Avinatan

Hassidim, and Yossi Matias. 2022. True: Re-

evaluating factual consistency evaluation. In

Proceedings of the Second DialDoc Workshop

on Document-grounded Dialogue and Conver-

sational Question Answering, pages 161–175.

Or Honovich, Leshem Choshen, Roee Aharoni,

Ella Neeman, Idan Szpektor, and Omri Abend.

2021. Q2:: Evaluating factual consistency

in knowledge-grounded dialogues via question

generation and question answering. In Pro-

ceedings of the 2021 Conference on Empiri-

cal Methods in Natural Language Processing ,

pages 7856–7870.


# Page 10

Zhen Jia, Abdalghani Abujabal, Rishiraj

Saha Roy, Jannik Strötgen, and Gerhard

Weikum. 2018. Tempquestions: A benchmark

for temporal question answering. In Compan-

ion Proceedings of the The Web Conference

2018, pages 1057–1062.

Mandar Joshi, Kenton Lee, Yi Luan, and Kristina

Toutanova. 2020. Contextualized representa-

tions using textual encyclopedic knowledge.

arXiv preprint arXiv:2004.12006.

Vladimir Karpukhin, Barlas Oguz, Sewon Min,

Patrick Lewis, Ledell Wu, Sergey Edunov,

Danqi Chen, and Wen-tau Yih. 2020. Dense

passage retrieval for open-domain question an-

swering. In Proceedings of the 2020 Confer-

ence on Empirical Methods in Natural Lan-

guage Processing (EMNLP), pages 6769–6781.

Urvashi Khandelwal, Omer Levy, Dan Juraf-

sky, Luke Zettlemoyer, and Mike Lewis. 2020.

Generalization through memorization: Nearest

neighbor language models. In International

Conference on Learning Representations.

Daniel Khashabi, Yeganeh Kordi, and Hannaneh

Hajishirzi. 2022. Uniﬁedqa-v2: Stronger gen-

eralization via broader cross-format training.

arXiv preprint arXiv:2202.12359.

Daniel Khashabi, Sewon Min, Tushar Khot,

Ashish Sabharwal, Oyvind Tafjord, Peter Clark,

and Hannaneh Hajishirzi. 2020. Uniﬁedqa:

Crossing format boundaries with a single qa

system. In Findings of the Association for Com-

putational Linguistics: EMNLP 2020 , pages

1896–1907.

Takeshi Kojima, Shixiang Shane Gu, Machel

Reid, Yutaka Matsuo, and Yusuke Iwasawa.

2022. Large language models are zero-shot rea-

soners. arXiv preprint arXiv:2205.11916.

Mojtaba Komeili, Kurt Shuster, and Jason Weston.

2022. Internet-augmented dialogue generation.

In Proceedings of the 60th Annual Meeting of

the Association for Computational Linguistics

(Volume 1: Long Papers), pages 8460–8478.

Patrick Lewis, Ethan Perez, Aleksandra Piktus,

Fabio Petroni, Vladimir Karpukhin, Naman

Goyal, Heinrich Küttler, Mike Lewis, Wen-tau

Yih, Tim Rocktäschel, et al. 2020. Retrieval-

augmented generation for knowledge-intensive

nlp tasks. Advances in Neural Information Pro-

cessing Systems, 33:9459–9474.

Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin,

Jheng-Hong Yang, Ronak Pradeep, and Rodrigo

Nogueira. 2021. Pyserini: A Python toolkit for

reproducible information retrieval research with

sparse and dense representations. In Proceed-

ings of the 44th Annual International ACM SI-

GIR Conference on Research and Development

in Information Retrieval (SIGIR 2021) , pages

2356–2362.

Jiacheng Liu, Alisa Liu, Ximing Lu, Sean

Welleck, Peter West, Ronan Le Bras, Yejin

Choi, and Hannaneh Hajishirzi. 2022. Gen-

erated knowledge prompting for commonsense

reasoning. In Proceedings of the 60th Annual

Meeting of the Association for Computational

Linguistics (Volume 1: Long Papers) , pages

3154–3169.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei

Du, Mandar Joshi, Danqi Chen, Omer Levy,

Mike Lewis, Luke Zettlemoyer, and Veselin

Stoyanov. 2019. Roberta: A robustly opti-

mized bert pretraining approach. arXiv preprint

arXiv:1907.11692.

George A Miller. 1995. Wordnet: a lexical

database for english. Communications of the

ACM, 38(11):39–41.

Reiichiro Nakano, Jacob Hilton, Suchir Bal-

aji, Jeff Wu, Long Ouyang, Christina Kim,

Christopher Hesse, Shantanu Jain, Vineet

Kosaraju, William Saunders, et al. 2021. We-

bgpt: Browser-assisted question-answering

with human feedback. arXiv preprint

arXiv:2112.09332.

J Neeraja, Vivek Gupta, and Vivek Srikumar.

2021. Incorporating external knowledge to en-

hance tabular reasoning. In Proceedings of the

2021 Conference of the North American Chap-

ter of the Association for Computational Lin-

guistics: Human Language Technologies, pages

2799–2809.

Yixin Nie, Adina Williams, Emily Dinan, Mohit

Bansal, Jason Weston, and Douwe Kiela. 2020.

Adversarial nli: A new benchmark for natu-

ral language understanding. In Proceedings of

the 58th Annual Meeting of the Association for

Computational Linguistics, pages 4885–4901.


# Page 11

Maxwell Nye, Anders Johan Andreassen, Guy

Gur-Ari, Henryk Michalewski, Jacob Austin,

David Bieber, David Dohan, Aitor Lewkowycz,

Maarten Bosma, David Luan, et al. 2022. Show

your work: Scratchpads for intermediate com-

putation with language models. In Deep Learn-

ing for Code Workshop.

Maxwell Nye, Michael Tessler, Josh Tenenbaum,

and Brenden M Lake. 2021. Improving coher-

ence and consistency in neural sequence mod-

els with dual-system, neuro-symbolic reason-

ing. Advances in Neural Information Process-

ing Systems, 34:25192–25204.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,

Carroll L Wainwright, Pamela Mishkin, Chong

Zhang, Sandhini Agarwal, Katarina Slama,

Alex Ray, et al. 2022. Training language mod-

els to follow instructions with human feedback.

arXiv preprint arXiv:2203.02155.

Colin Raffel, Noam Shazeer, Adam Roberts,

Katherine Lee, Sharan Narang, Michael

Matena, Yanqi Zhou, Wei Li, and Peter J Liu.

2020. Exploring the limits of transfer learning

with a uniﬁed text-to-text transformer. Journal

of Machine Learning Research, 21:1–67.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopy-

rev, and Percy Liang. 2016. Squad: 100,000+

questions for machine comprehension of text.

In Proceedings of the 2016 Conference on Em-

pirical Methods in Natural Language Process-

ing, pages 2383–2392.

Stephen Robertson, Hugo Zaragoza, et al. 2009.

The probabilistic relevance framework: Bm25

and beyond. Foundations and Trends® in In-

formation Retrieval, 3(4):333–389.

Kurt Shuster, Mojtaba Komeili, Leonard Adolphs,

Stephen Roller, Arthur Szlam, and Jason We-

ston. 2022. Language models that seek for

knowledge: Modular search &amp; generation

for dialogue and prompt completion. arXiv

preprint arXiv:2203.13224.

Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu,

and Tie-Yan Liu. 2020. Mpnet: Masked and

permuted pre-training for language understand-

ing. Advances in Neural Information Process-

ing Systems, 33:16857–16867.

Robyn Speer, Joshua Chin, and Catherine Havasi.

2017. Conceptnet 5.5: An open multilingual

graph of general knowledge. In Thirty-ﬁrst

AAAI conference on artiﬁcial intelligence.

Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav

Goldberg, and Jonathan Berant. 2020. Leap-

of-thought: Teaching pre-trained models to

systematically reason over implicit knowledge.

Advances in Neural Information Processing

Systems, 33:20227–20237.

Romal Thoppilan, Daniel De Freitas, Jamie Hall,

Noam Shazeer, Apoorv Kulshreshtha, Heng-

Tze Cheng, Alicia Jin, Taylor Bos, Leslie

Baker, Yu Du, et al. 2022. Lamda: Language

models for dialog applications. arXiv preprint

arXiv:2201.08239.

Yerram Varun, Aayush Sharma, and Vivek Gupta.

2022. Trans-kblstm: An external knowledge

enhanced transformer bilstm model for tabular

reasoning. In Proceedings of Deep Learning In-

side Out (DeeLIO 2022): The 3rd Workshop on

Knowledge Extraction and Integration for Deep

Learning Architectures, pages 62–78.

Ashish Vaswani, Noam Shazeer, Niki Parmar,

Jakob Uszkoreit, Llion Jones, Aidan N Gomez,

Łukasz Kaiser, and Illia Polosukhin. 2017. At-

tention is all you need. Advances in neural in-

formation processing systems, 30.

Denny Vrande ˇci´c and Markus Krötzsch. 2014.

Wikidata: a free collaborative knowledgebase.

Communications of the ACM, 57(10):78–85.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc

Le, Ed Chi, and Denny Zhou. 2022. Self-

consistency improves chain of thought rea-

soning in language models. arXiv preprint

arXiv:2203.11171.

Jason Wei, Xuezhi Wang, Dale Schuurmans,

Maarten Bosma, Ed Chi, Quoc Le, and Denny

Zhou. 2022. Chain of thought prompting elic-

its reasoning in large language models. arXiv

preprint arXiv:2201.11903.

Thomas Wolf, Lysandre Debut, Victor Sanh,

Julien Chaumond, Clement Delangue, Anthony

Moi, Pierric Cistac, Tim Rault, Rémi Louf,

Morgan Funtowicz, et al. 2020. Transformers:

State-of-the-art natural language processing. In


# Page 12

Proceedings of the 2020 conference on empir-

ical methods in natural language processing:

system demonstrations, pages 38–45.

Xi Ye and Greg Durrett. 2022. The unreliability

of explanations in few-shot in-context learning.

arXiv preprint arXiv:2205.03401.

Eric Zelikman, Yuhuai Wu, and Noah D Good-

man. 2022. Star: Bootstrapping reasoning with

reasoning. arXiv preprint arXiv:2203.14465.

Susan Zhang, Stephen Roller, Naman Goyal,

Mikel Artetxe, Moya Chen, Shuohui Chen,

Christopher Dewan, Mona Diab, Xian Li,

Xi Victoria Lin, et al. 2022. Opt: Open pre-

trained transformer language models. arXiv

preprint arXiv:2205.01068.

Denny Zhou, Nathanael Schärli, Le Hou, Ja-

son Wei, Nathan Scales, Xuezhi Wang, Dale

Schuurmans, Olivier Bousquet, Quoc Le, and

Ed Chi. 2022. Least-to-most prompting enables

complex reasoning in large language models.

arXiv preprint arXiv:2205.10625.


# Page 13

A Appendix

In this section, we provide additional details on

our experimental setup. Further information can

be found in our code.

A.1 Detailed Prompts

We adopt the same CoT prompt for commonsense

reasoning (i.e., StrategyQA) as those presented in

Wei et al. (2022). The CoT prompt for tempo-

ral reasoning is provided in Table 6. For tabular

reasoning, we adopt the method of Brown et al.

(2020) for converting NLI into QA for RTE (Da-

gan et al., 2005), and randomly sample6 examples

from the training data to construct the prompt, as

shown in Table 8. The few-shot prompt utilizes

the same exemplars as the CoT prompt and does

not involve CoT reasoning processes.

A.2 Description of Faithfulness Functions

For a sentence s, we denote its MPNet similarity,

entailment score, and contradiction score asM(s),

E(s), and C(s), respectively. In our experiments,

the corresponding thresholds for these scores are

Tm = 0.5, Te = 0.6, and Tc = 0.99. Given the

entailment scores, contradiction scores, and MP-

Net similarities of all supporting facts (denoted as

S) in the explanation of a reasoning pathR, differ-

ent faithfulness functionsfKB(·) can be adopted in

different settings as follows:

(1) fKB(R) =∑

s∈S[M(s)×(M(s) &gt;= Tm)+

E(s) ×(M(s) &lt; Tm) −C(s)]

(2) fKB(R) =∑

s∈S[M(s) +E(s)]

(3) fKB(R) = ∑

s∈S[E(s) ×(E(s) &gt;= Te) −

C(s) ×(C(s) &gt;= Tc)]

In Section 4, we employ function (1) for com-

monsense and tabular reasoning. For temporal rea-

soning, we use function (2) as the distinct nature of

sentences converted from temporal relations leads

to unreliable contradiction scores. In Sections 5.3-

5.4, we use function (3) for commonsense reason-

ing with evidence paragraphs, as the high quality

of the relevant knowledge negates the need for the

complementary use of the MPNet similarity to im-

prove the entailment score.

A.3 Comparison of Retrieval Systems

For commonsense reasoning, we utilized different

retrieval systems in Karpukhin et al. (2020) to re-

trieve relevant paragraphs from Wikipedia. The

performance of BM25, DPR, and BM25+DPR

were 77.73%, 58.52%, and 77.29%, respectively,

indicating that BM25 is the best choice in our case.

A.4 Implementation Details for the Two

Variants of RR

Fact selection implementation details. In this

work, we utilize the information present in the top-

ranked output produced by our basic approach as

a guide. To this end, we apply a greedy clustering

algorithm to group the sentences from all outputs

into distinct topic categories based on the cosine

similarity of their MPNet sentence embeddings.

For each fact in the top-ranked output of our ba-

sic approach, we identify the fact with the highest

faithfulness within the same topic group and re-

place it in the output. The faithfulness of a fact is

calculated using the fKBfunction by replacing the

supporting facts with a single fact.

Fact generation implementation details. In

this part, we generate questions for the named en-

tities present in each fact of the top-ranked output

produced by our basic approach, and retrieve the

corresponding answers from the evidence para-

graphs using UniﬁedQA. We employ the ques-

tion generation model described in Deutsch et al.

(2021), which has been shown to be more ex-

tractive compared to other models as demon-

strated in Fabbri et al. (2021). We adopt the

question ﬁltering approach proposed in Honovich

et al. (2021) using an off-the-shelf extractive

QA model (ktrapeznikov/albert-xlarge-v2-squad-

v2 from Hugging Face (Wolf et al., 2020)). We

then use an off-the-shelf model (MarkS/bart-base-

qa2d from Hugging Face) to convert the generated

QA pairs into declarative sentences. We apply

simple rules based on the entailment and contra-

diction scores of the selected facts from the fact se-

lection variant and the generated declarative sen-

tences to obtain the ﬁnal generated facts.

A.5 Comparison of Different Inference

Methods with Supporting Facts

In our experiments, we utilize UniﬁedQA for the

ﬁnal step of inference in both variants. However,

it is worth noting that GPT-3 could also be used

for this purpose. As shown in Table 7, we observe

that UniﬁedQA performs better at inference with

generated facts, while GPT-3 with CoT prompt-

ing performs better with empty or gold facts. This

suggests that UniﬁedQA is more robust to noisy


# Page 14

Q: who was governor of minnesota when maathaad maathaadu mallige was released?

A: Maathaad Maathaadu Mallige was released on 24 August 2007. Tim Pawlenty served as the 39th gov-

ernor of Minnesota from 2003 to 2011. Thus, Tim Pawlenty was governor of minnesota when maathaad

maathaadu mallige was released. So the answer is Tim Pawlenty.

Q: who was us president during the costa rican civil war?

A: The Costa Rican civil war was a civil war in Costa Rica from 12 March to 24 April 1948. Harry S.

Truman was the 33rd president of the United States, serving from 1945 to 1953. Thus, Harry S. Truman

was us president during the costa rican civil war. So the answer is Harry S. Truman.

Q: who was governor of oregon when the collector was released?

A: The Collector premiered at the Cannes Film Festival on May 20, 1965. Mark Hatﬁeld served as the

29th governor of Oregon from 1959 to 1967. Thus, Mark Hatﬁeld was governor of oregon when the

collector was released. So the answer is Mark Hatﬁeld.

Q: who was governor of oregon when shanghai noon was released?

A: Shanghai Noon was released on May 26, 2000. John Kitzhaber served as the 35th governor of Oregon

from 1995 to 2003. Thus, John Kitzhaber was governor of oregon when shanghai noon was released. So

the answer is John Kitzhaber.

Q: who was us president when john andrew shulze was a teenager?

A: John Andrew Shulze was born on July 19, 1775. A teenager is someone who is between 13 and 19

years old. George Washington served as the ﬁrst president of the United States from 1789 to 1797. Thus,

George Washington was us president when john andrew shulze was a teenager.So the answer is George

Washington.

Q: who was us president during the seventh coalition?

A: The War of the Seventh Coalition was from 20 March to 8 July 1815. James Madison served as the

fourth president of the United States from 1809 to 1817. Thus, James Madison was us president during

the seventh coalition. So the answer is James Madison.

Table 6: The CoT prompt for temporal reasoning.

Methods Accuracy (%)

Empty facts

GPT-3 (zero-shot) 58.08

GPT-3 (CoT) 65.94

UniﬁedQA 58.95

Gold facts

GPT-3 (zero-shot) 81.66

GPT-3 (CoT) 91.70

UniﬁedQA 90.83

Generated facts

GPT-3 (zero-shot) 69.87

GPT-3 (CoT) 76.42

UniﬁedQA 78.60

Table 7: Comparison of different inference methods on

empty, gold, and generated facts.

inputs compared to GPT-3. Additionally, both

UniﬁedQA and GPT-3 with CoT prompting signif-

icantly outperform GPT-3 with zero-shot prompt-

ing, indicating that the CoT prompting is also ben-

eﬁcial for the ﬁnal step of inference.


# Page 15

Charles Sumner Tainter was Born on April 25, 1854 ( 1854-04-25 ) Watertown, Massachusetts, U.S..

Charles Sumner Tainter was Died on April 20, 1940 ( 1940-04-21 ) (aged 85) San Diego, California,

U.S.. The Nationality of Charles Sumner Tainter are American. The Known for of Charles Sumner

Tainter are Photophone, phonograph Father Of The Speaking Machine.

Question: Charles Sumner Tainter never left the state of Massachusetts. True or False?

Answer: Charles Sumner Tainter was died in San Diego, California, U.S.. California is a state. Thus,

Charles Sumner Tainter has left the state of Massachusetts. So the answer is false.

The Region of Curitiba are South. The Elevation of Curitiba are 934.6 m (3,066.3 ft). The Density of

Curitiba are 4,062/km 2 (10,523/sq mi). The Metro density of Curitiba are 210.9/km 2 (546.2/sq mi).

Question: Curitiba is above sea level. True or False?

Answer: The elevation of Curitiba are 934.6 m (3,066.3 ft). Elevation is a hypernym of level. Thus,

Curitiba is above sea level. So the answer is true.

Charles (Prince of Wales) was Born on 14 November 1948 ( 1948-11-14 ) (age 70) Buckingham Palace,

London, England. The Spouse of Charles (Prince of Wales) are Lady Diana Spencer ( m. 1981 ; div.

1996 ) , and Camilla Parker Bowles ( m. 2005 ). The Issue of Charles (Prince of Wales) are Prince

William, Duke of Cambridge , and Prince Harry, Duke of Sussex.

Question: Charles was born in 1948 and has been married twice. True or False?

Answer: Charles (Prince of Wales) was Born on 14 November 1948. The Spouse of Charles (Prince of

Wales) are Lady Diana Spencer ( m. 1981 ; div. 1996 ) , and Camilla Parker Bowles ( m. 2005 ). Married

is related to spouse. Thus, Charles was born in 1948 and has been married twice. So the answer is true.

The Born of Idris Elba are 6 September 1972 (age 46) Hackney, London, England. The Residence of

Idris Elba are London. The Other names of Idris Elba are DJ Big Driis, Big Driis the Londoner, Big

Driis, and 7 Dub. The Occupation of Idris Elba are Actor, producer, director, musician, and DJ.

Question: Idris Elba is an English entertainer. True or False?

Answer: The residence of Idris Elba is London. English is related to London. The occupation of Idris

Elba are actor, producer, director, musician, and DJ. Actor is a hyponym of entertainer. Musician is a

hyponym of entertainer. DJ is an entertainer. Thus, Idris Elba is an English entertainer. So the answer

is true.

The Breed of Jean, the Vitagraph Dog are Scotch Collie. The Sex of Jean, the Vitagraph Dog are Female.

The Born of Jean, the Vitagraph Dog are 1902 Eastport, Maine. The Years active of Jean, the Vitagraph

Dog are 1909 - 1916.

Question: Jean, the Vitagraph Dog was a Golden Retriever which perform in circus. True or False?

Answer: The Breed of Jean, the Vitagraph Dog are Scotch Collie. Collie is a hyponym of dog. Retriever

is a hyponym of dog. Thus, Jean, the Vitagraph Dog was not a Golden Retriever which perform in circus.

So the answer is false.

The Studio of Hydrograd are Sphere Studios, North Hollywood, Los Angeles. The Genre of Hydrograd

are Hard rock. The Label of Hydrograd are Roadrunner. The Producer of Hydrograd are Jay Ruston.

Question: Hydrograd is in the rap genre. True or False?

Answer: The Genre of Hydrograd are Hard rock. Rap is distinct from rock. Thus, Hydrograd is not in

the rap genre. So the answer is false.

Table 8: The CoT prompt for tabular reasoning.



Out: None
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">[Step 2: Duration 6.48 seconds| Input tokens: 5,777 | Output tokens: 251]</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #d4b702; text-decoration-color: #d4b702">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style="font-weight: bold">Step </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span><span style="color: #d4b702; text-decoration-color: #d4b702"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"> ─ <span style="font-weight: bold">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── 
  <span style="color: #ff4689; text-decoration-color: #ff4689; background-color: #272822">import</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822"> json</span><span style="background-color: #272822">                                                                                                    </span>  
  <span style="background-color: #272822">                                                                                                               </span>  
  <span style="color: #959077; text-decoration-color: #959077; background-color: #272822"># Define the slides</span><span style="background-color: #272822">                                                                                            </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">slides </span><span style="color: #ff4689; text-decoration-color: #ff4689; background-color: #272822">=</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822"> [</span><span style="background-color: #272822">                                                                                                     </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    {</span><span style="background-color: #272822">                                                                                                          </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"title"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Rethinking with Retrieval: Faithful Large Language Model Inference"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                         </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"content"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: [</span><span style="background-color: #272822">                                                                                           </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Authors: Hangfeng He, Hongming Zhang, Dan Roth"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                  </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Link: https://arxiv.org/abs/2301.00303v1"</span><span style="background-color: #272822">                                                         </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        ]</span><span style="background-color: #272822">                                                                                                      </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    },</span><span style="background-color: #272822">                                                                                                         </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    {</span><span style="background-color: #272822">                                                                                                          </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"title"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Introduction"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                               </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"content"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: [</span><span style="background-color: #272822">                                                                                           </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Large language models (LLMs) have shown exceptional performance across various tasks"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">            </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"However, the knowledge stored in LLMs may inevitably be incomplete, out-of-date, or incorrect"</span><span style="background-color: #272822">    </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        ]</span><span style="background-color: #272822">                                                                                                      </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    },</span><span style="background-color: #272822">                                                                                                         </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    {</span><span style="background-color: #272822">                                                                                                          </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"title"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Related Work"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                               </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"content"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: [</span><span style="background-color: #272822">                                                                                           </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Enhancing LMs through retrieval"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                 </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Incorporating external knowledge into LMs"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                       </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Uncovering latent knowledge in LLMs"</span><span style="background-color: #272822">                                                              </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        ]</span><span style="background-color: #272822">                                                                                                      </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    },</span><span style="background-color: #272822">                                                                                                         </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    {</span><span style="background-color: #272822">                                                                                                          </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"title"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Rethinking with Retrieval"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                  </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"content"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: [</span><span style="background-color: #272822">                                                                                           </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Overview"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                                        </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Chain-of-thought prompting"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                      </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Sampling diverse reasoning paths"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Knowledge retrieval"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                             </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Faithful inference"</span><span style="background-color: #272822">                                                                               </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        ]</span><span style="background-color: #272822">                                                                                                      </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    },</span><span style="background-color: #272822">                                                                                                         </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    {</span><span style="background-color: #272822">                                                                                                          </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"title"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Experiments"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                                </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"content"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: [</span><span style="background-color: #272822">                                                                                           </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Commonsense reasoning"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                           </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Temporal reasoning"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                              </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Tabular reasoning"</span><span style="background-color: #272822">                                                                                </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        ]</span><span style="background-color: #272822">                                                                                                      </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    },</span><span style="background-color: #272822">                                                                                                         </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    {</span><span style="background-color: #272822">                                                                                                          </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"title"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Analysis"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                                   </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"content"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: [</span><span style="background-color: #272822">                                                                                           </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Limitations of LLMs in reasoning"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Impact of the size of LMs"</span><span style="background-color: #272822">                                                                        </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        ]</span><span style="background-color: #272822">                                                                                                      </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    },</span><span style="background-color: #272822">                                                                                                         </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    {</span><span style="background-color: #272822">                                                                                                          </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"title"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Conclusion"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">,</span><span style="background-color: #272822">                                                                                 </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"content"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">: [</span><span style="background-color: #272822">                                                                                           </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">            </span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"The proposed approach is a promising solution for utilizing external knowledge to assist LLMs"</span><span style="background-color: #272822">    </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">        ]</span><span style="background-color: #272822">                                                                                                      </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">    }</span><span style="background-color: #272822">                                                                                                          </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">]</span><span style="background-color: #272822">                                                                                                              </span>  
  <span style="background-color: #272822">                                                                                                               </span>  
  <span style="color: #959077; text-decoration-color: #959077; background-color: #272822"># Convert the slides to a JSON string</span><span style="background-color: #272822">                                                                          </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">slides_json </span><span style="color: #ff4689; text-decoration-color: #ff4689; background-color: #272822">=</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822"> json</span><span style="color: #ff4689; text-decoration-color: #ff4689; background-color: #272822">.</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">dumps(slides)</span><span style="background-color: #272822">                                                                               </span>  
  <span style="background-color: #272822">                                                                                                               </span>  
  <span style="color: #959077; text-decoration-color: #959077; background-color: #272822"># Create the PowerPoint slide deck</span><span style="background-color: #272822">                                                                             </span>  
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">make_powerpoint_slides(slides</span><span style="color: #ff4689; text-decoration-color: #ff4689; background-color: #272822">=</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">slides_json, filename</span><span style="color: #ff4689; text-decoration-color: #ff4689; background-color: #272822">=</span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"Rethinking_with_Retrieval.pptx"</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">)</span><span style="background-color: #272822">                          </span>  
 ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── 
</pre>
</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PPTX
</pre></div>
</div>
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Out: The Powerpoint Presentation was saved as [Rethinking_with_Retrieval.pptx](Rethinking_with_Retrieval.pptx)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">[Step 3: Duration 24.52 seconds| Input tokens: 22,399 | Output tokens: 722]</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #d4b702; text-decoration-color: #d4b702">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style="font-weight: bold">Step </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="color: #d4b702; text-decoration-color: #d4b702"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"> ─ <span style="font-weight: bold">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── 
  <span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">final_answer(</span><span style="color: #e6db74; text-decoration-color: #e6db74; background-color: #272822">"The PowerPoint slide deck is ready."</span><span style="color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822">)</span><span style="background-color: #272822">                                                            </span>  
 ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── 
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #d4b702; text-decoration-color: #d4b702; font-weight: bold">Out - Final answer: The PowerPoint slide deck is ready.</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">[Step 4: Duration 5.07 seconds| Input tokens: 40,077 | Output tokens: 767]</span>
</pre>
</div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;The PowerPoint slide deck is ready.&#39;
</pre></div>
</div>
</div>
</div>
<section id="exercise">
<h2>Exercise<a class="headerlink" href="#exercise" title="Permalink to this heading">#</a></h2>
<p>The following language models are available on the Server. Find out which of those are capable of generating a slide deck.
E.g. run the prompt above for every LLM 10 times and count how often a pptx file is created.</p>
<p>Hints:</p>
<ul class="simple">
<li><p>You may have to specify the pptx filename to make this work.</p></li>
<li><p>To see what the agent is doing under the hood, consider setting <code class="docutils literal notranslate"><span class="pre">verbose=True</span></code>.</p></li>
</ul>
<p>Available models are:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;https://llm.scads.ai/v1&quot;</span><span class="p">,</span>
                       <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;SCADSAI_API_KEY&#39;</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">id</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">client</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">list</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id1">
<h2>Exercise<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>Program your own agent that creates a powerpoint presentation for a PDF you provide.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./66_arxiv_agent"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

            </article>
            

            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
  <a class='left-prev' id="prev-link" href="../50_function_calling/12_function_calling_scadsai_llm.html" title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
          <p class="prev-next-subtitle">previous</p>
          <p class="prev-next-title">Function calling using ScaDS.AI’s LLM service</p>
      </div>
  </a>
  <a class='right-next' id="next-link" href="../imprint.html" title="next page">
  <div class="prev-next-info">
      <p class="prev-next-subtitle">next</p>
      <p class="prev-next-title">Imprint</p>
  </div>
  <i class="fa-solid fa-angle-right"></i>
  </a>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise">
   Exercise
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Exercise
  </a>
 </li>
</ul>

</nav>
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Robert Haase
</p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    <p class="last-updated">
Last updated on 2025-03-09.<br>
</p>
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>
Copyright: Licensed <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">CC-BY 4.0</a> unless mentioned otherwise. 
Contributions and feedback are welcome.
</p>

</div>
  </div>
  
</div>
          </div>
        </footer>
        

      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  </body>
</html>