{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2f5b58-9e18-4c5a-9cf7-157a71c82cf2",
   "metadata": {},
   "source": [
    "# PowerPoint Karaoke about Arxiv papers\n",
    "In this notebook we program an agent that is capable of generating PowerPoint slide decks out of Arxiv papers. We will use the [smolagents](https://github.com/huggingface/smolagents) framework for programming the agent.\n",
    "\n",
    "We will use the [ScaDS.AI LLM infrastructure](https://llm.scads.ai/) infrastructure at the [Center for Information Services and High Performance Computing (ZIH) of TU Dresden](https://tu-dresden.de/zih). To use it, you must be connected via [TU Dresden VPN](https://tu-dresden.de/zih/dienste/service-katalog/arbeitsumgebung/zugang_datennetz/vpn) and have your API key stored in a `SCADSAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e08d14-4b62-4995-98b2-c8ebc9ad3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "from utilities import convert_to_markdown, search_arxiv, download_pdf, pdf_to_markdown, make_powerpoint_slides\n",
    "\n",
    "from smolagents.agents import ToolCallingAgent, CodeAgent\n",
    "from smolagents import tool, LiteLLMModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7397cb8-2f95-4bcb-9939-394efc29b729",
   "metadata": {},
   "source": [
    "First, we initialize the LLM. The server supports the OpenAI-API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae074353-f26d-4f8e-8b97-ef9ac5a661d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LiteLLMModel(model_id=\"openai/meta-llama/Llama-3.3-70B-Instruct\", \n",
    "                         api_base=\"https://llm.scads.ai/v1\", \n",
    "                         api_key=os.environ.get('SCADSAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719e892-177a-4b80-800c-9da41326755b",
   "metadata": {},
   "source": [
    "Next, we specify tools. The actual functionality is programmed in [arxiv_utilities.py](arxiv_utilities.py). Note: To make these functions work, they require detailed docstrings describing precisely what parameters the functions require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20164efd-2b4a-42e3-b05d-1c25a4253303",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []\n",
    "\n",
    "@tools.append\n",
    "@tool\n",
    "def search_publications(query:str=None, author:str=None, year:str=None, max_results:int=10)->str:\n",
    "    \"\"\"Searches the arxiv for papers using a query, selects papers from given authors and/or by year.\n",
    "    \n",
    "    Args:\n",
    "        query: Search terms\n",
    "        author: Author(s) of the searched items\n",
    "        year: publication year\n",
    "        max_results: maximum number of results\n",
    "\n",
    "    Returns:\n",
    "        Found paper(s)\n",
    "    \"\"\"\n",
    "    print(\"Searching...\")\n",
    "    papers = search_arxiv(query=query, author=author, year=year, max_results=max_results)\n",
    "    markdown = convert_to_markdown(papers)\n",
    "    \n",
    "    return markdown\n",
    "\n",
    "@tools.append\n",
    "@tool\n",
    "def download_paper(paper_link:str)->str:\n",
    "    \"\"\"Downloads a paper and return its contents as markdown.\n",
    "    \n",
    "    Args:\n",
    "        paper_link: url of the paper to be downloaded\n",
    "\n",
    "    Returns:\n",
    "        Content of the paper\n",
    "    \"\"\"\n",
    "    print(\"Downloading...\", filename)\n",
    "\n",
    "    filename = download_pdf(paper_link)\n",
    "\n",
    "    if filename is not None:\n",
    "        return pdf_to_markdown(filename)\n",
    "\n",
    "# You can also add external tools like this.\n",
    "tools.append(tool(make_powerpoint_slides))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec543c89-4743-415b-8bc7-fd8a00964ec3",
   "metadata": {},
   "source": [
    "We can then initialize the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf93ba3-9116-492d-bb26-0e9ea9a90c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CodeAgent(tools=tools, model=model,\n",
    "                 additional_authorized_imports=[\"json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4eeb54c-7951-41a3-9de3-ab67bcdceae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment this to show detailed output\n",
    "#from smolagents.utils import console\n",
    "#console.quiet = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05188920-a507-4f65-9188-eea961ed1ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">I need to give a presentation about the latest arxiv paper from the year 2022 that was about LLMs.</span>              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Please make a powerpoint slide deck about this paper.</span>                                                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">The first slide should have the same title as the paper, and mention the authors, and give a link to the paper.</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">The following slides are about the individual chapters of the paper.</span>                                            <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - openai/meta-llama/Llama-3.3-70B-Instruct ───────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mI need to give a presentation about the latest arxiv paper from the year 2022 that was about LLMs.\u001b[0m              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mPlease make a powerpoint slide deck about this paper.\u001b[0m                                                           \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mThe first slide should have the same title as the paper, and mention the authors, and give a link to the paper.\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mThe following slides are about the individual chapters of the paper.\u001b[0m                                            \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - openai/meta-llama/Llama-3.3-70B-Instruct \u001b[0m\u001b[38;2;212;183;2m──────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_link </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> search_publications(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"LLMs\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, year</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"2022\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, max_results</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">1</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                     </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(paper_link)</span><span style=\"background-color: #272822\">                                                                                              </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpaper_link\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msearch_publications\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLLMs\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34myear\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m2022\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmax_results\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m1\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                     \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_link\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                              \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search\n",
      "Searched # Search Results\n",
      "\n",
      "## Paper 1\n",
      "**Title:** Rethinking with Retrieval: Faithful Large Language Model Inference\n",
      "\n",
      "**Authors:** Hangfeng He, Hongming Zhang, Dan Roth\n",
      "\n",
      "**Published:** 2022-12-31T22:35:34Z\n",
      "\n",
      "**Summary:** Despite the success of large language models (LLMs) in various natural\n",
      "language processing (NLP) tasks, the stored knowledge in these models may\n",
      "inevitably be incomplete, out-of-date, or incorrect. This motivates the need to\n",
      "utilize external knowledge to assist LLMs. Unfortunately, current methods for\n",
      "incorporating external knowledge often require additional training or\n",
      "fine-tuning, which can be costly and may not be feasible for LLMs. To address\n",
      "this issue, we propose a novel post-processing approach, rethinking with\n",
      "retrieval (RR), which retrieves relevant external knowledge based on the\n",
      "decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting.\n",
      "This lightweight approach does not require additional training or fine-tuning\n",
      "and is not limited by the input length of LLMs. We evaluate the effectiveness\n",
      "of RR through extensive experiments with GPT-3 on three complex reasoning\n",
      "tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our\n",
      "results show that RR can produce more faithful explanations and improve the\n",
      "performance of LLMs.\n",
      "\n",
      "**Link:** [Read More](http://arxiv.org/abs/2301.00303v1)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "# Search Results\n",
       "\n",
       "## Paper 1\n",
       "**Title:** Rethinking with Retrieval: Faithful Large Language Model Inference\n",
       "\n",
       "**Authors:** Hangfeng He, Hongming Zhang, Dan Roth\n",
       "\n",
       "**Published:** 2022-12-31T22:35:34Z\n",
       "\n",
       "**Summary:** Despite the success of large language models (LLMs) in various natural\n",
       "language processing (NLP) tasks, the stored knowledge in these models may\n",
       "inevitably be incomplete, out-of-date, or incorrect. This motivates the need to\n",
       "utilize external knowledge to assist LLMs. Unfortunately, current methods for\n",
       "incorporating external knowledge often require additional training or\n",
       "fine-tuning, which can be costly and may not be feasible for LLMs. To address\n",
       "this issue, we propose a novel post-processing approach, rethinking with\n",
       "retrieval (RR), which retrieves relevant external knowledge based on the\n",
       "decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting.\n",
       "This lightweight approach does not require additional training or fine-tuning\n",
       "and is not limited by the input length of LLMs. We evaluate the effectiveness\n",
       "of RR through extensive experiments with GPT-3 on three complex reasoning\n",
       "tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our\n",
       "results show that RR can produce more faithful explanations and improve the\n",
       "performance of LLMs.\n",
       "\n",
       "**Link:** [Read More](http://arxiv.org/abs/2301.00303v1)\n",
       "\n",
       "\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "# Search Results\n",
       "\n",
       "## Paper 1\n",
       "**Title:** Rethinking with Retrieval: Faithful Large Language Model Inference\n",
       "\n",
       "**Authors:** Hangfeng He, Hongming Zhang, Dan Roth\n",
       "\n",
       "**Published:** 2022-12-31T22:35:34Z\n",
       "\n",
       "**Summary:** Despite the success of large language models (LLMs) in various natural\n",
       "language processing (NLP) tasks, the stored knowledge in these models may\n",
       "inevitably be incomplete, out-of-date, or incorrect. This motivates the need to\n",
       "utilize external knowledge to assist LLMs. Unfortunately, current methods for\n",
       "incorporating external knowledge often require additional training or\n",
       "fine-tuning, which can be costly and may not be feasible for LLMs. To address\n",
       "this issue, we propose a novel post-processing approach, rethinking with\n",
       "retrieval (RR), which retrieves relevant external knowledge based on the\n",
       "decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting.\n",
       "This lightweight approach does not require additional training or fine-tuning\n",
       "and is not limited by the input length of LLMs. We evaluate the effectiveness\n",
       "of RR through extensive experiments with GPT-3 on three complex reasoning\n",
       "tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our\n",
       "results show that RR can produce more faithful explanations and improve the\n",
       "performance of LLMs.\n",
       "\n",
       "**Link:** [Read More](http://arxiv.org/abs/2301.00303v1)\n",
       "\n",
       "\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 5.66 seconds| Input tokens: 2,633 | Output tokens: 109]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 5.66 seconds| Input tokens: 2,633 | Output tokens: 109]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_contents </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> download_paper(paper_link</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"http://arxiv.org/abs/2301.00303v1\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(paper_contents)</span><span style=\"background-color: #272822\">                                                                                          </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpaper_contents\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdownload_paper\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_link\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mhttp://arxiv.org/abs/2301.00303v1\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_contents\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "PDF downloaded: [http://arxiv.org/abs/2301.00303v1](pdf_filename), licensed CC-BY 4.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2301.00303v1.pdf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "\n",
       "# Page 1\n",
       "\n",
       "Rethinking with Retrieval: Faithful Large Language Model Inference\n",
       "\n",
       "Hangfeng He†∗ Hongming Zhang‡ Dan Roth§\n",
       "\n",
       "†University of Rochester ‡Tencent AI Lab, Seattle §University of Pennsylvania\n",
       "\n",
       "hanfeng.he@rochester.edu, hongmzhang@global.tencent.com\n",
       "\n",
       "danroth@seas.upenn.edu\n",
       "\n",
       "Abstract\n",
       "\n",
       "Despite the success of large language mod-\n",
       "\n",
       "els (LLMs) in various natural language pro-\n",
       "\n",
       "cessing (NLP) tasks, the stored knowledge\n",
       "\n",
       "in these models may inevitably be incom-\n",
       "\n",
       "plete, out-of-date, or incorrect. This mo-\n",
       "\n",
       "tivates the need to utilize external knowl-\n",
       "\n",
       "edge to assist LLMs. Unfortunately, current\n",
       "\n",
       "methods for incorporating external knowl-\n",
       "\n",
       "edge often require additional training or\n",
       "\n",
       "ﬁne-tuning, which can be costly and may\n",
       "\n",
       "not be feasible for LLMs. To address this\n",
       "\n",
       "issue, we propose a novel post-processing\n",
       "\n",
       "approach, rethinking with retrieval (RR),\n",
       "\n",
       "which retrieves relevant external knowledge\n",
       "\n",
       "based on the decomposed reasoning steps\n",
       "\n",
       "obtained from the chain-of-thought (CoT)\n",
       "\n",
       "prompting. This lightweight approach does\n",
       "\n",
       "not require additional training or ﬁne-tuning\n",
       "\n",
       "and is not limited by the input length of\n",
       "\n",
       "LLMs. We evaluate the effectiveness of RR\n",
       "\n",
       "through extensive experiments with GPT-3\n",
       "\n",
       "on three complex reasoning tasks: common-\n",
       "\n",
       "sense reasoning, temporal reasoning, and\n",
       "\n",
       "tabular reasoning. Our results show that RR\n",
       "\n",
       "can produce more faithful explanations and\n",
       "\n",
       "improve the performance of LLMs.1\n",
       "\n",
       "1 Introduction\n",
       "\n",
       "Large language models (LLMs) have shown\n",
       "\n",
       "exceptional performance across various tasks\n",
       "\n",
       "through in-context learning without task-speciﬁc\n",
       "\n",
       "training or ﬁne-tuning (Brown et al., 2020;\n",
       "\n",
       "Chowdhery et al., 2022; Zhang et al., 2022;\n",
       "\n",
       "Ouyang et al., 2022). Recent progress in prompt-\n",
       "\n",
       "ing (Wei et al., 2022; Zhou et al., 2022; Kojima\n",
       "\n",
       "et al., 2022) and decoding (Wang et al., 2022) has\n",
       "\n",
       "made it feasible for LLMs to tackle tasks that de-\n",
       "\n",
       "mand complex reasoning.\n",
       "\n",
       "∗Part of this work was done while the author was at the\n",
       "\n",
       "University of Pennsylvania.\n",
       "\n",
       "1Our code is publicly available at https://github.\n",
       "\n",
       "com/HornHehhf/RR.\n",
       "\n",
       "Query PredictionLLM\n",
       "\n",
       "Query Explanation + PredictionLLM\n",
       "\n",
       "Query Explanation + PredictionLLM\n",
       "\n",
       "(a)\n",
       "\n",
       "(b)\n",
       "\n",
       "(c)\n",
       "\n",
       "Knowledge\n",
       "\n",
       "Chain of thought\n",
       "\n",
       "Chain of thoughtRetrievalRethinking\n",
       "\n",
       "Figure 1: An overview of three approaches for using\n",
       "\n",
       "LLMs: (a) Standard prompting for generating a pre-\n",
       "\n",
       "diction in response to a query. (b) Chain-of-thought\n",
       "\n",
       "prompting for generating both an explanation and a\n",
       "\n",
       "prediction in response to a query. (c) Rethinking with\n",
       "\n",
       "retrieval, our proposed approach for using the decom-\n",
       "\n",
       "posed reasoning steps obtained from chain-of-thought\n",
       "\n",
       "prompting to retrieve relevant external knowledge for\n",
       "\n",
       "LLMs, leading to more faithful explanations and im-\n",
       "\n",
       "proved predictions in response to a query.\n",
       "\n",
       "However, the knowledge stored in LLMs might\n",
       "\n",
       "inevitably be incomplete, out-of-date, or incorrect.\n",
       "\n",
       "As a result, external sources of knowledge, such\n",
       "\n",
       "as Wikipedia, may be essential for the success-\n",
       "\n",
       "ful deployment of LLMs for real-world applica-\n",
       "\n",
       "tions. Previously, people tried to utilize knowl-\n",
       "\n",
       "edge for smaller language models (LMs), such\n",
       "\n",
       "as T5 (Raffel et al., 2020), BERT (Devlin et al.,\n",
       "\n",
       "2019), and RoBERTa (Liu et al., 2019). However,\n",
       "\n",
       "these methods often require additional training or\n",
       "\n",
       "ﬁne-tuning, which can be costly and thus imprac-\n",
       "\n",
       "tical for LLMs.\n",
       "\n",
       "In this paper, we present a post-processing\n",
       "\n",
       "approach called rethinking with retrieval (RR)\n",
       "\n",
       "for utilizing external knowledge in LLMs. Our\n",
       "\n",
       "method begins by using the chain-of-thought\n",
       "\n",
       "(CoT) prompting method (Wei et al., 2022) to gen-\n",
       "\n",
       "erate a diverse set of reasoning paths, as described\n",
       "\n",
       "in Wang et al. (2022). We then use each rea-\n",
       "\n",
       "soning step in those paths to retrieve relevant ex-\n",
       "\n",
       "ternal knowledge, which enables RR to provide\n",
       "\n",
       "arXiv:2301.00303v1  [cs.CL]  31 Dec 2022\n",
       "\n",
       "\n",
       "# Page 2\n",
       "\n",
       "more faithful explanations and more accurate pre-\n",
       "\n",
       "dictions, as illustrated in Figure 1.\n",
       "\n",
       "We evaluate the effectiveness of our proposed\n",
       "\n",
       "method, RR, on three complex reasoning tasks:\n",
       "\n",
       "commonsense reasoning, temporal reasoning, and\n",
       "\n",
       "tabular reasoning, using GPT-3 175B (Brown\n",
       "\n",
       "et al., 2020) and different external knowledge\n",
       "\n",
       "sources: Wikipedia, Wikidata (Vrande ˇci´c and\n",
       "\n",
       "Krötzsch, 2014), WordNet (Miller, 1995), and\n",
       "\n",
       "Conceptnet (Speer et al., 2017). The results\n",
       "\n",
       "demonstrate that RR consistently outperforms all\n",
       "\n",
       "baselines on all three tasks without requiring ad-\n",
       "\n",
       "ditional training or ﬁne-tuning, indicating the su-\n",
       "\n",
       "periority of our approach in leveraging external\n",
       "\n",
       "knowledge to enhance the performance of LLMs.\n",
       "\n",
       "2 Related Work\n",
       "\n",
       "Enhancing LMs through retrieval. Retrieval-\n",
       "\n",
       "enhanced LMs have received signiﬁcant attention\n",
       "\n",
       "as a means of improving performance through the\n",
       "\n",
       "incorporation of external knowledge. For exam-\n",
       "\n",
       "ple, the k-most similar training contexts can be re-\n",
       "\n",
       "trieved to improve the estimation of the next word\n",
       "\n",
       "distribution in both the training stage (Borgeaud\n",
       "\n",
       "et al., 2021) and the inference stage (Khandelwal\n",
       "\n",
       "et al., 2020). Furthermore, search query genera-\n",
       "\n",
       "tors have been adopted to generate search queries\n",
       "\n",
       "for search engines to retrieve relevant documents\n",
       "\n",
       "(Komeili et al., 2022; Shuster et al., 2022; Thop-\n",
       "\n",
       "pilan et al., 2022). Other approaches have uti-\n",
       "\n",
       "lized retrieved documents as the additional con-\n",
       "\n",
       "text in generation tasks (Joshi et al., 2020; Guu\n",
       "\n",
       "et al., 2020; Lewis et al., 2020). Nakano et al.\n",
       "\n",
       "(2021) instead use human feedback in a text-based\n",
       "\n",
       "web-browsing environment. Among these pre-\n",
       "\n",
       "vious works, Khandelwal et al. (2020) is most\n",
       "\n",
       "closely related to our approach. However, they\n",
       "\n",
       "focus on improving local inference by using the\n",
       "\n",
       "nearest neighbor datastore constructed from train-\n",
       "\n",
       "ing data, whereas we focus on conducting faith-\n",
       "\n",
       "ful inference using external knowledge. In con-\n",
       "\n",
       "trast to other aforementioned approaches, which\n",
       "\n",
       "require training or ﬁne-tuning to incorporate re-\n",
       "\n",
       "trieved knowledge, we propose a post-processing\n",
       "\n",
       "method for leveraging retrieved knowledge with-\n",
       "\n",
       "out additional training or ﬁne-tuning.\n",
       "\n",
       "Incorporating external knowledge into LMs.\n",
       "\n",
       "Signiﬁcant effort has been devoted to leveraging\n",
       "\n",
       "external knowledge to improve the reasoning abil-\n",
       "\n",
       "ity of LMs. Previous work has incorporated exter-\n",
       "\n",
       "nal knowledge sources such as WordNet (Miller,\n",
       "\n",
       "1995) and ConceptNet (Speer et al., 2017) to en-\n",
       "\n",
       "hance LMs for tabular reasoning tasks (Neeraja\n",
       "\n",
       "et al., 2021; Varun et al., 2022). Explicit rules\n",
       "\n",
       "have also been added to inputs to improve rea-\n",
       "\n",
       "soning ability over implicit knowledge (Talmor\n",
       "\n",
       "et al., 2020). In addition, explicit knowledge from\n",
       "\n",
       "Wikidata (Vrandeˇci´c and Krötzsch, 2014) and im-\n",
       "\n",
       "plicit knowledge in LLMs have been integrated\n",
       "\n",
       "into a transformer (Vaswani et al., 2017) for vi-\n",
       "\n",
       "sual question answering (Gui et al., 2021). Nye\n",
       "\n",
       "et al. (2021) instead introduces a symbolic reason-\n",
       "\n",
       "ing module to improve coherence and consistency\n",
       "\n",
       "in LLMs. Among these previous works, Nye et al.\n",
       "\n",
       "(2021) is the most relevant to our approach. Still,\n",
       "\n",
       "they focus on incorporating logical constraints to\n",
       "\n",
       "improve coherence and consistency, whereas we\n",
       "\n",
       "aim to improve the faithfulness of explanations\n",
       "\n",
       "through the use of external knowledge. In con-\n",
       "\n",
       "trast to other aforementioned approaches that in-\n",
       "\n",
       "corporate external knowledge before generation\n",
       "\n",
       "and require additional training or ﬁne-tuning, our\n",
       "\n",
       "proposal leverages external knowledge in a post-\n",
       "\n",
       "processing manner to enhance LMs without addi-\n",
       "\n",
       "tional training or ﬁne-tuning.\n",
       "\n",
       "Uncovering latent Knowledge in LLMs. There\n",
       "\n",
       "has been a line of work exploring the knowledge\n",
       "\n",
       "hidden within LLMs for reasoning. This has in-\n",
       "\n",
       "cluded the use of careful prompting to encourage\n",
       "\n",
       "LLMs to generate explanations in the reasoning\n",
       "\n",
       "process, such as through chain of thought prompt-\n",
       "\n",
       "ing in few-shot (Wei et al., 2022) or zero-shot\n",
       "\n",
       "(Kojima et al., 2022) learning, or through the use\n",
       "\n",
       "of scratchpads for intermediate computation (Nye\n",
       "\n",
       "et al., 2022). In addition, various methods based\n",
       "\n",
       "on sampling a diverse set of reasoning paths in\n",
       "\n",
       "LLMs have been proposed, including training ver-\n",
       "\n",
       "iﬁers to judge the correctness of model comple-\n",
       "\n",
       "tions (Cobbe et al., 2021), calibrating model pre-\n",
       "\n",
       "dictions based on the reliability of the explana-\n",
       "\n",
       "tions (Ye and Durrett, 2022), and promoting self-\n",
       "\n",
       "consistency over diverse reasoning paths (Wang\n",
       "\n",
       "et al., 2022). Zelikman et al. (2022) instead it-\n",
       "\n",
       "eratively bootstrap the ability of LLMs to gener-\n",
       "\n",
       "ate high-quality rationales from a few initial ex-\n",
       "\n",
       "amples. Liu et al. (2022) further propose generat-\n",
       "\n",
       "ing knowledge from LLMs, which is then used as\n",
       "\n",
       "additional input to improve commonsense reason-\n",
       "\n",
       "ing. In contrast to this line of work, our proposal\n",
       "\n",
       "focuses on leveraging external knowledge to en-\n",
       "\n",
       "hance LLMs, while they aim to explore the knowl-\n",
       "\n",
       "edge hidden within LLMs.\n",
       "\n",
       "\n",
       "# Page 3\n",
       "\n",
       "3 Rethinking with Retrieval\n",
       "\n",
       "LLMs have been shown to generate incorrect sup-\n",
       "\n",
       "porting facts from time to time, even when they ac-\n",
       "\n",
       "curately capture the perspective needed to answer\n",
       "\n",
       "a question. This phenomenon highlights intrinsic\n",
       "\n",
       "issues in the way LLMs store and retrieve knowl-\n",
       "\n",
       "edge, including (1) the presence of out-of-date,\n",
       "\n",
       "incorrect, or missing relevant knowledge in the\n",
       "\n",
       "pre-training corpus; (2) incorrect memorization of\n",
       "\n",
       "relevant knowledge during pre-training; and (3)\n",
       "\n",
       "incorrect retrieval of relevant knowledge during\n",
       "\n",
       "the inference stage. To address these issues, we\n",
       "\n",
       "propose the use of RR, which leverages external\n",
       "\n",
       "knowledge through the retrieval of relevant infor-\n",
       "\n",
       "mation based on decomposed reasoning steps.\n",
       "\n",
       "Overview. Given a queryQ, we utilize chain-of-\n",
       "\n",
       "thought prompting to generate a diverse set of rea-\n",
       "\n",
       "soning paths R1, R2, ···RN , where each reason-\n",
       "\n",
       "ing path Ri consists of an explanationEi followed\n",
       "\n",
       "by a prediction Pi. After that, we retrieve relevant\n",
       "\n",
       "knowledge K1, ···KM from a suitable knowledge\n",
       "\n",
       "base KBto support the explanation in each reason-\n",
       "\n",
       "ing path, and select the prediction ˆP that is most\n",
       "\n",
       "faithful to this knowledge. To better illustrate our\n",
       "\n",
       "proposal, we use “Did Aristotle use a laptop?” as\n",
       "\n",
       "a running example in this work.\n",
       "\n",
       "Chain-of-thought prompting. In contrast to\n",
       "\n",
       "standard prompting, CoT prompting (Wei et al.,\n",
       "\n",
       "2022) includes demonstrations of step-by-step rea-\n",
       "\n",
       "soning examples in the prompt to produce a series\n",
       "\n",
       "of short sentences that capture the reasoning pro-\n",
       "\n",
       "cess. For instance, given the question “ Did Aris-\n",
       "\n",
       "totle use a laptop?”, CoT prompting aims to gen-\n",
       "\n",
       "erate the complete reasoning path “Aristotle died\n",
       "\n",
       "in 322 BC. The ﬁrst laptop was invented in 1980.\n",
       "\n",
       "Thus, Aristotle did not use a laptop. So the answer\n",
       "\n",
       "is no.” rather than simply outputs “No.” Empirical\n",
       "\n",
       "results show that CoT prompting signiﬁcantly im-\n",
       "\n",
       "proves the performance of LLMs on many multi-\n",
       "\n",
       "step reasoning tasks. Therefore, we adopt CoT\n",
       "\n",
       "prompting to obtain both explanation E and pre-\n",
       "\n",
       "diction P for the query Q.\n",
       "\n",
       "Sampling diverse reasoning paths. Similar to\n",
       "\n",
       "Wang et al. (2022), we sample a diverse set of rea-\n",
       "\n",
       "soning paths R1, R2, ···RN rather than only con-\n",
       "\n",
       "sidering the greedy path as in Wei et al. (2022).\n",
       "\n",
       "For the question “Did Aristotle use a laptop?”, the\n",
       "\n",
       "potential reasoning paths can be as follows:\n",
       "\n",
       "(R1) Aristotle died in 2000. The ﬁrst laptop was\n",
       "\n",
       "invented in 1980. Thus, Aristotle used a lap-\n",
       "\n",
       "top. So the answer is yes.\n",
       "\n",
       "(R2) Aristotle died in 322BC. The ﬁrst laptop was\n",
       "\n",
       "invented in 2000. Thus, Aristotle did not use\n",
       "\n",
       "a laptop. So the answer is no.\n",
       "\n",
       "(R3) Aristotle died in 322BC. The ﬁrst laptop was\n",
       "\n",
       "invented in 1980. Thus, Aristotle did not use\n",
       "\n",
       "a laptop. So the answer is no.\n",
       "\n",
       "Knowledge retrieval. Different knowledge\n",
       "\n",
       "bases can be used to address different tasks. For\n",
       "\n",
       "example, to address the question “ Did Aristotle\n",
       "\n",
       "use a laptop? ”, we can use Wikipedia as the ex-\n",
       "\n",
       "ternal knowledge base KB. Information retrieval\n",
       "\n",
       "techniques can be applied to retrieve the relevant\n",
       "\n",
       "knowledge K1, ···KM from Wikipedia based\n",
       "\n",
       "on the decomposed reasoning steps. Ideally, we\n",
       "\n",
       "would obtain the following two paragraphs from\n",
       "\n",
       "Wikipedia for this question:\n",
       "\n",
       "(K1) Aristotle (384–322 BC) was a Greek philoso-\n",
       "\n",
       "pher and polymath during the Classical pe-\n",
       "\n",
       "riod in Ancient Greece. ...\n",
       "\n",
       "(K2) The Epson HX-20, the ﬁrst laptop computer,\n",
       "\n",
       "was invented in 1980. ...\n",
       "\n",
       "Faithful inference. The faithfulness of each rea-\n",
       "\n",
       "soning path Ri can be estimated using a function\n",
       "\n",
       "fKB(Ri), which is based on relevant knowledge\n",
       "\n",
       "K1, ··· , KM retrieved from the knowledge base\n",
       "\n",
       "KB. The ﬁnal prediction is obtained through the\n",
       "\n",
       "application of the following inference procedure2:\n",
       "\n",
       "ˆP = arg max\n",
       "\n",
       "Pi∈{P1,···,PN }\n",
       "\n",
       "N∑\n",
       "\n",
       "i=1\n",
       "\n",
       "1 (Pi = P)fKB(Ri), (1)\n",
       "\n",
       "where Pi denotes the corresponding prediction in\n",
       "\n",
       "the reasoning path Ri. This inference procedure\n",
       "\n",
       "is designed to identify the most faithful prediction\n",
       "\n",
       "ˆP to the knowledge base among all predictions in\n",
       "\n",
       "the N reasoning paths. For instance, in the run-\n",
       "\n",
       "ning example, given reasoning paths R1, R2, R3\n",
       "\n",
       "and the retrieved knowledgeK1, K2, the above in-\n",
       "\n",
       "ference procedure would output the prediction “So\n",
       "\n",
       "the answer is no.”, as it is supported by both R2\n",
       "\n",
       "and R3 and has a higher faithfulness score com-\n",
       "\n",
       "pared to the prediction “So the answer is yes.”,\n",
       "\n",
       "which is only supported by R1.\n",
       "\n",
       "2Note that this is the basic version of faithful inference,\n",
       "\n",
       "and further variations can be found in Section 5.3.\n",
       "\n",
       "\n",
       "# Page 4\n",
       "\n",
       "4 Experiments\n",
       "\n",
       "In this section, we present the evaluation of our\n",
       "\n",
       "proposed method, RR, on three complex reason-\n",
       "\n",
       "ing tasks: commonsense reasoning, temporal rea-\n",
       "\n",
       "soning, and tabular reasoning.\n",
       "\n",
       "4.1 Baselines\n",
       "\n",
       "We compare with the following baselines.\n",
       "\n",
       "Zero-shot/few-shot prompting. In our experi-\n",
       "\n",
       "ments, we consider GPT-3 with standard zero-\n",
       "\n",
       "shot/few-shot prompting as baselines, following\n",
       "\n",
       "the approach described in Brown et al. (2020), in\n",
       "\n",
       "which zero or few in-context exemplars of input-\n",
       "\n",
       "output pairs are provided in the prompt.\n",
       "\n",
       "Chain-of-thought prompting. In addition to\n",
       "\n",
       "the standard zero-shot/few-shot prompting, we\n",
       "\n",
       "also consider GPT-3 with the CoT prompting pro-\n",
       "\n",
       "posed in (Wei et al., 2022) as a baseline in our ex-\n",
       "\n",
       "periments. This approach involves feeding LLMs\n",
       "\n",
       "step-by-step reasoning examples instead of stan-\n",
       "\n",
       "dard input-output examples.\n",
       "\n",
       "Self-consistency. In addition, we also consider\n",
       "\n",
       "self-consistency (Wang et al., 2022) as a baseline\n",
       "\n",
       "in our experiments. This approach, proposed as an\n",
       "\n",
       "alternative to the naive greedy decoding used in\n",
       "\n",
       "CoT prompting (Wei et al., 2022), involves sam-\n",
       "\n",
       "pling a diverse set of reasoning paths and select-\n",
       "\n",
       "ing the most consistent answer by marginalizing\n",
       "\n",
       "the sampled paths.\n",
       "\n",
       "4.2 Commonsense Reasoning\n",
       "\n",
       "Dataset description. For commonsense reason-\n",
       "\n",
       "ing, we consider the StrategyQA dataset (Geva\n",
       "\n",
       "et al., 2021), which includes questions that require\n",
       "\n",
       "implicit reasoning strategies. For example, the\n",
       "\n",
       "question “Did Aristotle use a laptop? ” requires\n",
       "\n",
       "implicit decomposition into reasoning steps, while\n",
       "\n",
       "the question “Was Aristotle alive when the laptop\n",
       "\n",
       "was invented?” explicitly speciﬁes the reasoning\n",
       "\n",
       "process. The StrategyQA dataset includes 2, 290\n",
       "\n",
       "training examples, each consisting of a question\n",
       "\n",
       "(Q), a yes/no answer (A), a decomposition (D),\n",
       "\n",
       "evidence paragraphs (E), and supporting facts (F).\n",
       "\n",
       "On average, each question requires about2.93 rea-\n",
       "\n",
       "soning steps and 2.33 evidence paragraphs. In ad-\n",
       "\n",
       "dition, a development set is constructed by ran-\n",
       "\n",
       "domly sampling 10% of the training examples\n",
       "\n",
       "(i.e., 229 examples). The answer distribution is\n",
       "\n",
       "roughly balanced, with approximately 47% \"yes\"\n",
       "\n",
       "questions in both the training and development\n",
       "\n",
       "sets. Unless otherwise speciﬁed, the models are\n",
       "\n",
       "evaluated on the development set3 for StrategyQA.\n",
       "\n",
       "Implementation details. In this part, we uti-\n",
       "\n",
       "lize Wikipedia as the external knowledge base\n",
       "\n",
       "KB. For each sentence in the explanation of ev-\n",
       "\n",
       "ery reasoning path, we ﬁrst apply BM25 (Robert-\n",
       "\n",
       "son et al., 2009) to retrieve the top 10 most rele-\n",
       "\n",
       "vant paragraphs from Wikipedia. In particular, we\n",
       "\n",
       "use the re-implementation of the sparse retrieval\n",
       "\n",
       "BM254 in Karpukhin et al. (2020) from Pyserini\n",
       "\n",
       "(Lin et al., 2021). Subsequently, we use the pre-\n",
       "\n",
       "trained MPNet model (Song et al., 2020) to se-\n",
       "\n",
       "lect the most similar paragraph based on the cosine\n",
       "\n",
       "similarity between the sentence embeddings of the\n",
       "\n",
       "retrieved paragraph and the sentence. We then\n",
       "\n",
       "employ a pre-trained natural language inference\n",
       "\n",
       "(NLI) model (Nie et al., 2020) to obtain the en-\n",
       "\n",
       "tailment and contradiction scores for the sentence,\n",
       "\n",
       "treating the most similar paragraph as the premise.\n",
       "\n",
       "The faithfulness of each reasoning path is then\n",
       "\n",
       "calculated using fKB(·) based on the entailment\n",
       "\n",
       "scores, contradiction scores, and MPNet similari-\n",
       "\n",
       "ties of all sentences in the explanation of the rea-\n",
       "\n",
       "soning path. The ﬁnal prediction for each ques-\n",
       "\n",
       "tion is obtained through faithful inference (Equa-\n",
       "\n",
       "tion 1). More details about fKB(·) can be found in\n",
       "\n",
       "Appendix A.2.\n",
       "\n",
       "4.3 Temporal Reasoning\n",
       "\n",
       "Dataset description. In this experiment, we use\n",
       "\n",
       "the TempQuestions dataset (Jia et al., 2018) to\n",
       "\n",
       "investigate temporal reasoning. This dataset in-\n",
       "\n",
       "cludes 1, 271 temporal questions that are divided\n",
       "\n",
       "into four classes: explicit temporal, implicit tem-\n",
       "\n",
       "poral, temporal answer, and ordinal constraints.\n",
       "\n",
       "The questions are paired with their answers from\n",
       "\n",
       "Freebase (Bollacker et al., 2008). To examine the\n",
       "\n",
       "most challenging aspect of temporal reasoning, we\n",
       "\n",
       "focus on the set of implicit temporal questions,\n",
       "\n",
       "which contain implicit temporal expressions, in-\n",
       "\n",
       "cluding free-text temporal expressions. For ex-\n",
       "\n",
       "ample, the question “who was governor of oregon\n",
       "\n",
       "when shanghai noon was released?” is an implicit\n",
       "\n",
       "temporal question. To facilitate our analysis, we\n",
       "\n",
       "only consider questions with a single answer, re-\n",
       "\n",
       "sulting in a total of 175 examples. Of these ex-\n",
       "\n",
       "3As the annotations for the test set are not publicly avail-\n",
       "\n",
       "able, we use the development set for evaluation. This allows\n",
       "\n",
       "us to perform a more comprehensive analysis.\n",
       "\n",
       "4We also experimented with DPR and BM25+DPR, and\n",
       "\n",
       "found that BM25 outperformed these methods in our experi-\n",
       "\n",
       "ments. More details can be found in Appendix A.3.\n",
       "\n",
       "\n",
       "# Page 5\n",
       "\n",
       "Methods Commonsense Temporal Tabular\n",
       "\n",
       "GPT-3\n",
       "\n",
       "Zero-shot prompting 58.08 28.40 82.00\n",
       "\n",
       "Few-shot prompting 63.32 29.59 83.08\n",
       "\n",
       "Chain-of-thought prompting 65.94 33.14 83.33\n",
       "\n",
       "Self-consistency 73.36 37.28 84.00\n",
       "\n",
       "Rethinking with retrieval 77.73 39.05 84.83\n",
       "\n",
       "Table 1: Performance of different methods using GPT-3 on three reasoning tasks.\n",
       "\n",
       "amples, the ﬁrst 6 are used for prompting, and the\n",
       "\n",
       "remaining 169 are used for evaluation.\n",
       "\n",
       "Implementation details. In this part, we utilize\n",
       "\n",
       "Wikidata (Vrande ˇci´c and Krötzsch, 2014) as the\n",
       "\n",
       "external knowledge base KB, as it is the largest\n",
       "\n",
       "publicly available knowledge graph, and the data\n",
       "\n",
       "from Freebase has been migrated to Wikidata. To\n",
       "\n",
       "incorporate this knowledge into our system, we\n",
       "\n",
       "apply an entity linking system 5 to each sentence\n",
       "\n",
       "in the explanation of each reasoning path to iden-\n",
       "\n",
       "tify the corresponding Wikidata pages for all enti-\n",
       "\n",
       "ties in the sentence. Next, we extract all temporal\n",
       "\n",
       "relations from these relevant Wikidata pages and\n",
       "\n",
       "use templates to convert these temporal relations\n",
       "\n",
       "into sentences. This step generates a set of rele-\n",
       "\n",
       "vant knowledge sentences for each sentence in the\n",
       "\n",
       "explanation of each reasoning path. The ﬁnal pre-\n",
       "\n",
       "diction is then obtained by applying the procedure\n",
       "\n",
       "described in Section 4.2, in which the retrieved\n",
       "\n",
       "paragraphs are replaced with the relevant knowl-\n",
       "\n",
       "edge sentences from the current part.\n",
       "\n",
       "4.4 Tabular Reasoning\n",
       "\n",
       "Dataset description. We consider the IN-\n",
       "\n",
       "FOTABS dataset (Gupta et al., 2020) for tabu-\n",
       "\n",
       "lar reasoning, which consists of 23, 738 human-\n",
       "\n",
       "written textual hypotheses based on premises in\n",
       "\n",
       "the form of tables extracted from 2, 540 unique\n",
       "\n",
       "Wikipedia info-boxes. We focus on the develop-\n",
       "\n",
       "ment set, which includes 1, 800 hypotheses based\n",
       "\n",
       "on 200 tables, and only consider entailed and con-\n",
       "\n",
       "tradictory hypotheses as it is tricky to write CoT\n",
       "\n",
       "demonstrations for neutral hypotheses. This re-\n",
       "\n",
       "sults in a total of 1, 200 hypotheses based on 200\n",
       "\n",
       "tables for evaluation, with an equal number of en-\n",
       "\n",
       "tailed and contradictory hypotheses.\n",
       "\n",
       "Implementation details. In this part, we utilize\n",
       "\n",
       "WordNet (Miller, 1995) and ConceptNet (Speer\n",
       "\n",
       "5We use the spacy entity linker:https://pypi.org/\n",
       "\n",
       "project/spacy-entity-linker/.\n",
       "\n",
       "et al., 2017) as external knowledge bases. To con-\n",
       "\n",
       "vert tables into textual premises, we follow the\n",
       "\n",
       "same technique as in Varun et al. (2022). For each\n",
       "\n",
       "premise-hypothesis pair, we follow the procedure\n",
       "\n",
       "outlined in Varun et al. (2022) to retrieve rele-\n",
       "\n",
       "vant word relation triples that connect the premise\n",
       "\n",
       "and hypothesis words, such as “married” RelatedTo←−−−−→\n",
       "\n",
       "“spouse”. These triples are then converted into\n",
       "\n",
       "sentences using some simple templates. The re-\n",
       "\n",
       "sulting sentences, along with the textual premises\n",
       "\n",
       "from the tables, serve as relevant knowledge for\n",
       "\n",
       "each sentence in the explanation of each reasoning\n",
       "\n",
       "path. To obtain the ﬁnal prediction, the procedure\n",
       "\n",
       "described in Section 4.2 is applied, whereby the\n",
       "\n",
       "retrieved paragraphs in Section 4.2 are replaced\n",
       "\n",
       "with the relevant knowledge from the current part.\n",
       "\n",
       "4.5 Evaluation\n",
       "\n",
       "Experimental settings. In all experiments, we\n",
       "\n",
       "utilize GPT-3 text-davinci-002 unless oth-\n",
       "\n",
       "erwise stated. The maximum number of tokens for\n",
       "\n",
       "generation during completion is set to 256. For\n",
       "\n",
       "zero-shot, few-shot, and chain-of-thought prompt-\n",
       "\n",
       "ing, the temperature is ﬁxed at 0. For self-\n",
       "\n",
       "consistency and rethinking with retrieval, we ran-\n",
       "\n",
       "domly sample 10 outputs6 with temperature 0.7.\n",
       "\n",
       "Detailed prompts can be found in Appendix A.1.\n",
       "\n",
       "We evaluate the performance of different methods\n",
       "\n",
       "on commonsense and tabular reasoning using ac-\n",
       "\n",
       "curacy, and on temporal reasoning using the exact\n",
       "\n",
       "match metric as deﬁned in Rajpurkar et al. (2016).\n",
       "\n",
       "Results. As shown in Table 1, our proposed\n",
       "\n",
       "method, rethinking with retrieval, consistently\n",
       "\n",
       "outperforms all baselines on all three reasoning\n",
       "\n",
       "tasks without requiring additional training or ﬁne-\n",
       "\n",
       "tuning. The results highlight the effectiveness of\n",
       "\n",
       "our approach in leveraging external knowledge to\n",
       "\n",
       "improve the performance of LLMs.\n",
       "\n",
       "6For commonsense reasoning, we sample9 outputs, as we\n",
       "\n",
       "have found that odd numbers of outputs tend to yield better\n",
       "\n",
       "voting performance for self-consistency on StrategyQA.\n",
       "\n",
       "\n",
       "# Page 6\n",
       "\n",
       "Wrong supporting facts Wrong inference\n",
       "\n",
       "Question Was Lil Jon’s top ranked Billboard song a\n",
       "\n",
       "collaboration with a member of The Lox?\n",
       "\n",
       "Would the top of Mount Fuji stick out of\n",
       "\n",
       "the Sea of Japan?\n",
       "\n",
       "Answer No Yes\n",
       "\n",
       "Facts\n",
       "\n",
       "Lil Jon’s highest ranked billboard song\n",
       "\n",
       "was Yeah; Yeah was a collaboration be-\n",
       "\n",
       "tween Lil Jon, Usher, and Ludacris; The\n",
       "\n",
       "Lox is a rap trio consisting of: Styles P,\n",
       "\n",
       "Sheek Louch, and Jadakiss.\n",
       "\n",
       "The average depth of the Sea of Japan is\n",
       "\n",
       "5,748 feet (1,752 metres) and its maxi-\n",
       "\n",
       "mum depth is 12,276 feet (3,742 metres);\n",
       "\n",
       "Mount Fuji is 3,776.24 metres (12,389.2\n",
       "\n",
       "ft) tall.\n",
       "\n",
       "GPT-3\n",
       "\n",
       "Lil Jon’s top ranked Billboard song was\n",
       "\n",
       "\"Get Low\" with the East Side Boyz. The\n",
       "\n",
       "Lox is not a member of the East Side\n",
       "\n",
       "Boyz. Thus, Lil Jon’s top ranked Bill-\n",
       "\n",
       "board song was not a collaboration with\n",
       "\n",
       "a member of The Lox. So the answer is\n",
       "\n",
       "no.\n",
       "\n",
       "Mount Fuji is 3,776 meters tall. The\n",
       "\n",
       "Sea of Japan is about 3,741 meters deep.\n",
       "\n",
       "Thus, the top of Mount Fuji would not\n",
       "\n",
       "stick out of the Sea of Japan. So the an-\n",
       "\n",
       "swer is no.\n",
       "\n",
       "Table 2: Examples of incorrect outputs from GPT-3 with CoT prompting.\n",
       "\n",
       "5 Analysis\n",
       "\n",
       "In this section, we perform a thorough analysis to\n",
       "\n",
       "gain a deeper understanding of RR.\n",
       "\n",
       "5.1 Limitations of LLMs in Reasoning\n",
       "\n",
       "In this subsection, we present an analysis of GPT-\n",
       "\n",
       "3 with CoT prompting on the StrategyQA dataset.\n",
       "\n",
       "Upon closer examination of the outputs of GPT-\n",
       "\n",
       "3, we observed that it can provide reasonable ex-\n",
       "\n",
       "planations and correct predictions for a number\n",
       "\n",
       "of questions. For example, when given the ques-\n",
       "\n",
       "tion “Will the Albany in Georgia reach a hundred\n",
       "\n",
       "thousand occupants before the one in New York?”,\n",
       "\n",
       "GPT-3 produced the following output:\n",
       "\n",
       "The Albany in New York has a pop-\n",
       "\n",
       "ulation of about 98,000. The Albany\n",
       "\n",
       "in Georgia has a population of about\n",
       "\n",
       "77,000. Thus, the Albany in New York\n",
       "\n",
       "is more populous than the Albany in\n",
       "\n",
       "Georgia. So the answer is no.\n",
       "\n",
       "The above output consists of three components:\n",
       "\n",
       "(1) supporting facts (in cyan) that are based on a\n",
       "\n",
       "particular perspective, (2) chaining arguments (in\n",
       "\n",
       "orange), and (3) a prediction (in green). Com-\n",
       "\n",
       "ponents (1) and (2) contribute to the explanation.\n",
       "\n",
       "Overall, the output exhibits a high level of quality.\n",
       "\n",
       "However, we also observed that GPT-3 may occa-\n",
       "\n",
       "sionally produce incorrect supporting facts for its\n",
       "\n",
       "explanations or make incorrect inferences for its\n",
       "\n",
       "Retrieval Commonsense Tabular\n",
       "\n",
       "Query-based 73.36 36.69\n",
       "\n",
       "Decomposition-based 77.73 39.05\n",
       "\n",
       "Table 3: Comparison of query-based and\n",
       "\n",
       "decomposition-based retrieval on commonsense\n",
       "\n",
       "and tabular reasoning.\n",
       "\n",
       "predictions, despite generally being able to iden-\n",
       "\n",
       "tify suitable perspectives.\n",
       "\n",
       "Wrong supporting facts. As shown in Table 2,\n",
       "\n",
       "GPT-3 provides the incorrect supporting fact for\n",
       "\n",
       "Lil Jon’s top-ranked Billboard song, stating that\n",
       "\n",
       "it was “Get Low” instead of the correct answer,\n",
       "\n",
       "“Yeah”. However, it does have the correct per-\n",
       "\n",
       "spective on how to answer the question, “ Was \n",
       "..._This content has been truncated to stay below 50000 characters_...\n",
       "iﬁcantly outperform the voting baseline, self-\n",
       "\n",
       "consistency (i.e., 73.36%), as shown in Table 1.\n",
       "\n",
       "It is noteworthy that UniﬁedQA performs\n",
       "\n",
       "poorly on StrategyQA, achieving an accuracy of\n",
       "\n",
       "only 58.95%. However, when provided with\n",
       "\n",
       "gold supporting facts in StrategyQA, UniﬁedQA\n",
       "\n",
       "demonstrates excellent performance with an accu-\n",
       "\n",
       "racy of 90.83%. This suggests that UniﬁedQA is\n",
       "\n",
       "suitable for last-step inference, but not effective\n",
       "\n",
       "for answering questions in StrategyQA.\n",
       "\n",
       "5.4 Impact of the Size of LMs\n",
       "\n",
       "In this subsection, we examine the effect of the\n",
       "\n",
       "size of LMs on the performance of our proposed\n",
       "\n",
       "method, speciﬁcally in the context of the fact gen-\n",
       "\n",
       "eration variant. We compare the performance of\n",
       "\n",
       "our method using various sizes of OPT models\n",
       "\n",
       "(Zhang et al., 2022) in addition to GPT-3 (175B)\n",
       "\n",
       "using the same experimental setup as in Sec-\n",
       "\n",
       "tion 5.3. As shown in Figure 2, our proposed\n",
       "\n",
       "method (Variant II) consistently outperforms CoT\n",
       "\n",
       "prompting in terms of both prediction accuracy\n",
       "\n",
       "and the faithfulness of explanations, even when\n",
       "\n",
       "using smaller LMs.\n",
       "\n",
       "6 Conclusion\n",
       "\n",
       "In conclusion, the proposed approach is a promis-\n",
       "\n",
       "ing solution for utilizing external knowledge to as-\n",
       "\n",
       "sist LLMs. Unlike traditional methods, RR does\n",
       "\n",
       "not require additional training or ﬁne-tuning, mak-\n",
       "\n",
       "ing it a lightweight and feasible option for LLMs.\n",
       "\n",
       "Through extensive experiments on three reason-\n",
       "\n",
       "ing tasks using GPT-3, we have shown that RR is\n",
       "\n",
       "able to produce more faithful explanations and im-\n",
       "\n",
       "prove the performance of LLMs. In the future, we\n",
       "\n",
       "plan to investigate various variations of RR to en-\n",
       "\n",
       "hance its effectiveness and efﬁciency in augment-\n",
       "\n",
       "ing LLMs with external knowledge.\n",
       "\n",
       "\n",
       "# Page 9\n",
       "\n",
       "References\n",
       "\n",
       "Kurt Bollacker, Colin Evans, Praveen Paritosh,\n",
       "\n",
       "Tim Sturge, and Jamie Taylor. 2008. Freebase:\n",
       "\n",
       "a collaboratively created graph database for\n",
       "\n",
       "structuring human knowledge. In Proceedings\n",
       "\n",
       "of the 2008 ACM SIGMOD international con-\n",
       "\n",
       "ference on Management of data , pages 1247–\n",
       "\n",
       "1250.\n",
       "\n",
       "Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-\n",
       "\n",
       "mann, Trevor Cai, Eliza Rutherford, Katie Mil-\n",
       "\n",
       "lican, George van den Driessche, Jean-Baptiste\n",
       "\n",
       "Lespiau, Bogdan Damoc, Aidan Clark, et al.\n",
       "\n",
       "2021. Improving language models by retriev-\n",
       "\n",
       "ing from trillions of tokens. arXiv preprint\n",
       "\n",
       "arXiv:2112.04426.\n",
       "\n",
       "Tom Brown, Benjamin Mann, Nick Ryder,\n",
       "\n",
       "Melanie Subbiah, Jared D Kaplan, Prafulla\n",
       "\n",
       "Dhariwal, Arvind Neelakantan, Pranav Shyam,\n",
       "\n",
       "Girish Sastry, Amanda Askell, et al. 2020.\n",
       "\n",
       "Language models are few-shot learners. Ad-\n",
       "\n",
       "vances in neural information processing sys-\n",
       "\n",
       "tems, 33:1877–1901.\n",
       "\n",
       "Aakanksha Chowdhery, Sharan Narang, Jacob De-\n",
       "\n",
       "vlin, Maarten Bosma, Gaurav Mishra, Adam\n",
       "\n",
       "Roberts, Paul Barham, Hyung Won Chung,\n",
       "\n",
       "Charles Sutton, Sebastian Gehrmann, et al.\n",
       "\n",
       "2022. Palm: Scaling language modeling with\n",
       "\n",
       "pathways. arXiv preprint arXiv:2204.02311.\n",
       "\n",
       "Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-\n",
       "\n",
       "ian, Jacob Hilton, Reiichiro Nakano, Christo-\n",
       "\n",
       "pher Hesse, and John Schulman. 2021. Training\n",
       "\n",
       "veriﬁers to solve math word problems. arXiv\n",
       "\n",
       "preprint arXiv:2110.14168.\n",
       "\n",
       "Ido Dagan, Oren Glickman, and Bernardo\n",
       "\n",
       "Magnini. 2005. The pascal recognising tex-\n",
       "\n",
       "tual entailment challenge. In Machine learning\n",
       "\n",
       "challenges workshop, pages 177–190. Springer.\n",
       "\n",
       "Daniel Deutsch, Tania Bedrax-Weiss, and Dan\n",
       "\n",
       "Roth. 2021. Towards question-answering as\n",
       "\n",
       "an automatic metric for evaluating the content\n",
       "\n",
       "quality of a summary. Transactions of the Asso-\n",
       "\n",
       "ciation for Computational Linguistics , 9:774–\n",
       "\n",
       "789.\n",
       "\n",
       "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
       "\n",
       "Kristina Toutanova. 2019. BERT: Pre-training\n",
       "\n",
       "of deep bidirectional transformers for language\n",
       "\n",
       "understanding. In Proceedings of the 2019\n",
       "\n",
       "Conference of the North American Chapter\n",
       "\n",
       "of the Association for Computational Linguis-\n",
       "\n",
       "tics: Human Language Technologies, Volume 1\n",
       "\n",
       "(Long and Short Papers), pages 4171–4186.\n",
       "\n",
       "Alexander R Fabbri, Chien-Sheng Wu, Wenhao\n",
       "\n",
       "Liu, and Caiming Xiong. 2021. Qafacte-\n",
       "\n",
       "val: Improved qa-based factual consistency\n",
       "\n",
       "evaluation for summarization. arXiv preprint\n",
       "\n",
       "arXiv:2112.08542.\n",
       "\n",
       "Mor Geva, Daniel Khashabi, Elad Segal, Tushar\n",
       "\n",
       "Khot, Dan Roth, and Jonathan Berant. 2021.\n",
       "\n",
       "Did aristotle use a laptop? a question answer-\n",
       "\n",
       "ing benchmark with implicit reasoning strate-\n",
       "\n",
       "gies. Transactions of the Association for Com-\n",
       "\n",
       "putational Linguistics, 9:346–361.\n",
       "\n",
       "Liangke Gui, Borui Wang, Qiuyuan Huang, Alex\n",
       "\n",
       "Hauptmann, Yonatan Bisk, and Jianfeng Gao.\n",
       "\n",
       "2021. Kat: A knowledge augmented trans-\n",
       "\n",
       "former for vision-and-language. arXiv preprint\n",
       "\n",
       "arXiv:2112.08614.\n",
       "\n",
       "Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and\n",
       "\n",
       "Vivek Srikumar. 2020. Infotabs: Inference on\n",
       "\n",
       "tables as semi-structured data. In Proceed-\n",
       "\n",
       "ings of the 58th Annual Meeting of the As-\n",
       "\n",
       "sociation for Computational Linguistics , pages\n",
       "\n",
       "2309–2324.\n",
       "\n",
       "Kelvin Guu, Kenton Lee, Zora Tung, Panupong\n",
       "\n",
       "Pasupat, and Mingwei Chang. 2020. Retrieval\n",
       "\n",
       "augmented language model pre-training. In In-\n",
       "\n",
       "ternational Conference on Machine Learning ,\n",
       "\n",
       "pages 3929–3938. PMLR.\n",
       "\n",
       "Or Honovich, Roee Aharoni, Jonathan Herzig,\n",
       "\n",
       "Hagai Taitelbaum, Doron Kukliansy, Vered Co-\n",
       "\n",
       "hen, Thomas Scialom, Idan Szpektor, Avinatan\n",
       "\n",
       "Hassidim, and Yossi Matias. 2022. True: Re-\n",
       "\n",
       "evaluating factual consistency evaluation. In\n",
       "\n",
       "Proceedings of the Second DialDoc Workshop\n",
       "\n",
       "on Document-grounded Dialogue and Conver-\n",
       "\n",
       "sational Question Answering, pages 161–175.\n",
       "\n",
       "Or Honovich, Leshem Choshen, Roee Aharoni,\n",
       "\n",
       "Ella Neeman, Idan Szpektor, and Omri Abend.\n",
       "\n",
       "2021. Q2:: Evaluating factual consistency\n",
       "\n",
       "in knowledge-grounded dialogues via question\n",
       "\n",
       "generation and question answering. In Pro-\n",
       "\n",
       "ceedings of the 2021 Conference on Empiri-\n",
       "\n",
       "cal Methods in Natural Language Processing ,\n",
       "\n",
       "pages 7856–7870.\n",
       "\n",
       "\n",
       "# Page 10\n",
       "\n",
       "Zhen Jia, Abdalghani Abujabal, Rishiraj\n",
       "\n",
       "Saha Roy, Jannik Strötgen, and Gerhard\n",
       "\n",
       "Weikum. 2018. Tempquestions: A benchmark\n",
       "\n",
       "for temporal question answering. In Compan-\n",
       "\n",
       "ion Proceedings of the The Web Conference\n",
       "\n",
       "2018, pages 1057–1062.\n",
       "\n",
       "Mandar Joshi, Kenton Lee, Yi Luan, and Kristina\n",
       "\n",
       "Toutanova. 2020. Contextualized representa-\n",
       "\n",
       "tions using textual encyclopedic knowledge.\n",
       "\n",
       "arXiv preprint arXiv:2004.12006.\n",
       "\n",
       "Vladimir Karpukhin, Barlas Oguz, Sewon Min,\n",
       "\n",
       "Patrick Lewis, Ledell Wu, Sergey Edunov,\n",
       "\n",
       "Danqi Chen, and Wen-tau Yih. 2020. Dense\n",
       "\n",
       "passage retrieval for open-domain question an-\n",
       "\n",
       "swering. In Proceedings of the 2020 Confer-\n",
       "\n",
       "ence on Empirical Methods in Natural Lan-\n",
       "\n",
       "guage Processing (EMNLP), pages 6769–6781.\n",
       "\n",
       "Urvashi Khandelwal, Omer Levy, Dan Juraf-\n",
       "\n",
       "sky, Luke Zettlemoyer, and Mike Lewis. 2020.\n",
       "\n",
       "Generalization through memorization: Nearest\n",
       "\n",
       "neighbor language models. In International\n",
       "\n",
       "Conference on Learning Representations.\n",
       "\n",
       "Daniel Khashabi, Yeganeh Kordi, and Hannaneh\n",
       "\n",
       "Hajishirzi. 2022. Uniﬁedqa-v2: Stronger gen-\n",
       "\n",
       "eralization via broader cross-format training.\n",
       "\n",
       "arXiv preprint arXiv:2202.12359.\n",
       "\n",
       "Daniel Khashabi, Sewon Min, Tushar Khot,\n",
       "\n",
       "Ashish Sabharwal, Oyvind Tafjord, Peter Clark,\n",
       "\n",
       "and Hannaneh Hajishirzi. 2020. Uniﬁedqa:\n",
       "\n",
       "Crossing format boundaries with a single qa\n",
       "\n",
       "system. In Findings of the Association for Com-\n",
       "\n",
       "putational Linguistics: EMNLP 2020 , pages\n",
       "\n",
       "1896–1907.\n",
       "\n",
       "Takeshi Kojima, Shixiang Shane Gu, Machel\n",
       "\n",
       "Reid, Yutaka Matsuo, and Yusuke Iwasawa.\n",
       "\n",
       "2022. Large language models are zero-shot rea-\n",
       "\n",
       "soners. arXiv preprint arXiv:2205.11916.\n",
       "\n",
       "Mojtaba Komeili, Kurt Shuster, and Jason Weston.\n",
       "\n",
       "2022. Internet-augmented dialogue generation.\n",
       "\n",
       "In Proceedings of the 60th Annual Meeting of\n",
       "\n",
       "the Association for Computational Linguistics\n",
       "\n",
       "(Volume 1: Long Papers), pages 8460–8478.\n",
       "\n",
       "Patrick Lewis, Ethan Perez, Aleksandra Piktus,\n",
       "\n",
       "Fabio Petroni, Vladimir Karpukhin, Naman\n",
       "\n",
       "Goyal, Heinrich Küttler, Mike Lewis, Wen-tau\n",
       "\n",
       "Yih, Tim Rocktäschel, et al. 2020. Retrieval-\n",
       "\n",
       "augmented generation for knowledge-intensive\n",
       "\n",
       "nlp tasks. Advances in Neural Information Pro-\n",
       "\n",
       "cessing Systems, 33:9459–9474.\n",
       "\n",
       "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\n",
       "\n",
       "Jheng-Hong Yang, Ronak Pradeep, and Rodrigo\n",
       "\n",
       "Nogueira. 2021. Pyserini: A Python toolkit for\n",
       "\n",
       "reproducible information retrieval research with\n",
       "\n",
       "sparse and dense representations. In Proceed-\n",
       "\n",
       "ings of the 44th Annual International ACM SI-\n",
       "\n",
       "GIR Conference on Research and Development\n",
       "\n",
       "in Information Retrieval (SIGIR 2021) , pages\n",
       "\n",
       "2356–2362.\n",
       "\n",
       "Jiacheng Liu, Alisa Liu, Ximing Lu, Sean\n",
       "\n",
       "Welleck, Peter West, Ronan Le Bras, Yejin\n",
       "\n",
       "Choi, and Hannaneh Hajishirzi. 2022. Gen-\n",
       "\n",
       "erated knowledge prompting for commonsense\n",
       "\n",
       "reasoning. In Proceedings of the 60th Annual\n",
       "\n",
       "Meeting of the Association for Computational\n",
       "\n",
       "Linguistics (Volume 1: Long Papers) , pages\n",
       "\n",
       "3154–3169.\n",
       "\n",
       "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei\n",
       "\n",
       "Du, Mandar Joshi, Danqi Chen, Omer Levy,\n",
       "\n",
       "Mike Lewis, Luke Zettlemoyer, and Veselin\n",
       "\n",
       "Stoyanov. 2019. Roberta: A robustly opti-\n",
       "\n",
       "mized bert pretraining approach. arXiv preprint\n",
       "\n",
       "arXiv:1907.11692.\n",
       "\n",
       "George A Miller. 1995. Wordnet: a lexical\n",
       "\n",
       "database for english. Communications of the\n",
       "\n",
       "ACM, 38(11):39–41.\n",
       "\n",
       "Reiichiro Nakano, Jacob Hilton, Suchir Bal-\n",
       "\n",
       "aji, Jeff Wu, Long Ouyang, Christina Kim,\n",
       "\n",
       "Christopher Hesse, Shantanu Jain, Vineet\n",
       "\n",
       "Kosaraju, William Saunders, et al. 2021. We-\n",
       "\n",
       "bgpt: Browser-assisted question-answering\n",
       "\n",
       "with human feedback. arXiv preprint\n",
       "\n",
       "arXiv:2112.09332.\n",
       "\n",
       "J Neeraja, Vivek Gupta, and Vivek Srikumar.\n",
       "\n",
       "2021. Incorporating external knowledge to en-\n",
       "\n",
       "hance tabular reasoning. In Proceedings of the\n",
       "\n",
       "2021 Conference of the North American Chap-\n",
       "\n",
       "ter of the Association for Computational Lin-\n",
       "\n",
       "guistics: Human Language Technologies, pages\n",
       "\n",
       "2799–2809.\n",
       "\n",
       "Yixin Nie, Adina Williams, Emily Dinan, Mohit\n",
       "\n",
       "Bansal, Jason Weston, and Douwe Kiela. 2020.\n",
       "\n",
       "Adversarial nli: A new benchmark for natu-\n",
       "\n",
       "ral language understanding. In Proceedings of\n",
       "\n",
       "the 58th Annual Meeting of the Association for\n",
       "\n",
       "Computational Linguistics, pages 4885–4901.\n",
       "\n",
       "\n",
       "# Page 11\n",
       "\n",
       "Maxwell Nye, Anders Johan Andreassen, Guy\n",
       "\n",
       "Gur-Ari, Henryk Michalewski, Jacob Austin,\n",
       "\n",
       "David Bieber, David Dohan, Aitor Lewkowycz,\n",
       "\n",
       "Maarten Bosma, David Luan, et al. 2022. Show\n",
       "\n",
       "your work: Scratchpads for intermediate com-\n",
       "\n",
       "putation with language models. In Deep Learn-\n",
       "\n",
       "ing for Code Workshop.\n",
       "\n",
       "Maxwell Nye, Michael Tessler, Josh Tenenbaum,\n",
       "\n",
       "and Brenden M Lake. 2021. Improving coher-\n",
       "\n",
       "ence and consistency in neural sequence mod-\n",
       "\n",
       "els with dual-system, neuro-symbolic reason-\n",
       "\n",
       "ing. Advances in Neural Information Process-\n",
       "\n",
       "ing Systems, 34:25192–25204.\n",
       "\n",
       "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\n",
       "\n",
       "Carroll L Wainwright, Pamela Mishkin, Chong\n",
       "\n",
       "Zhang, Sandhini Agarwal, Katarina Slama,\n",
       "\n",
       "Alex Ray, et al. 2022. Training language mod-\n",
       "\n",
       "els to follow instructions with human feedback.\n",
       "\n",
       "arXiv preprint arXiv:2203.02155.\n",
       "\n",
       "Colin Raffel, Noam Shazeer, Adam Roberts,\n",
       "\n",
       "Katherine Lee, Sharan Narang, Michael\n",
       "\n",
       "Matena, Yanqi Zhou, Wei Li, and Peter J Liu.\n",
       "\n",
       "2020. Exploring the limits of transfer learning\n",
       "\n",
       "with a uniﬁed text-to-text transformer. Journal\n",
       "\n",
       "of Machine Learning Research, 21:1–67.\n",
       "\n",
       "Pranav Rajpurkar, Jian Zhang, Konstantin Lopy-\n",
       "\n",
       "rev, and Percy Liang. 2016. Squad: 100,000+\n",
       "\n",
       "questions for machine comprehension of text.\n",
       "\n",
       "In Proceedings of the 2016 Conference on Em-\n",
       "\n",
       "pirical Methods in Natural Language Process-\n",
       "\n",
       "ing, pages 2383–2392.\n",
       "\n",
       "Stephen Robertson, Hugo Zaragoza, et al. 2009.\n",
       "\n",
       "The probabilistic relevance framework: Bm25\n",
       "\n",
       "and beyond. Foundations and Trends® in In-\n",
       "\n",
       "formation Retrieval, 3(4):333–389.\n",
       "\n",
       "Kurt Shuster, Mojtaba Komeili, Leonard Adolphs,\n",
       "\n",
       "Stephen Roller, Arthur Szlam, and Jason We-\n",
       "\n",
       "ston. 2022. Language models that seek for\n",
       "\n",
       "knowledge: Modular search &amp; generation\n",
       "\n",
       "for dialogue and prompt completion. arXiv\n",
       "\n",
       "preprint arXiv:2203.13224.\n",
       "\n",
       "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu,\n",
       "\n",
       "and Tie-Yan Liu. 2020. Mpnet: Masked and\n",
       "\n",
       "permuted pre-training for language understand-\n",
       "\n",
       "ing. Advances in Neural Information Process-\n",
       "\n",
       "ing Systems, 33:16857–16867.\n",
       "\n",
       "Robyn Speer, Joshua Chin, and Catherine Havasi.\n",
       "\n",
       "2017. Conceptnet 5.5: An open multilingual\n",
       "\n",
       "graph of general knowledge. In Thirty-ﬁrst\n",
       "\n",
       "AAAI conference on artiﬁcial intelligence.\n",
       "\n",
       "Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav\n",
       "\n",
       "Goldberg, and Jonathan Berant. 2020. Leap-\n",
       "\n",
       "of-thought: Teaching pre-trained models to\n",
       "\n",
       "systematically reason over implicit knowledge.\n",
       "\n",
       "Advances in Neural Information Processing\n",
       "\n",
       "Systems, 33:20227–20237.\n",
       "\n",
       "Romal Thoppilan, Daniel De Freitas, Jamie Hall,\n",
       "\n",
       "Noam Shazeer, Apoorv Kulshreshtha, Heng-\n",
       "\n",
       "Tze Cheng, Alicia Jin, Taylor Bos, Leslie\n",
       "\n",
       "Baker, Yu Du, et al. 2022. Lamda: Language\n",
       "\n",
       "models for dialog applications. arXiv preprint\n",
       "\n",
       "arXiv:2201.08239.\n",
       "\n",
       "Yerram Varun, Aayush Sharma, and Vivek Gupta.\n",
       "\n",
       "2022. Trans-kblstm: An external knowledge\n",
       "\n",
       "enhanced transformer bilstm model for tabular\n",
       "\n",
       "reasoning. In Proceedings of Deep Learning In-\n",
       "\n",
       "side Out (DeeLIO 2022): The 3rd Workshop on\n",
       "\n",
       "Knowledge Extraction and Integration for Deep\n",
       "\n",
       "Learning Architectures, pages 62–78.\n",
       "\n",
       "Ashish Vaswani, Noam Shazeer, Niki Parmar,\n",
       "\n",
       "Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
       "\n",
       "Łukasz Kaiser, and Illia Polosukhin. 2017. At-\n",
       "\n",
       "tention is all you need. Advances in neural in-\n",
       "\n",
       "formation processing systems, 30.\n",
       "\n",
       "Denny Vrande ˇci´c and Markus Krötzsch. 2014.\n",
       "\n",
       "Wikidata: a free collaborative knowledgebase.\n",
       "\n",
       "Communications of the ACM, 57(10):78–85.\n",
       "\n",
       "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\n",
       "\n",
       "Le, Ed Chi, and Denny Zhou. 2022. Self-\n",
       "\n",
       "consistency improves chain of thought rea-\n",
       "\n",
       "soning in language models. arXiv preprint\n",
       "\n",
       "arXiv:2203.11171.\n",
       "\n",
       "Jason Wei, Xuezhi Wang, Dale Schuurmans,\n",
       "\n",
       "Maarten Bosma, Ed Chi, Quoc Le, and Denny\n",
       "\n",
       "Zhou. 2022. Chain of thought prompting elic-\n",
       "\n",
       "its reasoning in large language models. arXiv\n",
       "\n",
       "preprint arXiv:2201.11903.\n",
       "\n",
       "Thomas Wolf, Lysandre Debut, Victor Sanh,\n",
       "\n",
       "Julien Chaumond, Clement Delangue, Anthony\n",
       "\n",
       "Moi, Pierric Cistac, Tim Rault, Rémi Louf,\n",
       "\n",
       "Morgan Funtowicz, et al. 2020. Transformers:\n",
       "\n",
       "State-of-the-art natural language processing. In\n",
       "\n",
       "\n",
       "# Page 12\n",
       "\n",
       "Proceedings of the 2020 conference on empir-\n",
       "\n",
       "ical methods in natural language processing:\n",
       "\n",
       "system demonstrations, pages 38–45.\n",
       "\n",
       "Xi Ye and Greg Durrett. 2022. The unreliability\n",
       "\n",
       "of explanations in few-shot in-context learning.\n",
       "\n",
       "arXiv preprint arXiv:2205.03401.\n",
       "\n",
       "Eric Zelikman, Yuhuai Wu, and Noah D Good-\n",
       "\n",
       "man. 2022. Star: Bootstrapping reasoning with\n",
       "\n",
       "reasoning. arXiv preprint arXiv:2203.14465.\n",
       "\n",
       "Susan Zhang, Stephen Roller, Naman Goyal,\n",
       "\n",
       "Mikel Artetxe, Moya Chen, Shuohui Chen,\n",
       "\n",
       "Christopher Dewan, Mona Diab, Xian Li,\n",
       "\n",
       "Xi Victoria Lin, et al. 2022. Opt: Open pre-\n",
       "\n",
       "trained transformer language models. arXiv\n",
       "\n",
       "preprint arXiv:2205.01068.\n",
       "\n",
       "Denny Zhou, Nathanael Schärli, Le Hou, Ja-\n",
       "\n",
       "son Wei, Nathan Scales, Xuezhi Wang, Dale\n",
       "\n",
       "Schuurmans, Olivier Bousquet, Quoc Le, and\n",
       "\n",
       "Ed Chi. 2022. Least-to-most prompting enables\n",
       "\n",
       "complex reasoning in large language models.\n",
       "\n",
       "arXiv preprint arXiv:2205.10625.\n",
       "\n",
       "\n",
       "# Page 13\n",
       "\n",
       "A Appendix\n",
       "\n",
       "In this section, we provide additional details on\n",
       "\n",
       "our experimental setup. Further information can\n",
       "\n",
       "be found in our code.\n",
       "\n",
       "A.1 Detailed Prompts\n",
       "\n",
       "We adopt the same CoT prompt for commonsense\n",
       "\n",
       "reasoning (i.e., StrategyQA) as those presented in\n",
       "\n",
       "Wei et al. (2022). The CoT prompt for tempo-\n",
       "\n",
       "ral reasoning is provided in Table 6. For tabular\n",
       "\n",
       "reasoning, we adopt the method of Brown et al.\n",
       "\n",
       "(2020) for converting NLI into QA for RTE (Da-\n",
       "\n",
       "gan et al., 2005), and randomly sample6 examples\n",
       "\n",
       "from the training data to construct the prompt, as\n",
       "\n",
       "shown in Table 8. The few-shot prompt utilizes\n",
       "\n",
       "the same exemplars as the CoT prompt and does\n",
       "\n",
       "not involve CoT reasoning processes.\n",
       "\n",
       "A.2 Description of Faithfulness Functions\n",
       "\n",
       "For a sentence s, we denote its MPNet similarity,\n",
       "\n",
       "entailment score, and contradiction score asM(s),\n",
       "\n",
       "E(s), and C(s), respectively. In our experiments,\n",
       "\n",
       "the corresponding thresholds for these scores are\n",
       "\n",
       "Tm = 0.5, Te = 0.6, and Tc = 0.99. Given the\n",
       "\n",
       "entailment scores, contradiction scores, and MP-\n",
       "\n",
       "Net similarities of all supporting facts (denoted as\n",
       "\n",
       "S) in the explanation of a reasoning pathR, differ-\n",
       "\n",
       "ent faithfulness functionsfKB(·) can be adopted in\n",
       "\n",
       "different settings as follows:\n",
       "\n",
       "(1) fKB(R) =∑\n",
       "\n",
       "s∈S[M(s)×(M(s) &gt;= Tm)+\n",
       "\n",
       "E(s) ×(M(s) &lt; Tm) −C(s)]\n",
       "\n",
       "(2) fKB(R) =∑\n",
       "\n",
       "s∈S[M(s) +E(s)]\n",
       "\n",
       "(3) fKB(R) = ∑\n",
       "\n",
       "s∈S[E(s) ×(E(s) &gt;= Te) −\n",
       "\n",
       "C(s) ×(C(s) &gt;= Tc)]\n",
       "\n",
       "In Section 4, we employ function (1) for com-\n",
       "\n",
       "monsense and tabular reasoning. For temporal rea-\n",
       "\n",
       "soning, we use function (2) as the distinct nature of\n",
       "\n",
       "sentences converted from temporal relations leads\n",
       "\n",
       "to unreliable contradiction scores. In Sections 5.3-\n",
       "\n",
       "5.4, we use function (3) for commonsense reason-\n",
       "\n",
       "ing with evidence paragraphs, as the high quality\n",
       "\n",
       "of the relevant knowledge negates the need for the\n",
       "\n",
       "complementary use of the MPNet similarity to im-\n",
       "\n",
       "prove the entailment score.\n",
       "\n",
       "A.3 Comparison of Retrieval Systems\n",
       "\n",
       "For commonsense reasoning, we utilized different\n",
       "\n",
       "retrieval systems in Karpukhin et al. (2020) to re-\n",
       "\n",
       "trieve relevant paragraphs from Wikipedia. The\n",
       "\n",
       "performance of BM25, DPR, and BM25+DPR\n",
       "\n",
       "were 77.73%, 58.52%, and 77.29%, respectively,\n",
       "\n",
       "indicating that BM25 is the best choice in our case.\n",
       "\n",
       "A.4 Implementation Details for the Two\n",
       "\n",
       "Variants of RR\n",
       "\n",
       "Fact selection implementation details. In this\n",
       "\n",
       "work, we utilize the information present in the top-\n",
       "\n",
       "ranked output produced by our basic approach as\n",
       "\n",
       "a guide. To this end, we apply a greedy clustering\n",
       "\n",
       "algorithm to group the sentences from all outputs\n",
       "\n",
       "into distinct topic categories based on the cosine\n",
       "\n",
       "similarity of their MPNet sentence embeddings.\n",
       "\n",
       "For each fact in the top-ranked output of our ba-\n",
       "\n",
       "sic approach, we identify the fact with the highest\n",
       "\n",
       "faithfulness within the same topic group and re-\n",
       "\n",
       "place it in the output. The faithfulness of a fact is\n",
       "\n",
       "calculated using the fKBfunction by replacing the\n",
       "\n",
       "supporting facts with a single fact.\n",
       "\n",
       "Fact generation implementation details. In\n",
       "\n",
       "this part, we generate questions for the named en-\n",
       "\n",
       "tities present in each fact of the top-ranked output\n",
       "\n",
       "produced by our basic approach, and retrieve the\n",
       "\n",
       "corresponding answers from the evidence para-\n",
       "\n",
       "graphs using UniﬁedQA. We employ the ques-\n",
       "\n",
       "tion generation model described in Deutsch et al.\n",
       "\n",
       "(2021), which has been shown to be more ex-\n",
       "\n",
       "tractive compared to other models as demon-\n",
       "\n",
       "strated in Fabbri et al. (2021). We adopt the\n",
       "\n",
       "question ﬁltering approach proposed in Honovich\n",
       "\n",
       "et al. (2021) using an off-the-shelf extractive\n",
       "\n",
       "QA model (ktrapeznikov/albert-xlarge-v2-squad-\n",
       "\n",
       "v2 from Hugging Face (Wolf et al., 2020)). We\n",
       "\n",
       "then use an off-the-shelf model (MarkS/bart-base-\n",
       "\n",
       "qa2d from Hugging Face) to convert the generated\n",
       "\n",
       "QA pairs into declarative sentences. We apply\n",
       "\n",
       "simple rules based on the entailment and contra-\n",
       "\n",
       "diction scores of the selected facts from the fact se-\n",
       "\n",
       "lection variant and the generated declarative sen-\n",
       "\n",
       "tences to obtain the ﬁnal generated facts.\n",
       "\n",
       "A.5 Comparison of Different Inference\n",
       "\n",
       "Methods with Supporting Facts\n",
       "\n",
       "In our experiments, we utilize UniﬁedQA for the\n",
       "\n",
       "ﬁnal step of inference in both variants. However,\n",
       "\n",
       "it is worth noting that GPT-3 could also be used\n",
       "\n",
       "for this purpose. As shown in Table 7, we observe\n",
       "\n",
       "that UniﬁedQA performs better at inference with\n",
       "\n",
       "generated facts, while GPT-3 with CoT prompt-\n",
       "\n",
       "ing performs better with empty or gold facts. This\n",
       "\n",
       "suggests that UniﬁedQA is more robust to noisy\n",
       "\n",
       "\n",
       "# Page 14\n",
       "\n",
       "Q: who was governor of minnesota when maathaad maathaadu mallige was released?\n",
       "\n",
       "A: Maathaad Maathaadu Mallige was released on 24 August 2007. Tim Pawlenty served as the 39th gov-\n",
       "\n",
       "ernor of Minnesota from 2003 to 2011. Thus, Tim Pawlenty was governor of minnesota when maathaad\n",
       "\n",
       "maathaadu mallige was released. So the answer is Tim Pawlenty.\n",
       "\n",
       "Q: who was us president during the costa rican civil war?\n",
       "\n",
       "A: The Costa Rican civil war was a civil war in Costa Rica from 12 March to 24 April 1948. Harry S.\n",
       "\n",
       "Truman was the 33rd president of the United States, serving from 1945 to 1953. Thus, Harry S. Truman\n",
       "\n",
       "was us president during the costa rican civil war. So the answer is Harry S. Truman.\n",
       "\n",
       "Q: who was governor of oregon when the collector was released?\n",
       "\n",
       "A: The Collector premiered at the Cannes Film Festival on May 20, 1965. Mark Hatﬁeld served as the\n",
       "\n",
       "29th governor of Oregon from 1959 to 1967. Thus, Mark Hatﬁeld was governor of oregon when the\n",
       "\n",
       "collector was released. So the answer is Mark Hatﬁeld.\n",
       "\n",
       "Q: who was governor of oregon when shanghai noon was released?\n",
       "\n",
       "A: Shanghai Noon was released on May 26, 2000. John Kitzhaber served as the 35th governor of Oregon\n",
       "\n",
       "from 1995 to 2003. Thus, John Kitzhaber was governor of oregon when shanghai noon was released. So\n",
       "\n",
       "the answer is John Kitzhaber.\n",
       "\n",
       "Q: who was us president when john andrew shulze was a teenager?\n",
       "\n",
       "A: John Andrew Shulze was born on July 19, 1775. A teenager is someone who is between 13 and 19\n",
       "\n",
       "years old. George Washington served as the ﬁrst president of the United States from 1789 to 1797. Thus,\n",
       "\n",
       "George Washington was us president when john andrew shulze was a teenager.So the answer is George\n",
       "\n",
       "Washington.\n",
       "\n",
       "Q: who was us president during the seventh coalition?\n",
       "\n",
       "A: The War of the Seventh Coalition was from 20 March to 8 July 1815. James Madison served as the\n",
       "\n",
       "fourth president of the United States from 1809 to 1817. Thus, James Madison was us president during\n",
       "\n",
       "the seventh coalition. So the answer is James Madison.\n",
       "\n",
       "Table 6: The CoT prompt for temporal reasoning.\n",
       "\n",
       "Methods Accuracy (%)\n",
       "\n",
       "Empty facts\n",
       "\n",
       "GPT-3 (zero-shot) 58.08\n",
       "\n",
       "GPT-3 (CoT) 65.94\n",
       "\n",
       "UniﬁedQA 58.95\n",
       "\n",
       "Gold facts\n",
       "\n",
       "GPT-3 (zero-shot) 81.66\n",
       "\n",
       "GPT-3 (CoT) 91.70\n",
       "\n",
       "UniﬁedQA 90.83\n",
       "\n",
       "Generated facts\n",
       "\n",
       "GPT-3 (zero-shot) 69.87\n",
       "\n",
       "GPT-3 (CoT) 76.42\n",
       "\n",
       "UniﬁedQA 78.60\n",
       "\n",
       "Table 7: Comparison of different inference methods on\n",
       "\n",
       "empty, gold, and generated facts.\n",
       "\n",
       "inputs compared to GPT-3. Additionally, both\n",
       "\n",
       "UniﬁedQA and GPT-3 with CoT prompting signif-\n",
       "\n",
       "icantly outperform GPT-3 with zero-shot prompt-\n",
       "\n",
       "ing, indicating that the CoT prompting is also ben-\n",
       "\n",
       "eﬁcial for the ﬁnal step of inference.\n",
       "\n",
       "\n",
       "# Page 15\n",
       "\n",
       "Charles Sumner Tainter was Born on April 25, 1854 ( 1854-04-25 ) Watertown, Massachusetts, U.S..\n",
       "\n",
       "Charles Sumner Tainter was Died on April 20, 1940 ( 1940-04-21 ) (aged 85) San Diego, California,\n",
       "\n",
       "U.S.. The Nationality of Charles Sumner Tainter are American. The Known for of Charles Sumner\n",
       "\n",
       "Tainter are Photophone, phonograph Father Of The Speaking Machine.\n",
       "\n",
       "Question: Charles Sumner Tainter never left the state of Massachusetts. True or False?\n",
       "\n",
       "Answer: Charles Sumner Tainter was died in San Diego, California, U.S.. California is a state. Thus,\n",
       "\n",
       "Charles Sumner Tainter has left the state of Massachusetts. So the answer is false.\n",
       "\n",
       "The Region of Curitiba are South. The Elevation of Curitiba are 934.6 m (3,066.3 ft). The Density of\n",
       "\n",
       "Curitiba are 4,062/km 2 (10,523/sq mi). The Metro density of Curitiba are 210.9/km 2 (546.2/sq mi).\n",
       "\n",
       "Question: Curitiba is above sea level. True or False?\n",
       "\n",
       "Answer: The elevation of Curitiba are 934.6 m (3,066.3 ft). Elevation is a hypernym of level. Thus,\n",
       "\n",
       "Curitiba is above sea level. So the answer is true.\n",
       "\n",
       "Charles (Prince of Wales) was Born on 14 November 1948 ( 1948-11-14 ) (age 70) Buckingham Palace,\n",
       "\n",
       "London, England. The Spouse of Charles (Prince of Wales) are Lady Diana Spencer ( m. 1981 ; div.\n",
       "\n",
       "1996 ) , and Camilla Parker Bowles ( m. 2005 ). The Issue of Charles (Prince of Wales) are Prince\n",
       "\n",
       "William, Duke of Cambridge , and Prince Harry, Duke of Sussex.\n",
       "\n",
       "Question: Charles was born in 1948 and has been married twice. True or False?\n",
       "\n",
       "Answer: Charles (Prince of Wales) was Born on 14 November 1948. The Spouse of Charles (Prince of\n",
       "\n",
       "Wales) are Lady Diana Spencer ( m. 1981 ; div. 1996 ) , and Camilla Parker Bowles ( m. 2005 ). Married\n",
       "\n",
       "is related to spouse. Thus, Charles was born in 1948 and has been married twice. So the answer is true.\n",
       "\n",
       "The Born of Idris Elba are 6 September 1972 (age 46) Hackney, London, England. The Residence of\n",
       "\n",
       "Idris Elba are London. The Other names of Idris Elba are DJ Big Driis, Big Driis the Londoner, Big\n",
       "\n",
       "Driis, and 7 Dub. The Occupation of Idris Elba are Actor, producer, director, musician, and DJ.\n",
       "\n",
       "Question: Idris Elba is an English entertainer. True or False?\n",
       "\n",
       "Answer: The residence of Idris Elba is London. English is related to London. The occupation of Idris\n",
       "\n",
       "Elba are actor, producer, director, musician, and DJ. Actor is a hyponym of entertainer. Musician is a\n",
       "\n",
       "hyponym of entertainer. DJ is an entertainer. Thus, Idris Elba is an English entertainer. So the answer\n",
       "\n",
       "is true.\n",
       "\n",
       "The Breed of Jean, the Vitagraph Dog are Scotch Collie. The Sex of Jean, the Vitagraph Dog are Female.\n",
       "\n",
       "The Born of Jean, the Vitagraph Dog are 1902 Eastport, Maine. The Years active of Jean, the Vitagraph\n",
       "\n",
       "Dog are 1909 - 1916.\n",
       "\n",
       "Question: Jean, the Vitagraph Dog was a Golden Retriever which perform in circus. True or False?\n",
       "\n",
       "Answer: The Breed of Jean, the Vitagraph Dog are Scotch Collie. Collie is a hyponym of dog. Retriever\n",
       "\n",
       "is a hyponym of dog. Thus, Jean, the Vitagraph Dog was not a Golden Retriever which perform in circus.\n",
       "\n",
       "So the answer is false.\n",
       "\n",
       "The Studio of Hydrograd are Sphere Studios, North Hollywood, Los Angeles. The Genre of Hydrograd\n",
       "\n",
       "are Hard rock. The Label of Hydrograd are Roadrunner. The Producer of Hydrograd are Jay Ruston.\n",
       "\n",
       "Question: Hydrograd is in the rap genre. True or False?\n",
       "\n",
       "Answer: The Genre of Hydrograd are Hard rock. Rap is distinct from rock. Thus, Hydrograd is not in\n",
       "\n",
       "the rap genre. So the answer is false.\n",
       "\n",
       "Table 8: The CoT prompt for tabular reasoning.\n",
       "\n",
       "\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "\n",
       "# Page 1\n",
       "\n",
       "Rethinking with Retrieval: Faithful Large Language Model Inference\n",
       "\n",
       "Hangfeng He†∗ Hongming Zhang‡ Dan Roth§\n",
       "\n",
       "†University of Rochester ‡Tencent AI Lab, Seattle §University of Pennsylvania\n",
       "\n",
       "hanfeng.he@rochester.edu, hongmzhang@global.tencent.com\n",
       "\n",
       "danroth@seas.upenn.edu\n",
       "\n",
       "Abstract\n",
       "\n",
       "Despite the success of large language mod-\n",
       "\n",
       "els (LLMs) in various natural language pro-\n",
       "\n",
       "cessing (NLP) tasks, the stored knowledge\n",
       "\n",
       "in these models may inevitably be incom-\n",
       "\n",
       "plete, out-of-date, or incorrect. This mo-\n",
       "\n",
       "tivates the need to utilize external knowl-\n",
       "\n",
       "edge to assist LLMs. Unfortunately, current\n",
       "\n",
       "methods for incorporating external knowl-\n",
       "\n",
       "edge often require additional training or\n",
       "\n",
       "ﬁne-tuning, which can be costly and may\n",
       "\n",
       "not be feasible for LLMs. To address this\n",
       "\n",
       "issue, we propose a novel post-processing\n",
       "\n",
       "approach, rethinking with retrieval (RR),\n",
       "\n",
       "which retrieves relevant external knowledge\n",
       "\n",
       "based on the decomposed reasoning steps\n",
       "\n",
       "obtained from the chain-of-thought (CoT)\n",
       "\n",
       "prompting. This lightweight approach does\n",
       "\n",
       "not require additional training or ﬁne-tuning\n",
       "\n",
       "and is not limited by the input length of\n",
       "\n",
       "LLMs. We evaluate the effectiveness of RR\n",
       "\n",
       "through extensive experiments with GPT-3\n",
       "\n",
       "on three complex reasoning tasks: common-\n",
       "\n",
       "sense reasoning, temporal reasoning, and\n",
       "\n",
       "tabular reasoning. Our results show that RR\n",
       "\n",
       "can produce more faithful explanations and\n",
       "\n",
       "improve the performance of LLMs.1\n",
       "\n",
       "1 Introduction\n",
       "\n",
       "Large language models (LLMs) have shown\n",
       "\n",
       "exceptional performance across various tasks\n",
       "\n",
       "through in-context learning without task-speciﬁc\n",
       "\n",
       "training or ﬁne-tuning (Brown et al., 2020;\n",
       "\n",
       "Chowdhery et al., 2022; Zhang et al., 2022;\n",
       "\n",
       "Ouyang et al., 2022). Recent progress in prompt-\n",
       "\n",
       "ing (Wei et al., 2022; Zhou et al., 2022; Kojima\n",
       "\n",
       "et al., 2022) and decoding (Wang et al., 2022) has\n",
       "\n",
       "made it feasible for LLMs to tackle tasks that de-\n",
       "\n",
       "mand complex reasoning.\n",
       "\n",
       "∗Part of this work was done while the author was at the\n",
       "\n",
       "University of Pennsylvania.\n",
       "\n",
       "1Our code is publicly available at https://github.\n",
       "\n",
       "com/HornHehhf/RR.\n",
       "\n",
       "Query PredictionLLM\n",
       "\n",
       "Query Explanation + PredictionLLM\n",
       "\n",
       "Query Explanation + PredictionLLM\n",
       "\n",
       "(a)\n",
       "\n",
       "(b)\n",
       "\n",
       "(c)\n",
       "\n",
       "Knowledge\n",
       "\n",
       "Chain of thought\n",
       "\n",
       "Chain of thoughtRetrievalRethinking\n",
       "\n",
       "Figure 1: An overview of three approaches for using\n",
       "\n",
       "LLMs: (a) Standard prompting for generating a pre-\n",
       "\n",
       "diction in response to a query. (b) Chain-of-thought\n",
       "\n",
       "prompting for generating both an explanation and a\n",
       "\n",
       "prediction in response to a query. (c) Rethinking with\n",
       "\n",
       "retrieval, our proposed approach for using the decom-\n",
       "\n",
       "posed reasoning steps obtained from chain-of-thought\n",
       "\n",
       "prompting to retrieve relevant external knowledge for\n",
       "\n",
       "LLMs, leading to more faithful explanations and im-\n",
       "\n",
       "proved predictions in response to a query.\n",
       "\n",
       "However, the knowledge stored in LLMs might\n",
       "\n",
       "inevitably be incomplete, out-of-date, or incorrect.\n",
       "\n",
       "As a result, external sources of knowledge, such\n",
       "\n",
       "as Wikipedia, may be essential for the success-\n",
       "\n",
       "ful deployment of LLMs for real-world applica-\n",
       "\n",
       "tions. Previously, people tried to utilize knowl-\n",
       "\n",
       "edge for smaller language models (LMs), such\n",
       "\n",
       "as T5 (Raffel et al., 2020), BERT (Devlin et al.,\n",
       "\n",
       "2019), and RoBERTa (Liu et al., 2019). However,\n",
       "\n",
       "these methods often require additional training or\n",
       "\n",
       "ﬁne-tuning, which can be costly and thus imprac-\n",
       "\n",
       "tical for LLMs.\n",
       "\n",
       "In this paper, we present a post-processing\n",
       "\n",
       "approach called rethinking with retrieval (RR)\n",
       "\n",
       "for utilizing external knowledge in LLMs. Our\n",
       "\n",
       "method begins by using the chain-of-thought\n",
       "\n",
       "(CoT) prompting method (Wei et al., 2022) to gen-\n",
       "\n",
       "erate a diverse set of reasoning paths, as described\n",
       "\n",
       "in Wang et al. (2022). We then use each rea-\n",
       "\n",
       "soning step in those paths to retrieve relevant ex-\n",
       "\n",
       "ternal knowledge, which enables RR to provide\n",
       "\n",
       "arXiv:2301.00303v1  [cs.CL]  31 Dec 2022\n",
       "\n",
       "\n",
       "# Page 2\n",
       "\n",
       "more faithful explanations and more accurate pre-\n",
       "\n",
       "dictions, as illustrated in Figure 1.\n",
       "\n",
       "We evaluate the effectiveness of our proposed\n",
       "\n",
       "method, RR, on three complex reasoning tasks:\n",
       "\n",
       "commonsense reasoning, temporal reasoning, and\n",
       "\n",
       "tabular reasoning, using GPT-3 175B (Brown\n",
       "\n",
       "et al., 2020) and different external knowledge\n",
       "\n",
       "sources: Wikipedia, Wikidata (Vrande ˇci´c and\n",
       "\n",
       "Krötzsch, 2014), WordNet (Miller, 1995), and\n",
       "\n",
       "Conceptnet (Speer et al., 2017). The results\n",
       "\n",
       "demonstrate that RR consistently outperforms all\n",
       "\n",
       "baselines on all three tasks without requiring ad-\n",
       "\n",
       "ditional training or ﬁne-tuning, indicating the su-\n",
       "\n",
       "periority of our approach in leveraging external\n",
       "\n",
       "knowledge to enhance the performance of LLMs.\n",
       "\n",
       "2 Related Work\n",
       "\n",
       "Enhancing LMs through retrieval. Retrieval-\n",
       "\n",
       "enhanced LMs have received signiﬁcant attention\n",
       "\n",
       "as a means of improving performance through the\n",
       "\n",
       "incorporation of external knowledge. For exam-\n",
       "\n",
       "ple, the k-most similar training contexts can be re-\n",
       "\n",
       "trieved to improve the estimation of the next word\n",
       "\n",
       "distribution in both the training stage (Borgeaud\n",
       "\n",
       "et al., 2021) and the inference stage (Khandelwal\n",
       "\n",
       "et al., 2020). Furthermore, search query genera-\n",
       "\n",
       "tors have been adopted to generate search queries\n",
       "\n",
       "for search engines to retrieve relevant documents\n",
       "\n",
       "(Komeili et al., 2022; Shuster et al., 2022; Thop-\n",
       "\n",
       "pilan et al., 2022). Other approaches have uti-\n",
       "\n",
       "lized retrieved documents as the additional con-\n",
       "\n",
       "text in generation tasks (Joshi et al., 2020; Guu\n",
       "\n",
       "et al., 2020; Lewis et al., 2020). Nakano et al.\n",
       "\n",
       "(2021) instead use human feedback in a text-based\n",
       "\n",
       "web-browsing environment. Among these pre-\n",
       "\n",
       "vious works, Khandelwal et al. (2020) is most\n",
       "\n",
       "closely related to our approach. However, they\n",
       "\n",
       "focus on improving local inference by using the\n",
       "\n",
       "nearest neighbor datastore constructed from train-\n",
       "\n",
       "ing data, whereas we focus on conducting faith-\n",
       "\n",
       "ful inference using external knowledge. In con-\n",
       "\n",
       "trast to other aforementioned approaches, which\n",
       "\n",
       "require training or ﬁne-tuning to incorporate re-\n",
       "\n",
       "trieved knowledge, we propose a post-processing\n",
       "\n",
       "method for leveraging retrieved knowledge with-\n",
       "\n",
       "out additional training or ﬁne-tuning.\n",
       "\n",
       "Incorporating external knowledge into LMs.\n",
       "\n",
       "Signiﬁcant effort has been devoted to leveraging\n",
       "\n",
       "external knowledge to improve the reasoning abil-\n",
       "\n",
       "ity of LMs. Previous work has incorporated exter-\n",
       "\n",
       "nal knowledge sources such as WordNet (Miller,\n",
       "\n",
       "1995) and ConceptNet (Speer et al., 2017) to en-\n",
       "\n",
       "hance LMs for tabular reasoning tasks (Neeraja\n",
       "\n",
       "et al., 2021; Varun et al., 2022). Explicit rules\n",
       "\n",
       "have also been added to inputs to improve rea-\n",
       "\n",
       "soning ability over implicit knowledge (Talmor\n",
       "\n",
       "et al., 2020). In addition, explicit knowledge from\n",
       "\n",
       "Wikidata (Vrandeˇci´c and Krötzsch, 2014) and im-\n",
       "\n",
       "plicit knowledge in LLMs have been integrated\n",
       "\n",
       "into a transformer (Vaswani et al., 2017) for vi-\n",
       "\n",
       "sual question answering (Gui et al., 2021). Nye\n",
       "\n",
       "et al. (2021) instead introduces a symbolic reason-\n",
       "\n",
       "ing module to improve coherence and consistency\n",
       "\n",
       "in LLMs. Among these previous works, Nye et al.\n",
       "\n",
       "(2021) is the most relevant to our approach. Still,\n",
       "\n",
       "they focus on incorporating logical constraints to\n",
       "\n",
       "improve coherence and consistency, whereas we\n",
       "\n",
       "aim to improve the faithfulness of explanations\n",
       "\n",
       "through the use of external knowledge. In con-\n",
       "\n",
       "trast to other aforementioned approaches that in-\n",
       "\n",
       "corporate external knowledge before generation\n",
       "\n",
       "and require additional training or ﬁne-tuning, our\n",
       "\n",
       "proposal leverages external knowledge in a post-\n",
       "\n",
       "processing manner to enhance LMs without addi-\n",
       "\n",
       "tional training or ﬁne-tuning.\n",
       "\n",
       "Uncovering latent Knowledge in LLMs. There\n",
       "\n",
       "has been a line of work exploring the knowledge\n",
       "\n",
       "hidden within LLMs for reasoning. This has in-\n",
       "\n",
       "cluded the use of careful prompting to encourage\n",
       "\n",
       "LLMs to generate explanations in the reasoning\n",
       "\n",
       "process, such as through chain of thought prompt-\n",
       "\n",
       "ing in few-shot (Wei et al., 2022) or zero-shot\n",
       "\n",
       "(Kojima et al., 2022) learning, or through the use\n",
       "\n",
       "of scratchpads for intermediate computation (Nye\n",
       "\n",
       "et al., 2022). In addition, various methods based\n",
       "\n",
       "on sampling a diverse set of reasoning paths in\n",
       "\n",
       "LLMs have been proposed, including training ver-\n",
       "\n",
       "iﬁers to judge the correctness of model comple-\n",
       "\n",
       "tions (Cobbe et al., 2021), calibrating model pre-\n",
       "\n",
       "dictions based on the reliability of the explana-\n",
       "\n",
       "tions (Ye and Durrett, 2022), and promoting self-\n",
       "\n",
       "consistency over diverse reasoning paths (Wang\n",
       "\n",
       "et al., 2022). Zelikman et al. (2022) instead it-\n",
       "\n",
       "eratively bootstrap the ability of LLMs to gener-\n",
       "\n",
       "ate high-quality rationales from a few initial ex-\n",
       "\n",
       "amples. Liu et al. (2022) further propose generat-\n",
       "\n",
       "ing knowledge from LLMs, which is then used as\n",
       "\n",
       "additional input to improve commonsense reason-\n",
       "\n",
       "ing. In contrast to this line of work, our proposal\n",
       "\n",
       "focuses on leveraging external knowledge to en-\n",
       "\n",
       "hance LLMs, while they aim to explore the knowl-\n",
       "\n",
       "edge hidden within LLMs.\n",
       "\n",
       "\n",
       "# Page 3\n",
       "\n",
       "3 Rethinking with Retrieval\n",
       "\n",
       "LLMs have been shown to generate incorrect sup-\n",
       "\n",
       "porting facts from time to time, even when they ac-\n",
       "\n",
       "curately capture the perspective needed to answer\n",
       "\n",
       "a question. This phenomenon highlights intrinsic\n",
       "\n",
       "issues in the way LLMs store and retrieve knowl-\n",
       "\n",
       "edge, including (1) the presence of out-of-date,\n",
       "\n",
       "incorrect, or missing relevant knowledge in the\n",
       "\n",
       "pre-training corpus; (2) incorrect memorization of\n",
       "\n",
       "relevant knowledge during pre-training; and (3)\n",
       "\n",
       "incorrect retrieval of relevant knowledge during\n",
       "\n",
       "the inference stage. To address these issues, we\n",
       "\n",
       "propose the use of RR, which leverages external\n",
       "\n",
       "knowledge through the retrieval of relevant infor-\n",
       "\n",
       "mation based on decomposed reasoning steps.\n",
       "\n",
       "Overview. Given a queryQ, we utilize chain-of-\n",
       "\n",
       "thought prompting to generate a diverse set of rea-\n",
       "\n",
       "soning paths R1, R2, ···RN , where each reason-\n",
       "\n",
       "ing path Ri consists of an explanationEi followed\n",
       "\n",
       "by a prediction Pi. After that, we retrieve relevant\n",
       "\n",
       "knowledge K1, ···KM from a suitable knowledge\n",
       "\n",
       "base KBto support the explanation in each reason-\n",
       "\n",
       "ing path, and select the prediction ˆP that is most\n",
       "\n",
       "faithful to this knowledge. To better illustrate our\n",
       "\n",
       "proposal, we use “Did Aristotle use a laptop?” as\n",
       "\n",
       "a running example in this work.\n",
       "\n",
       "Chain-of-thought prompting. In contrast to\n",
       "\n",
       "standard prompting, CoT prompting (Wei et al.,\n",
       "\n",
       "2022) includes demonstrations of step-by-step rea-\n",
       "\n",
       "soning examples in the prompt to produce a series\n",
       "\n",
       "of short sentences that capture the reasoning pro-\n",
       "\n",
       "cess. For instance, given the question “ Did Aris-\n",
       "\n",
       "totle use a laptop?”, CoT prompting aims to gen-\n",
       "\n",
       "erate the complete reasoning path “Aristotle died\n",
       "\n",
       "in 322 BC. The ﬁrst laptop was invented in 1980.\n",
       "\n",
       "Thus, Aristotle did not use a laptop. So the answer\n",
       "\n",
       "is no.” rather than simply outputs “No.” Empirical\n",
       "\n",
       "results show that CoT prompting signiﬁcantly im-\n",
       "\n",
       "proves the performance of LLMs on many multi-\n",
       "\n",
       "step reasoning tasks. Therefore, we adopt CoT\n",
       "\n",
       "prompting to obtain both explanation E and pre-\n",
       "\n",
       "diction P for the query Q.\n",
       "\n",
       "Sampling diverse reasoning paths. Similar to\n",
       "\n",
       "Wang et al. (2022), we sample a diverse set of rea-\n",
       "\n",
       "soning paths R1, R2, ···RN rather than only con-\n",
       "\n",
       "sidering the greedy path as in Wei et al. (2022).\n",
       "\n",
       "For the question “Did Aristotle use a laptop?”, the\n",
       "\n",
       "potential reasoning paths can be as follows:\n",
       "\n",
       "(R1) Aristotle died in 2000. The ﬁrst laptop was\n",
       "\n",
       "invented in 1980. Thus, Aristotle used a lap-\n",
       "\n",
       "top. So the answer is yes.\n",
       "\n",
       "(R2) Aristotle died in 322BC. The ﬁrst laptop was\n",
       "\n",
       "invented in 2000. Thus, Aristotle did not use\n",
       "\n",
       "a laptop. So the answer is no.\n",
       "\n",
       "(R3) Aristotle died in 322BC. The ﬁrst laptop was\n",
       "\n",
       "invented in 1980. Thus, Aristotle did not use\n",
       "\n",
       "a laptop. So the answer is no.\n",
       "\n",
       "Knowledge retrieval. Different knowledge\n",
       "\n",
       "bases can be used to address different tasks. For\n",
       "\n",
       "example, to address the question “ Did Aristotle\n",
       "\n",
       "use a laptop? ”, we can use Wikipedia as the ex-\n",
       "\n",
       "ternal knowledge base KB. Information retrieval\n",
       "\n",
       "techniques can be applied to retrieve the relevant\n",
       "\n",
       "knowledge K1, ···KM from Wikipedia based\n",
       "\n",
       "on the decomposed reasoning steps. Ideally, we\n",
       "\n",
       "would obtain the following two paragraphs from\n",
       "\n",
       "Wikipedia for this question:\n",
       "\n",
       "(K1) Aristotle (384–322 BC) was a Greek philoso-\n",
       "\n",
       "pher and polymath during the Classical pe-\n",
       "\n",
       "riod in Ancient Greece. ...\n",
       "\n",
       "(K2) The Epson HX-20, the ﬁrst laptop computer,\n",
       "\n",
       "was invented in 1980. ...\n",
       "\n",
       "Faithful inference. The faithfulness of each rea-\n",
       "\n",
       "soning path Ri can be estimated using a function\n",
       "\n",
       "fKB(Ri), which is based on relevant knowledge\n",
       "\n",
       "K1, ··· , KM retrieved from the knowledge base\n",
       "\n",
       "KB. The ﬁnal prediction is obtained through the\n",
       "\n",
       "application of the following inference procedure2:\n",
       "\n",
       "ˆP = arg max\n",
       "\n",
       "Pi∈{P1,···,PN }\n",
       "\n",
       "N∑\n",
       "\n",
       "i=1\n",
       "\n",
       "1 (Pi = P)fKB(Ri), (1)\n",
       "\n",
       "where Pi denotes the corresponding prediction in\n",
       "\n",
       "the reasoning path Ri. This inference procedure\n",
       "\n",
       "is designed to identify the most faithful prediction\n",
       "\n",
       "ˆP to the knowledge base among all predictions in\n",
       "\n",
       "the N reasoning paths. For instance, in the run-\n",
       "\n",
       "ning example, given reasoning paths R1, R2, R3\n",
       "\n",
       "and the retrieved knowledgeK1, K2, the above in-\n",
       "\n",
       "ference procedure would output the prediction “So\n",
       "\n",
       "the answer is no.”, as it is supported by both R2\n",
       "\n",
       "and R3 and has a higher faithfulness score com-\n",
       "\n",
       "pared to the prediction “So the answer is yes.”,\n",
       "\n",
       "which is only supported by R1.\n",
       "\n",
       "2Note that this is the basic version of faithful inference,\n",
       "\n",
       "and further variations can be found in Section 5.3.\n",
       "\n",
       "\n",
       "# Page 4\n",
       "\n",
       "4 Experiments\n",
       "\n",
       "In this section, we present the evaluation of our\n",
       "\n",
       "proposed method, RR, on three complex reason-\n",
       "\n",
       "ing tasks: commonsense reasoning, temporal rea-\n",
       "\n",
       "soning, and tabular reasoning.\n",
       "\n",
       "4.1 Baselines\n",
       "\n",
       "We compare with the following baselines.\n",
       "\n",
       "Zero-shot/few-shot prompting. In our experi-\n",
       "\n",
       "ments, we consider GPT-3 with standard zero-\n",
       "\n",
       "shot/few-shot prompting as baselines, following\n",
       "\n",
       "the approach described in Brown et al. (2020), in\n",
       "\n",
       "which zero or few in-context exemplars of input-\n",
       "\n",
       "output pairs are provided in the prompt.\n",
       "\n",
       "Chain-of-thought prompting. In addition to\n",
       "\n",
       "the standard zero-shot/few-shot prompting, we\n",
       "\n",
       "also consider GPT-3 with the CoT prompting pro-\n",
       "\n",
       "posed in (Wei et al., 2022) as a baseline in our ex-\n",
       "\n",
       "periments. This approach involves feeding LLMs\n",
       "\n",
       "step-by-step reasoning examples instead of stan-\n",
       "\n",
       "dard input-output examples.\n",
       "\n",
       "Self-consistency. In addition, we also consider\n",
       "\n",
       "self-consistency (Wang et al., 2022) as a baseline\n",
       "\n",
       "in our experiments. This approach, proposed as an\n",
       "\n",
       "alternative to the naive greedy decoding used in\n",
       "\n",
       "CoT prompting (Wei et al., 2022), involves sam-\n",
       "\n",
       "pling a diverse set of reasoning paths and select-\n",
       "\n",
       "ing the most consistent answer by marginalizing\n",
       "\n",
       "the sampled paths.\n",
       "\n",
       "4.2 Commonsense Reasoning\n",
       "\n",
       "Dataset description. For commonsense reason-\n",
       "\n",
       "ing, we consider the StrategyQA dataset (Geva\n",
       "\n",
       "et al., 2021), which includes questions that require\n",
       "\n",
       "implicit reasoning strategies. For example, the\n",
       "\n",
       "question “Did Aristotle use a laptop? ” requires\n",
       "\n",
       "implicit decomposition into reasoning steps, while\n",
       "\n",
       "the question “Was Aristotle alive when the laptop\n",
       "\n",
       "was invented?” explicitly speciﬁes the reasoning\n",
       "\n",
       "process. The StrategyQA dataset includes 2, 290\n",
       "\n",
       "training examples, each consisting of a question\n",
       "\n",
       "(Q), a yes/no answer (A), a decomposition (D),\n",
       "\n",
       "evidence paragraphs (E), and supporting facts (F).\n",
       "\n",
       "On average, each question requires about2.93 rea-\n",
       "\n",
       "soning steps and 2.33 evidence paragraphs. In ad-\n",
       "\n",
       "dition, a development set is constructed by ran-\n",
       "\n",
       "domly sampling 10% of the training examples\n",
       "\n",
       "(i.e., 229 examples). The answer distribution is\n",
       "\n",
       "roughly balanced, with approximately 47% \"yes\"\n",
       "\n",
       "questions in both the training and development\n",
       "\n",
       "sets. Unless otherwise speciﬁed, the models are\n",
       "\n",
       "evaluated on the development set3 for StrategyQA.\n",
       "\n",
       "Implementation details. In this part, we uti-\n",
       "\n",
       "lize Wikipedia as the external knowledge base\n",
       "\n",
       "KB. For each sentence in the explanation of ev-\n",
       "\n",
       "ery reasoning path, we ﬁrst apply BM25 (Robert-\n",
       "\n",
       "son et al., 2009) to retrieve the top 10 most rele-\n",
       "\n",
       "vant paragraphs from Wikipedia. In particular, we\n",
       "\n",
       "use the re-implementation of the sparse retrieval\n",
       "\n",
       "BM254 in Karpukhin et al. (2020) from Pyserini\n",
       "\n",
       "(Lin et al., 2021). Subsequently, we use the pre-\n",
       "\n",
       "trained MPNet model (Song et al., 2020) to se-\n",
       "\n",
       "lect the most similar paragraph based on the cosine\n",
       "\n",
       "similarity between the sentence embeddings of the\n",
       "\n",
       "retrieved paragraph and the sentence. We then\n",
       "\n",
       "employ a pre-trained natural language inference\n",
       "\n",
       "(NLI) model (Nie et al., 2020) to obtain the en-\n",
       "\n",
       "tailment and contradiction scores for the sentence,\n",
       "\n",
       "treating the most similar paragraph as the premise.\n",
       "\n",
       "The faithfulness of each reasoning path is then\n",
       "\n",
       "calculated using fKB(·) based on the entailment\n",
       "\n",
       "scores, contradiction scores, and MPNet similari-\n",
       "\n",
       "ties of all sentences in the explanation of the rea-\n",
       "\n",
       "soning path. The ﬁnal prediction for each ques-\n",
       "\n",
       "tion is obtained through faithful inference (Equa-\n",
       "\n",
       "tion 1). More details about fKB(·) can be found in\n",
       "\n",
       "Appendix A.2.\n",
       "\n",
       "4.3 Temporal Reasoning\n",
       "\n",
       "Dataset description. In this experiment, we use\n",
       "\n",
       "the TempQuestions dataset (Jia et al., 2018) to\n",
       "\n",
       "investigate temporal reasoning. This dataset in-\n",
       "\n",
       "cludes 1, 271 temporal questions that are divided\n",
       "\n",
       "into four classes: explicit temporal, implicit tem-\n",
       "\n",
       "poral, temporal answer, and ordinal constraints.\n",
       "\n",
       "The questions are paired with their answers from\n",
       "\n",
       "Freebase (Bollacker et al., 2008). To examine the\n",
       "\n",
       "most challenging aspect of temporal reasoning, we\n",
       "\n",
       "focus on the set of implicit temporal questions,\n",
       "\n",
       "which contain implicit temporal expressions, in-\n",
       "\n",
       "cluding free-text temporal expressions. For ex-\n",
       "\n",
       "ample, the question “who was governor of oregon\n",
       "\n",
       "when shanghai noon was released?” is an implicit\n",
       "\n",
       "temporal question. To facilitate our analysis, we\n",
       "\n",
       "only consider questions with a single answer, re-\n",
       "\n",
       "sulting in a total of 175 examples. Of these ex-\n",
       "\n",
       "3As the annotations for the test set are not publicly avail-\n",
       "\n",
       "able, we use the development set for evaluation. This allows\n",
       "\n",
       "us to perform a more comprehensive analysis.\n",
       "\n",
       "4We also experimented with DPR and BM25+DPR, and\n",
       "\n",
       "found that BM25 outperformed these methods in our experi-\n",
       "\n",
       "ments. More details can be found in Appendix A.3.\n",
       "\n",
       "\n",
       "# Page 5\n",
       "\n",
       "Methods Commonsense Temporal Tabular\n",
       "\n",
       "GPT-3\n",
       "\n",
       "Zero-shot prompting 58.08 28.40 82.00\n",
       "\n",
       "Few-shot prompting 63.32 29.59 83.08\n",
       "\n",
       "Chain-of-thought prompting 65.94 33.14 83.33\n",
       "\n",
       "Self-consistency 73.36 37.28 84.00\n",
       "\n",
       "Rethinking with retrieval 77.73 39.05 84.83\n",
       "\n",
       "Table 1: Performance of different methods using GPT-3 on three reasoning tasks.\n",
       "\n",
       "amples, the ﬁrst 6 are used for prompting, and the\n",
       "\n",
       "remaining 169 are used for evaluation.\n",
       "\n",
       "Implementation details. In this part, we utilize\n",
       "\n",
       "Wikidata (Vrande ˇci´c and Krötzsch, 2014) as the\n",
       "\n",
       "external knowledge base KB, as it is the largest\n",
       "\n",
       "publicly available knowledge graph, and the data\n",
       "\n",
       "from Freebase has been migrated to Wikidata. To\n",
       "\n",
       "incorporate this knowledge into our system, we\n",
       "\n",
       "apply an entity linking system 5 to each sentence\n",
       "\n",
       "in the explanation of each reasoning path to iden-\n",
       "\n",
       "tify the corresponding Wikidata pages for all enti-\n",
       "\n",
       "ties in the sentence. Next, we extract all temporal\n",
       "\n",
       "relations from these relevant Wikidata pages and\n",
       "\n",
       "use templates to convert these temporal relations\n",
       "\n",
       "into sentences. This step generates a set of rele-\n",
       "\n",
       "vant knowledge sentences for each sentence in the\n",
       "\n",
       "explanation of each reasoning path. The ﬁnal pre-\n",
       "\n",
       "diction is then obtained by applying the procedure\n",
       "\n",
       "described in Section 4.2, in which the retrieved\n",
       "\n",
       "paragraphs are replaced with the relevant knowl-\n",
       "\n",
       "edge sentences from the current part.\n",
       "\n",
       "4.4 Tabular Reasoning\n",
       "\n",
       "Dataset description. We consider the IN-\n",
       "\n",
       "FOTABS dataset (Gupta et al., 2020) for tabu-\n",
       "\n",
       "lar reasoning, which consists of 23, 738 human-\n",
       "\n",
       "written textual hypotheses based on premises in\n",
       "\n",
       "the form of tables extracted from 2, 540 unique\n",
       "\n",
       "Wikipedia info-boxes. We focus on the develop-\n",
       "\n",
       "ment set, which includes 1, 800 hypotheses based\n",
       "\n",
       "on 200 tables, and only consider entailed and con-\n",
       "\n",
       "tradictory hypotheses as it is tricky to write CoT\n",
       "\n",
       "demonstrations for neutral hypotheses. This re-\n",
       "\n",
       "sults in a total of 1, 200 hypotheses based on 200\n",
       "\n",
       "tables for evaluation, with an equal number of en-\n",
       "\n",
       "tailed and contradictory hypotheses.\n",
       "\n",
       "Implementation details. In this part, we utilize\n",
       "\n",
       "WordNet (Miller, 1995) and ConceptNet (Speer\n",
       "\n",
       "5We use the spacy entity linker:https://pypi.org/\n",
       "\n",
       "project/spacy-entity-linker/.\n",
       "\n",
       "et al., 2017) as external knowledge bases. To con-\n",
       "\n",
       "vert tables into textual premises, we follow the\n",
       "\n",
       "same technique as in Varun et al. (2022). For each\n",
       "\n",
       "premise-hypothesis pair, we follow the procedure\n",
       "\n",
       "outlined in Varun et al. (2022) to retrieve rele-\n",
       "\n",
       "vant word relation triples that connect the premise\n",
       "\n",
       "and hypothesis words, such as “married” RelatedTo←−−−−→\n",
       "\n",
       "“spouse”. These triples are then converted into\n",
       "\n",
       "sentences using some simple templates. The re-\n",
       "\n",
       "sulting sentences, along with the textual premises\n",
       "\n",
       "from the tables, serve as relevant knowledge for\n",
       "\n",
       "each sentence in the explanation of each reasoning\n",
       "\n",
       "path. To obtain the ﬁnal prediction, the procedure\n",
       "\n",
       "described in Section 4.2 is applied, whereby the\n",
       "\n",
       "retrieved paragraphs in Section 4.2 are replaced\n",
       "\n",
       "with the relevant knowledge from the current part.\n",
       "\n",
       "4.5 Evaluation\n",
       "\n",
       "Experimental settings. In all experiments, we\n",
       "\n",
       "utilize GPT-3 text-davinci-002 unless oth-\n",
       "\n",
       "erwise stated. The maximum number of tokens for\n",
       "\n",
       "generation during completion is set to 256. For\n",
       "\n",
       "zero-shot, few-shot, and chain-of-thought prompt-\n",
       "\n",
       "ing, the temperature is ﬁxed at 0. For self-\n",
       "\n",
       "consistency and rethinking with retrieval, we ran-\n",
       "\n",
       "domly sample 10 outputs6 with temperature 0.7.\n",
       "\n",
       "Detailed prompts can be found in Appendix A.1.\n",
       "\n",
       "We evaluate the performance of different methods\n",
       "\n",
       "on commonsense and tabular reasoning using ac-\n",
       "\n",
       "curacy, and on temporal reasoning using the exact\n",
       "\n",
       "match metric as deﬁned in Rajpurkar et al. (2016).\n",
       "\n",
       "Results. As shown in Table 1, our proposed\n",
       "\n",
       "method, rethinking with retrieval, consistently\n",
       "\n",
       "outperforms all baselines on all three reasoning\n",
       "\n",
       "tasks without requiring additional training or ﬁne-\n",
       "\n",
       "tuning. The results highlight the effectiveness of\n",
       "\n",
       "our approach in leveraging external knowledge to\n",
       "\n",
       "improve the performance of LLMs.\n",
       "\n",
       "6For commonsense reasoning, we sample9 outputs, as we\n",
       "\n",
       "have found that odd numbers of outputs tend to yield better\n",
       "\n",
       "voting performance for self-consistency on StrategyQA.\n",
       "\n",
       "\n",
       "# Page 6\n",
       "\n",
       "Wrong supporting facts Wrong inference\n",
       "\n",
       "Question Was Lil Jon’s top ranked Billboard song a\n",
       "\n",
       "collaboration with a member of The Lox?\n",
       "\n",
       "Would the top of Mount Fuji stick out of\n",
       "\n",
       "the Sea of Japan?\n",
       "\n",
       "Answer No Yes\n",
       "\n",
       "Facts\n",
       "\n",
       "Lil Jon’s highest ranked billboard song\n",
       "\n",
       "was Yeah; Yeah was a collaboration be-\n",
       "\n",
       "tween Lil Jon, Usher, and Ludacris; The\n",
       "\n",
       "Lox is a rap trio consisting of: Styles P,\n",
       "\n",
       "Sheek Louch, and Jadakiss.\n",
       "\n",
       "The average depth of the Sea of Japan is\n",
       "\n",
       "5,748 feet (1,752 metres) and its maxi-\n",
       "\n",
       "mum depth is 12,276 feet (3,742 metres);\n",
       "\n",
       "Mount Fuji is 3,776.24 metres (12,389.2\n",
       "\n",
       "ft) tall.\n",
       "\n",
       "GPT-3\n",
       "\n",
       "Lil Jon’s top ranked Billboard song was\n",
       "\n",
       "\"Get Low\" with the East Side Boyz. The\n",
       "\n",
       "Lox is not a member of the East Side\n",
       "\n",
       "Boyz. Thus, Lil Jon’s top ranked Bill-\n",
       "\n",
       "board song was not a collaboration with\n",
       "\n",
       "a member of The Lox. So the answer is\n",
       "\n",
       "no.\n",
       "\n",
       "Mount Fuji is 3,776 meters tall. The\n",
       "\n",
       "Sea of Japan is about 3,741 meters deep.\n",
       "\n",
       "Thus, the top of Mount Fuji would not\n",
       "\n",
       "stick out of the Sea of Japan. So the an-\n",
       "\n",
       "swer is no.\n",
       "\n",
       "Table 2: Examples of incorrect outputs from GPT-3 with CoT prompting.\n",
       "\n",
       "5 Analysis\n",
       "\n",
       "In this section, we perform a thorough analysis to\n",
       "\n",
       "gain a deeper understanding of RR.\n",
       "\n",
       "5.1 Limitations of LLMs in Reasoning\n",
       "\n",
       "In this subsection, we present an analysis of GPT-\n",
       "\n",
       "3 with CoT prompting on the StrategyQA dataset.\n",
       "\n",
       "Upon closer examination of the outputs of GPT-\n",
       "\n",
       "3, we observed that it can provide reasonable ex-\n",
       "\n",
       "planations and correct predictions for a number\n",
       "\n",
       "of questions. For example, when given the ques-\n",
       "\n",
       "tion “Will the Albany in Georgia reach a hundred\n",
       "\n",
       "thousand occupants before the one in New York?”,\n",
       "\n",
       "GPT-3 produced the following output:\n",
       "\n",
       "The Albany in New York has a pop-\n",
       "\n",
       "ulation of about 98,000. The Albany\n",
       "\n",
       "in Georgia has a population of about\n",
       "\n",
       "77,000. Thus, the Albany in New York\n",
       "\n",
       "is more populous than the Albany in\n",
       "\n",
       "Georgia. So the answer is no.\n",
       "\n",
       "The above output consists of three components:\n",
       "\n",
       "(1) supporting facts (in cyan) that are based on a\n",
       "\n",
       "particular perspective, (2) chaining arguments (in\n",
       "\n",
       "orange), and (3) a prediction (in green). Com-\n",
       "\n",
       "ponents (1) and (2) contribute to the explanation.\n",
       "\n",
       "Overall, the output exhibits a high level of quality.\n",
       "\n",
       "However, we also observed that GPT-3 may occa-\n",
       "\n",
       "sionally produce incorrect supporting facts for its\n",
       "\n",
       "explanations or make incorrect inferences for its\n",
       "\n",
       "Retrieval Commonsense Tabular\n",
       "\n",
       "Query-based 73.36 36.69\n",
       "\n",
       "Decomposition-based 77.73 39.05\n",
       "\n",
       "Table 3: Comparison of query-based and\n",
       "\n",
       "decomposition-based retrieval on commonsense\n",
       "\n",
       "and tabular reasoning.\n",
       "\n",
       "predictions, despite generally being able to iden-\n",
       "\n",
       "tify suitable perspectives.\n",
       "\n",
       "Wrong supporting facts. As shown in Table 2,\n",
       "\n",
       "GPT-3 provides the incorrect supporting fact for\n",
       "\n",
       "Lil Jon’s top-ranked Billboard song, stating that\n",
       "\n",
       "it was “Get Low” instead of the correct answer,\n",
       "\n",
       "“Yeah”. However, it does have the correct per-\n",
       "\n",
       "spective on how to answer the question, “ Was \n",
       "..._This content has been truncated to stay below 50000 characters_...\n",
       "iﬁcantly outperform the voting baseline, self-\n",
       "\n",
       "consistency (i.e., 73.36%), as shown in Table 1.\n",
       "\n",
       "It is noteworthy that UniﬁedQA performs\n",
       "\n",
       "poorly on StrategyQA, achieving an accuracy of\n",
       "\n",
       "only 58.95%. However, when provided with\n",
       "\n",
       "gold supporting facts in StrategyQA, UniﬁedQA\n",
       "\n",
       "demonstrates excellent performance with an accu-\n",
       "\n",
       "racy of 90.83%. This suggests that UniﬁedQA is\n",
       "\n",
       "suitable for last-step inference, but not effective\n",
       "\n",
       "for answering questions in StrategyQA.\n",
       "\n",
       "5.4 Impact of the Size of LMs\n",
       "\n",
       "In this subsection, we examine the effect of the\n",
       "\n",
       "size of LMs on the performance of our proposed\n",
       "\n",
       "method, speciﬁcally in the context of the fact gen-\n",
       "\n",
       "eration variant. We compare the performance of\n",
       "\n",
       "our method using various sizes of OPT models\n",
       "\n",
       "(Zhang et al., 2022) in addition to GPT-3 (175B)\n",
       "\n",
       "using the same experimental setup as in Sec-\n",
       "\n",
       "tion 5.3. As shown in Figure 2, our proposed\n",
       "\n",
       "method (Variant II) consistently outperforms CoT\n",
       "\n",
       "prompting in terms of both prediction accuracy\n",
       "\n",
       "and the faithfulness of explanations, even when\n",
       "\n",
       "using smaller LMs.\n",
       "\n",
       "6 Conclusion\n",
       "\n",
       "In conclusion, the proposed approach is a promis-\n",
       "\n",
       "ing solution for utilizing external knowledge to as-\n",
       "\n",
       "sist LLMs. Unlike traditional methods, RR does\n",
       "\n",
       "not require additional training or ﬁne-tuning, mak-\n",
       "\n",
       "ing it a lightweight and feasible option for LLMs.\n",
       "\n",
       "Through extensive experiments on three reason-\n",
       "\n",
       "ing tasks using GPT-3, we have shown that RR is\n",
       "\n",
       "able to produce more faithful explanations and im-\n",
       "\n",
       "prove the performance of LLMs. In the future, we\n",
       "\n",
       "plan to investigate various variations of RR to en-\n",
       "\n",
       "hance its effectiveness and efﬁciency in augment-\n",
       "\n",
       "ing LLMs with external knowledge.\n",
       "\n",
       "\n",
       "# Page 9\n",
       "\n",
       "References\n",
       "\n",
       "Kurt Bollacker, Colin Evans, Praveen Paritosh,\n",
       "\n",
       "Tim Sturge, and Jamie Taylor. 2008. Freebase:\n",
       "\n",
       "a collaboratively created graph database for\n",
       "\n",
       "structuring human knowledge. In Proceedings\n",
       "\n",
       "of the 2008 ACM SIGMOD international con-\n",
       "\n",
       "ference on Management of data , pages 1247–\n",
       "\n",
       "1250.\n",
       "\n",
       "Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-\n",
       "\n",
       "mann, Trevor Cai, Eliza Rutherford, Katie Mil-\n",
       "\n",
       "lican, George van den Driessche, Jean-Baptiste\n",
       "\n",
       "Lespiau, Bogdan Damoc, Aidan Clark, et al.\n",
       "\n",
       "2021. Improving language models by retriev-\n",
       "\n",
       "ing from trillions of tokens. arXiv preprint\n",
       "\n",
       "arXiv:2112.04426.\n",
       "\n",
       "Tom Brown, Benjamin Mann, Nick Ryder,\n",
       "\n",
       "Melanie Subbiah, Jared D Kaplan, Prafulla\n",
       "\n",
       "Dhariwal, Arvind Neelakantan, Pranav Shyam,\n",
       "\n",
       "Girish Sastry, Amanda Askell, et al. 2020.\n",
       "\n",
       "Language models are few-shot learners. Ad-\n",
       "\n",
       "vances in neural information processing sys-\n",
       "\n",
       "tems, 33:1877–1901.\n",
       "\n",
       "Aakanksha Chowdhery, Sharan Narang, Jacob De-\n",
       "\n",
       "vlin, Maarten Bosma, Gaurav Mishra, Adam\n",
       "\n",
       "Roberts, Paul Barham, Hyung Won Chung,\n",
       "\n",
       "Charles Sutton, Sebastian Gehrmann, et al.\n",
       "\n",
       "2022. Palm: Scaling language modeling with\n",
       "\n",
       "pathways. arXiv preprint arXiv:2204.02311.\n",
       "\n",
       "Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-\n",
       "\n",
       "ian, Jacob Hilton, Reiichiro Nakano, Christo-\n",
       "\n",
       "pher Hesse, and John Schulman. 2021. Training\n",
       "\n",
       "veriﬁers to solve math word problems. arXiv\n",
       "\n",
       "preprint arXiv:2110.14168.\n",
       "\n",
       "Ido Dagan, Oren Glickman, and Bernardo\n",
       "\n",
       "Magnini. 2005. The pascal recognising tex-\n",
       "\n",
       "tual entailment challenge. In Machine learning\n",
       "\n",
       "challenges workshop, pages 177–190. Springer.\n",
       "\n",
       "Daniel Deutsch, Tania Bedrax-Weiss, and Dan\n",
       "\n",
       "Roth. 2021. Towards question-answering as\n",
       "\n",
       "an automatic metric for evaluating the content\n",
       "\n",
       "quality of a summary. Transactions of the Asso-\n",
       "\n",
       "ciation for Computational Linguistics , 9:774–\n",
       "\n",
       "789.\n",
       "\n",
       "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
       "\n",
       "Kristina Toutanova. 2019. BERT: Pre-training\n",
       "\n",
       "of deep bidirectional transformers for language\n",
       "\n",
       "understanding. In Proceedings of the 2019\n",
       "\n",
       "Conference of the North American Chapter\n",
       "\n",
       "of the Association for Computational Linguis-\n",
       "\n",
       "tics: Human Language Technologies, Volume 1\n",
       "\n",
       "(Long and Short Papers), pages 4171–4186.\n",
       "\n",
       "Alexander R Fabbri, Chien-Sheng Wu, Wenhao\n",
       "\n",
       "Liu, and Caiming Xiong. 2021. Qafacte-\n",
       "\n",
       "val: Improved qa-based factual consistency\n",
       "\n",
       "evaluation for summarization. arXiv preprint\n",
       "\n",
       "arXiv:2112.08542.\n",
       "\n",
       "Mor Geva, Daniel Khashabi, Elad Segal, Tushar\n",
       "\n",
       "Khot, Dan Roth, and Jonathan Berant. 2021.\n",
       "\n",
       "Did aristotle use a laptop? a question answer-\n",
       "\n",
       "ing benchmark with implicit reasoning strate-\n",
       "\n",
       "gies. Transactions of the Association for Com-\n",
       "\n",
       "putational Linguistics, 9:346–361.\n",
       "\n",
       "Liangke Gui, Borui Wang, Qiuyuan Huang, Alex\n",
       "\n",
       "Hauptmann, Yonatan Bisk, and Jianfeng Gao.\n",
       "\n",
       "2021. Kat: A knowledge augmented trans-\n",
       "\n",
       "former for vision-and-language. arXiv preprint\n",
       "\n",
       "arXiv:2112.08614.\n",
       "\n",
       "Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and\n",
       "\n",
       "Vivek Srikumar. 2020. Infotabs: Inference on\n",
       "\n",
       "tables as semi-structured data. In Proceed-\n",
       "\n",
       "ings of the 58th Annual Meeting of the As-\n",
       "\n",
       "sociation for Computational Linguistics , pages\n",
       "\n",
       "2309–2324.\n",
       "\n",
       "Kelvin Guu, Kenton Lee, Zora Tung, Panupong\n",
       "\n",
       "Pasupat, and Mingwei Chang. 2020. Retrieval\n",
       "\n",
       "augmented language model pre-training. In In-\n",
       "\n",
       "ternational Conference on Machine Learning ,\n",
       "\n",
       "pages 3929–3938. PMLR.\n",
       "\n",
       "Or Honovich, Roee Aharoni, Jonathan Herzig,\n",
       "\n",
       "Hagai Taitelbaum, Doron Kukliansy, Vered Co-\n",
       "\n",
       "hen, Thomas Scialom, Idan Szpektor, Avinatan\n",
       "\n",
       "Hassidim, and Yossi Matias. 2022. True: Re-\n",
       "\n",
       "evaluating factual consistency evaluation. In\n",
       "\n",
       "Proceedings of the Second DialDoc Workshop\n",
       "\n",
       "on Document-grounded Dialogue and Conver-\n",
       "\n",
       "sational Question Answering, pages 161–175.\n",
       "\n",
       "Or Honovich, Leshem Choshen, Roee Aharoni,\n",
       "\n",
       "Ella Neeman, Idan Szpektor, and Omri Abend.\n",
       "\n",
       "2021. Q2:: Evaluating factual consistency\n",
       "\n",
       "in knowledge-grounded dialogues via question\n",
       "\n",
       "generation and question answering. In Pro-\n",
       "\n",
       "ceedings of the 2021 Conference on Empiri-\n",
       "\n",
       "cal Methods in Natural Language Processing ,\n",
       "\n",
       "pages 7856–7870.\n",
       "\n",
       "\n",
       "# Page 10\n",
       "\n",
       "Zhen Jia, Abdalghani Abujabal, Rishiraj\n",
       "\n",
       "Saha Roy, Jannik Strötgen, and Gerhard\n",
       "\n",
       "Weikum. 2018. Tempquestions: A benchmark\n",
       "\n",
       "for temporal question answering. In Compan-\n",
       "\n",
       "ion Proceedings of the The Web Conference\n",
       "\n",
       "2018, pages 1057–1062.\n",
       "\n",
       "Mandar Joshi, Kenton Lee, Yi Luan, and Kristina\n",
       "\n",
       "Toutanova. 2020. Contextualized representa-\n",
       "\n",
       "tions using textual encyclopedic knowledge.\n",
       "\n",
       "arXiv preprint arXiv:2004.12006.\n",
       "\n",
       "Vladimir Karpukhin, Barlas Oguz, Sewon Min,\n",
       "\n",
       "Patrick Lewis, Ledell Wu, Sergey Edunov,\n",
       "\n",
       "Danqi Chen, and Wen-tau Yih. 2020. Dense\n",
       "\n",
       "passage retrieval for open-domain question an-\n",
       "\n",
       "swering. In Proceedings of the 2020 Confer-\n",
       "\n",
       "ence on Empirical Methods in Natural Lan-\n",
       "\n",
       "guage Processing (EMNLP), pages 6769–6781.\n",
       "\n",
       "Urvashi Khandelwal, Omer Levy, Dan Juraf-\n",
       "\n",
       "sky, Luke Zettlemoyer, and Mike Lewis. 2020.\n",
       "\n",
       "Generalization through memorization: Nearest\n",
       "\n",
       "neighbor language models. In International\n",
       "\n",
       "Conference on Learning Representations.\n",
       "\n",
       "Daniel Khashabi, Yeganeh Kordi, and Hannaneh\n",
       "\n",
       "Hajishirzi. 2022. Uniﬁedqa-v2: Stronger gen-\n",
       "\n",
       "eralization via broader cross-format training.\n",
       "\n",
       "arXiv preprint arXiv:2202.12359.\n",
       "\n",
       "Daniel Khashabi, Sewon Min, Tushar Khot,\n",
       "\n",
       "Ashish Sabharwal, Oyvind Tafjord, Peter Clark,\n",
       "\n",
       "and Hannaneh Hajishirzi. 2020. Uniﬁedqa:\n",
       "\n",
       "Crossing format boundaries with a single qa\n",
       "\n",
       "system. In Findings of the Association for Com-\n",
       "\n",
       "putational Linguistics: EMNLP 2020 , pages\n",
       "\n",
       "1896–1907.\n",
       "\n",
       "Takeshi Kojima, Shixiang Shane Gu, Machel\n",
       "\n",
       "Reid, Yutaka Matsuo, and Yusuke Iwasawa.\n",
       "\n",
       "2022. Large language models are zero-shot rea-\n",
       "\n",
       "soners. arXiv preprint arXiv:2205.11916.\n",
       "\n",
       "Mojtaba Komeili, Kurt Shuster, and Jason Weston.\n",
       "\n",
       "2022. Internet-augmented dialogue generation.\n",
       "\n",
       "In Proceedings of the 60th Annual Meeting of\n",
       "\n",
       "the Association for Computational Linguistics\n",
       "\n",
       "(Volume 1: Long Papers), pages 8460–8478.\n",
       "\n",
       "Patrick Lewis, Ethan Perez, Aleksandra Piktus,\n",
       "\n",
       "Fabio Petroni, Vladimir Karpukhin, Naman\n",
       "\n",
       "Goyal, Heinrich Küttler, Mike Lewis, Wen-tau\n",
       "\n",
       "Yih, Tim Rocktäschel, et al. 2020. Retrieval-\n",
       "\n",
       "augmented generation for knowledge-intensive\n",
       "\n",
       "nlp tasks. Advances in Neural Information Pro-\n",
       "\n",
       "cessing Systems, 33:9459–9474.\n",
       "\n",
       "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\n",
       "\n",
       "Jheng-Hong Yang, Ronak Pradeep, and Rodrigo\n",
       "\n",
       "Nogueira. 2021. Pyserini: A Python toolkit for\n",
       "\n",
       "reproducible information retrieval research with\n",
       "\n",
       "sparse and dense representations. In Proceed-\n",
       "\n",
       "ings of the 44th Annual International ACM SI-\n",
       "\n",
       "GIR Conference on Research and Development\n",
       "\n",
       "in Information Retrieval (SIGIR 2021) , pages\n",
       "\n",
       "2356–2362.\n",
       "\n",
       "Jiacheng Liu, Alisa Liu, Ximing Lu, Sean\n",
       "\n",
       "Welleck, Peter West, Ronan Le Bras, Yejin\n",
       "\n",
       "Choi, and Hannaneh Hajishirzi. 2022. Gen-\n",
       "\n",
       "erated knowledge prompting for commonsense\n",
       "\n",
       "reasoning. In Proceedings of the 60th Annual\n",
       "\n",
       "Meeting of the Association for Computational\n",
       "\n",
       "Linguistics (Volume 1: Long Papers) , pages\n",
       "\n",
       "3154–3169.\n",
       "\n",
       "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei\n",
       "\n",
       "Du, Mandar Joshi, Danqi Chen, Omer Levy,\n",
       "\n",
       "Mike Lewis, Luke Zettlemoyer, and Veselin\n",
       "\n",
       "Stoyanov. 2019. Roberta: A robustly opti-\n",
       "\n",
       "mized bert pretraining approach. arXiv preprint\n",
       "\n",
       "arXiv:1907.11692.\n",
       "\n",
       "George A Miller. 1995. Wordnet: a lexical\n",
       "\n",
       "database for english. Communications of the\n",
       "\n",
       "ACM, 38(11):39–41.\n",
       "\n",
       "Reiichiro Nakano, Jacob Hilton, Suchir Bal-\n",
       "\n",
       "aji, Jeff Wu, Long Ouyang, Christina Kim,\n",
       "\n",
       "Christopher Hesse, Shantanu Jain, Vineet\n",
       "\n",
       "Kosaraju, William Saunders, et al. 2021. We-\n",
       "\n",
       "bgpt: Browser-assisted question-answering\n",
       "\n",
       "with human feedback. arXiv preprint\n",
       "\n",
       "arXiv:2112.09332.\n",
       "\n",
       "J Neeraja, Vivek Gupta, and Vivek Srikumar.\n",
       "\n",
       "2021. Incorporating external knowledge to en-\n",
       "\n",
       "hance tabular reasoning. In Proceedings of the\n",
       "\n",
       "2021 Conference of the North American Chap-\n",
       "\n",
       "ter of the Association for Computational Lin-\n",
       "\n",
       "guistics: Human Language Technologies, pages\n",
       "\n",
       "2799–2809.\n",
       "\n",
       "Yixin Nie, Adina Williams, Emily Dinan, Mohit\n",
       "\n",
       "Bansal, Jason Weston, and Douwe Kiela. 2020.\n",
       "\n",
       "Adversarial nli: A new benchmark for natu-\n",
       "\n",
       "ral language understanding. In Proceedings of\n",
       "\n",
       "the 58th Annual Meeting of the Association for\n",
       "\n",
       "Computational Linguistics, pages 4885–4901.\n",
       "\n",
       "\n",
       "# Page 11\n",
       "\n",
       "Maxwell Nye, Anders Johan Andreassen, Guy\n",
       "\n",
       "Gur-Ari, Henryk Michalewski, Jacob Austin,\n",
       "\n",
       "David Bieber, David Dohan, Aitor Lewkowycz,\n",
       "\n",
       "Maarten Bosma, David Luan, et al. 2022. Show\n",
       "\n",
       "your work: Scratchpads for intermediate com-\n",
       "\n",
       "putation with language models. In Deep Learn-\n",
       "\n",
       "ing for Code Workshop.\n",
       "\n",
       "Maxwell Nye, Michael Tessler, Josh Tenenbaum,\n",
       "\n",
       "and Brenden M Lake. 2021. Improving coher-\n",
       "\n",
       "ence and consistency in neural sequence mod-\n",
       "\n",
       "els with dual-system, neuro-symbolic reason-\n",
       "\n",
       "ing. Advances in Neural Information Process-\n",
       "\n",
       "ing Systems, 34:25192–25204.\n",
       "\n",
       "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\n",
       "\n",
       "Carroll L Wainwright, Pamela Mishkin, Chong\n",
       "\n",
       "Zhang, Sandhini Agarwal, Katarina Slama,\n",
       "\n",
       "Alex Ray, et al. 2022. Training language mod-\n",
       "\n",
       "els to follow instructions with human feedback.\n",
       "\n",
       "arXiv preprint arXiv:2203.02155.\n",
       "\n",
       "Colin Raffel, Noam Shazeer, Adam Roberts,\n",
       "\n",
       "Katherine Lee, Sharan Narang, Michael\n",
       "\n",
       "Matena, Yanqi Zhou, Wei Li, and Peter J Liu.\n",
       "\n",
       "2020. Exploring the limits of transfer learning\n",
       "\n",
       "with a uniﬁed text-to-text transformer. Journal\n",
       "\n",
       "of Machine Learning Research, 21:1–67.\n",
       "\n",
       "Pranav Rajpurkar, Jian Zhang, Konstantin Lopy-\n",
       "\n",
       "rev, and Percy Liang. 2016. Squad: 100,000+\n",
       "\n",
       "questions for machine comprehension of text.\n",
       "\n",
       "In Proceedings of the 2016 Conference on Em-\n",
       "\n",
       "pirical Methods in Natural Language Process-\n",
       "\n",
       "ing, pages 2383–2392.\n",
       "\n",
       "Stephen Robertson, Hugo Zaragoza, et al. 2009.\n",
       "\n",
       "The probabilistic relevance framework: Bm25\n",
       "\n",
       "and beyond. Foundations and Trends® in In-\n",
       "\n",
       "formation Retrieval, 3(4):333–389.\n",
       "\n",
       "Kurt Shuster, Mojtaba Komeili, Leonard Adolphs,\n",
       "\n",
       "Stephen Roller, Arthur Szlam, and Jason We-\n",
       "\n",
       "ston. 2022. Language models that seek for\n",
       "\n",
       "knowledge: Modular search & generation\n",
       "\n",
       "for dialogue and prompt completion. arXiv\n",
       "\n",
       "preprint arXiv:2203.13224.\n",
       "\n",
       "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu,\n",
       "\n",
       "and Tie-Yan Liu. 2020. Mpnet: Masked and\n",
       "\n",
       "permuted pre-training for language understand-\n",
       "\n",
       "ing. Advances in Neural Information Process-\n",
       "\n",
       "ing Systems, 33:16857–16867.\n",
       "\n",
       "Robyn Speer, Joshua Chin, and Catherine Havasi.\n",
       "\n",
       "2017. Conceptnet 5.5: An open multilingual\n",
       "\n",
       "graph of general knowledge. In Thirty-ﬁrst\n",
       "\n",
       "AAAI conference on artiﬁcial intelligence.\n",
       "\n",
       "Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav\n",
       "\n",
       "Goldberg, and Jonathan Berant. 2020. Leap-\n",
       "\n",
       "of-thought: Teaching pre-trained models to\n",
       "\n",
       "systematically reason over implicit knowledge.\n",
       "\n",
       "Advances in Neural Information Processing\n",
       "\n",
       "Systems, 33:20227–20237.\n",
       "\n",
       "Romal Thoppilan, Daniel De Freitas, Jamie Hall,\n",
       "\n",
       "Noam Shazeer, Apoorv Kulshreshtha, Heng-\n",
       "\n",
       "Tze Cheng, Alicia Jin, Taylor Bos, Leslie\n",
       "\n",
       "Baker, Yu Du, et al. 2022. Lamda: Language\n",
       "\n",
       "models for dialog applications. arXiv preprint\n",
       "\n",
       "arXiv:2201.08239.\n",
       "\n",
       "Yerram Varun, Aayush Sharma, and Vivek Gupta.\n",
       "\n",
       "2022. Trans-kblstm: An external knowledge\n",
       "\n",
       "enhanced transformer bilstm model for tabular\n",
       "\n",
       "reasoning. In Proceedings of Deep Learning In-\n",
       "\n",
       "side Out (DeeLIO 2022): The 3rd Workshop on\n",
       "\n",
       "Knowledge Extraction and Integration for Deep\n",
       "\n",
       "Learning Architectures, pages 62–78.\n",
       "\n",
       "Ashish Vaswani, Noam Shazeer, Niki Parmar,\n",
       "\n",
       "Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
       "\n",
       "Łukasz Kaiser, and Illia Polosukhin. 2017. At-\n",
       "\n",
       "tention is all you need. Advances in neural in-\n",
       "\n",
       "formation processing systems, 30.\n",
       "\n",
       "Denny Vrande ˇci´c and Markus Krötzsch. 2014.\n",
       "\n",
       "Wikidata: a free collaborative knowledgebase.\n",
       "\n",
       "Communications of the ACM, 57(10):78–85.\n",
       "\n",
       "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\n",
       "\n",
       "Le, Ed Chi, and Denny Zhou. 2022. Self-\n",
       "\n",
       "consistency improves chain of thought rea-\n",
       "\n",
       "soning in language models. arXiv preprint\n",
       "\n",
       "arXiv:2203.11171.\n",
       "\n",
       "Jason Wei, Xuezhi Wang, Dale Schuurmans,\n",
       "\n",
       "Maarten Bosma, Ed Chi, Quoc Le, and Denny\n",
       "\n",
       "Zhou. 2022. Chain of thought prompting elic-\n",
       "\n",
       "its reasoning in large language models. arXiv\n",
       "\n",
       "preprint arXiv:2201.11903.\n",
       "\n",
       "Thomas Wolf, Lysandre Debut, Victor Sanh,\n",
       "\n",
       "Julien Chaumond, Clement Delangue, Anthony\n",
       "\n",
       "Moi, Pierric Cistac, Tim Rault, Rémi Louf,\n",
       "\n",
       "Morgan Funtowicz, et al. 2020. Transformers:\n",
       "\n",
       "State-of-the-art natural language processing. In\n",
       "\n",
       "\n",
       "# Page 12\n",
       "\n",
       "Proceedings of the 2020 conference on empir-\n",
       "\n",
       "ical methods in natural language processing:\n",
       "\n",
       "system demonstrations, pages 38–45.\n",
       "\n",
       "Xi Ye and Greg Durrett. 2022. The unreliability\n",
       "\n",
       "of explanations in few-shot in-context learning.\n",
       "\n",
       "arXiv preprint arXiv:2205.03401.\n",
       "\n",
       "Eric Zelikman, Yuhuai Wu, and Noah D Good-\n",
       "\n",
       "man. 2022. Star: Bootstrapping reasoning with\n",
       "\n",
       "reasoning. arXiv preprint arXiv:2203.14465.\n",
       "\n",
       "Susan Zhang, Stephen Roller, Naman Goyal,\n",
       "\n",
       "Mikel Artetxe, Moya Chen, Shuohui Chen,\n",
       "\n",
       "Christopher Dewan, Mona Diab, Xian Li,\n",
       "\n",
       "Xi Victoria Lin, et al. 2022. Opt: Open pre-\n",
       "\n",
       "trained transformer language models. arXiv\n",
       "\n",
       "preprint arXiv:2205.01068.\n",
       "\n",
       "Denny Zhou, Nathanael Schärli, Le Hou, Ja-\n",
       "\n",
       "son Wei, Nathan Scales, Xuezhi Wang, Dale\n",
       "\n",
       "Schuurmans, Olivier Bousquet, Quoc Le, and\n",
       "\n",
       "Ed Chi. 2022. Least-to-most prompting enables\n",
       "\n",
       "complex reasoning in large language models.\n",
       "\n",
       "arXiv preprint arXiv:2205.10625.\n",
       "\n",
       "\n",
       "# Page 13\n",
       "\n",
       "A Appendix\n",
       "\n",
       "In this section, we provide additional details on\n",
       "\n",
       "our experimental setup. Further information can\n",
       "\n",
       "be found in our code.\n",
       "\n",
       "A.1 Detailed Prompts\n",
       "\n",
       "We adopt the same CoT prompt for commonsense\n",
       "\n",
       "reasoning (i.e., StrategyQA) as those presented in\n",
       "\n",
       "Wei et al. (2022). The CoT prompt for tempo-\n",
       "\n",
       "ral reasoning is provided in Table 6. For tabular\n",
       "\n",
       "reasoning, we adopt the method of Brown et al.\n",
       "\n",
       "(2020) for converting NLI into QA for RTE (Da-\n",
       "\n",
       "gan et al., 2005), and randomly sample6 examples\n",
       "\n",
       "from the training data to construct the prompt, as\n",
       "\n",
       "shown in Table 8. The few-shot prompt utilizes\n",
       "\n",
       "the same exemplars as the CoT prompt and does\n",
       "\n",
       "not involve CoT reasoning processes.\n",
       "\n",
       "A.2 Description of Faithfulness Functions\n",
       "\n",
       "For a sentence s, we denote its MPNet similarity,\n",
       "\n",
       "entailment score, and contradiction score asM(s),\n",
       "\n",
       "E(s), and C(s), respectively. In our experiments,\n",
       "\n",
       "the corresponding thresholds for these scores are\n",
       "\n",
       "Tm = 0.5, Te = 0.6, and Tc = 0.99. Given the\n",
       "\n",
       "entailment scores, contradiction scores, and MP-\n",
       "\n",
       "Net similarities of all supporting facts (denoted as\n",
       "\n",
       "S) in the explanation of a reasoning pathR, differ-\n",
       "\n",
       "ent faithfulness functionsfKB(·) can be adopted in\n",
       "\n",
       "different settings as follows:\n",
       "\n",
       "(1) fKB(R) =∑\n",
       "\n",
       "s∈S[M(s)×(M(s) >= Tm)+\n",
       "\n",
       "E(s) ×(M(s) < Tm) −C(s)]\n",
       "\n",
       "(2) fKB(R) =∑\n",
       "\n",
       "s∈S[M(s) +E(s)]\n",
       "\n",
       "(3) fKB(R) = ∑\n",
       "\n",
       "s∈S[E(s) ×(E(s) >= Te) −\n",
       "\n",
       "C(s) ×(C(s) >= Tc)]\n",
       "\n",
       "In Section 4, we employ function (1) for com-\n",
       "\n",
       "monsense and tabular reasoning. For temporal rea-\n",
       "\n",
       "soning, we use function (2) as the distinct nature of\n",
       "\n",
       "sentences converted from temporal relations leads\n",
       "\n",
       "to unreliable contradiction scores. In Sections 5.3-\n",
       "\n",
       "5.4, we use function (3) for commonsense reason-\n",
       "\n",
       "ing with evidence paragraphs, as the high quality\n",
       "\n",
       "of the relevant knowledge negates the need for the\n",
       "\n",
       "complementary use of the MPNet similarity to im-\n",
       "\n",
       "prove the entailment score.\n",
       "\n",
       "A.3 Comparison of Retrieval Systems\n",
       "\n",
       "For commonsense reasoning, we utilized different\n",
       "\n",
       "retrieval systems in Karpukhin et al. (2020) to re-\n",
       "\n",
       "trieve relevant paragraphs from Wikipedia. The\n",
       "\n",
       "performance of BM25, DPR, and BM25+DPR\n",
       "\n",
       "were 77.73%, 58.52%, and 77.29%, respectively,\n",
       "\n",
       "indicating that BM25 is the best choice in our case.\n",
       "\n",
       "A.4 Implementation Details for the Two\n",
       "\n",
       "Variants of RR\n",
       "\n",
       "Fact selection implementation details. In this\n",
       "\n",
       "work, we utilize the information present in the top-\n",
       "\n",
       "ranked output produced by our basic approach as\n",
       "\n",
       "a guide. To this end, we apply a greedy clustering\n",
       "\n",
       "algorithm to group the sentences from all outputs\n",
       "\n",
       "into distinct topic categories based on the cosine\n",
       "\n",
       "similarity of their MPNet sentence embeddings.\n",
       "\n",
       "For each fact in the top-ranked output of our ba-\n",
       "\n",
       "sic approach, we identify the fact with the highest\n",
       "\n",
       "faithfulness within the same topic group and re-\n",
       "\n",
       "place it in the output. The faithfulness of a fact is\n",
       "\n",
       "calculated using the fKBfunction by replacing the\n",
       "\n",
       "supporting facts with a single fact.\n",
       "\n",
       "Fact generation implementation details. In\n",
       "\n",
       "this part, we generate questions for the named en-\n",
       "\n",
       "tities present in each fact of the top-ranked output\n",
       "\n",
       "produced by our basic approach, and retrieve the\n",
       "\n",
       "corresponding answers from the evidence para-\n",
       "\n",
       "graphs using UniﬁedQA. We employ the ques-\n",
       "\n",
       "tion generation model described in Deutsch et al.\n",
       "\n",
       "(2021), which has been shown to be more ex-\n",
       "\n",
       "tractive compared to other models as demon-\n",
       "\n",
       "strated in Fabbri et al. (2021). We adopt the\n",
       "\n",
       "question ﬁltering approach proposed in Honovich\n",
       "\n",
       "et al. (2021) using an off-the-shelf extractive\n",
       "\n",
       "QA model (ktrapeznikov/albert-xlarge-v2-squad-\n",
       "\n",
       "v2 from Hugging Face (Wolf et al., 2020)). We\n",
       "\n",
       "then use an off-the-shelf model (MarkS/bart-base-\n",
       "\n",
       "qa2d from Hugging Face) to convert the generated\n",
       "\n",
       "QA pairs into declarative sentences. We apply\n",
       "\n",
       "simple rules based on the entailment and contra-\n",
       "\n",
       "diction scores of the selected facts from the fact se-\n",
       "\n",
       "lection variant and the generated declarative sen-\n",
       "\n",
       "tences to obtain the ﬁnal generated facts.\n",
       "\n",
       "A.5 Comparison of Different Inference\n",
       "\n",
       "Methods with Supporting Facts\n",
       "\n",
       "In our experiments, we utilize UniﬁedQA for the\n",
       "\n",
       "ﬁnal step of inference in both variants. However,\n",
       "\n",
       "it is worth noting that GPT-3 could also be used\n",
       "\n",
       "for this purpose. As shown in Table 7, we observe\n",
       "\n",
       "that UniﬁedQA performs better at inference with\n",
       "\n",
       "generated facts, while GPT-3 with CoT prompt-\n",
       "\n",
       "ing performs better with empty or gold facts. This\n",
       "\n",
       "suggests that UniﬁedQA is more robust to noisy\n",
       "\n",
       "\n",
       "# Page 14\n",
       "\n",
       "Q: who was governor of minnesota when maathaad maathaadu mallige was released?\n",
       "\n",
       "A: Maathaad Maathaadu Mallige was released on 24 August 2007. Tim Pawlenty served as the 39th gov-\n",
       "\n",
       "ernor of Minnesota from 2003 to 2011. Thus, Tim Pawlenty was governor of minnesota when maathaad\n",
       "\n",
       "maathaadu mallige was released. So the answer is Tim Pawlenty.\n",
       "\n",
       "Q: who was us president during the costa rican civil war?\n",
       "\n",
       "A: The Costa Rican civil war was a civil war in Costa Rica from 12 March to 24 April 1948. Harry S.\n",
       "\n",
       "Truman was the 33rd president of the United States, serving from 1945 to 1953. Thus, Harry S. Truman\n",
       "\n",
       "was us president during the costa rican civil war. So the answer is Harry S. Truman.\n",
       "\n",
       "Q: who was governor of oregon when the collector was released?\n",
       "\n",
       "A: The Collector premiered at the Cannes Film Festival on May 20, 1965. Mark Hatﬁeld served as the\n",
       "\n",
       "29th governor of Oregon from 1959 to 1967. Thus, Mark Hatﬁeld was governor of oregon when the\n",
       "\n",
       "collector was released. So the answer is Mark Hatﬁeld.\n",
       "\n",
       "Q: who was governor of oregon when shanghai noon was released?\n",
       "\n",
       "A: Shanghai Noon was released on May 26, 2000. John Kitzhaber served as the 35th governor of Oregon\n",
       "\n",
       "from 1995 to 2003. Thus, John Kitzhaber was governor of oregon when shanghai noon was released. So\n",
       "\n",
       "the answer is John Kitzhaber.\n",
       "\n",
       "Q: who was us president when john andrew shulze was a teenager?\n",
       "\n",
       "A: John Andrew Shulze was born on July 19, 1775. A teenager is someone who is between 13 and 19\n",
       "\n",
       "years old. George Washington served as the ﬁrst president of the United States from 1789 to 1797. Thus,\n",
       "\n",
       "George Washington was us president when john andrew shulze was a teenager.So the answer is George\n",
       "\n",
       "Washington.\n",
       "\n",
       "Q: who was us president during the seventh coalition?\n",
       "\n",
       "A: The War of the Seventh Coalition was from 20 March to 8 July 1815. James Madison served as the\n",
       "\n",
       "fourth president of the United States from 1809 to 1817. Thus, James Madison was us president during\n",
       "\n",
       "the seventh coalition. So the answer is James Madison.\n",
       "\n",
       "Table 6: The CoT prompt for temporal reasoning.\n",
       "\n",
       "Methods Accuracy (%)\n",
       "\n",
       "Empty facts\n",
       "\n",
       "GPT-3 (zero-shot) 58.08\n",
       "\n",
       "GPT-3 (CoT) 65.94\n",
       "\n",
       "UniﬁedQA 58.95\n",
       "\n",
       "Gold facts\n",
       "\n",
       "GPT-3 (zero-shot) 81.66\n",
       "\n",
       "GPT-3 (CoT) 91.70\n",
       "\n",
       "UniﬁedQA 90.83\n",
       "\n",
       "Generated facts\n",
       "\n",
       "GPT-3 (zero-shot) 69.87\n",
       "\n",
       "GPT-3 (CoT) 76.42\n",
       "\n",
       "UniﬁedQA 78.60\n",
       "\n",
       "Table 7: Comparison of different inference methods on\n",
       "\n",
       "empty, gold, and generated facts.\n",
       "\n",
       "inputs compared to GPT-3. Additionally, both\n",
       "\n",
       "UniﬁedQA and GPT-3 with CoT prompting signif-\n",
       "\n",
       "icantly outperform GPT-3 with zero-shot prompt-\n",
       "\n",
       "ing, indicating that the CoT prompting is also ben-\n",
       "\n",
       "eﬁcial for the ﬁnal step of inference.\n",
       "\n",
       "\n",
       "# Page 15\n",
       "\n",
       "Charles Sumner Tainter was Born on April 25, 1854 ( 1854-04-25 ) Watertown, Massachusetts, U.S..\n",
       "\n",
       "Charles Sumner Tainter was Died on April 20, 1940 ( 1940-04-21 ) (aged 85) San Diego, California,\n",
       "\n",
       "U.S.. The Nationality of Charles Sumner Tainter are American. The Known for of Charles Sumner\n",
       "\n",
       "Tainter are Photophone, phonograph Father Of The Speaking Machine.\n",
       "\n",
       "Question: Charles Sumner Tainter never left the state of Massachusetts. True or False?\n",
       "\n",
       "Answer: Charles Sumner Tainter was died in San Diego, California, U.S.. California is a state. Thus,\n",
       "\n",
       "Charles Sumner Tainter has left the state of Massachusetts. So the answer is false.\n",
       "\n",
       "The Region of Curitiba are South. The Elevation of Curitiba are 934.6 m (3,066.3 ft). The Density of\n",
       "\n",
       "Curitiba are 4,062/km 2 (10,523/sq mi). The Metro density of Curitiba are 210.9/km 2 (546.2/sq mi).\n",
       "\n",
       "Question: Curitiba is above sea level. True or False?\n",
       "\n",
       "Answer: The elevation of Curitiba are 934.6 m (3,066.3 ft). Elevation is a hypernym of level. Thus,\n",
       "\n",
       "Curitiba is above sea level. So the answer is true.\n",
       "\n",
       "Charles (Prince of Wales) was Born on 14 November 1948 ( 1948-11-14 ) (age 70) Buckingham Palace,\n",
       "\n",
       "London, England. The Spouse of Charles (Prince of Wales) are Lady Diana Spencer ( m. 1981 ; div.\n",
       "\n",
       "1996 ) , and Camilla Parker Bowles ( m. 2005 ). The Issue of Charles (Prince of Wales) are Prince\n",
       "\n",
       "William, Duke of Cambridge , and Prince Harry, Duke of Sussex.\n",
       "\n",
       "Question: Charles was born in 1948 and has been married twice. True or False?\n",
       "\n",
       "Answer: Charles (Prince of Wales) was Born on 14 November 1948. The Spouse of Charles (Prince of\n",
       "\n",
       "Wales) are Lady Diana Spencer ( m. 1981 ; div. 1996 ) , and Camilla Parker Bowles ( m. 2005 ). Married\n",
       "\n",
       "is related to spouse. Thus, Charles was born in 1948 and has been married twice. So the answer is true.\n",
       "\n",
       "The Born of Idris Elba are 6 September 1972 (age 46) Hackney, London, England. The Residence of\n",
       "\n",
       "Idris Elba are London. The Other names of Idris Elba are DJ Big Driis, Big Driis the Londoner, Big\n",
       "\n",
       "Driis, and 7 Dub. The Occupation of Idris Elba are Actor, producer, director, musician, and DJ.\n",
       "\n",
       "Question: Idris Elba is an English entertainer. True or False?\n",
       "\n",
       "Answer: The residence of Idris Elba is London. English is related to London. The occupation of Idris\n",
       "\n",
       "Elba are actor, producer, director, musician, and DJ. Actor is a hyponym of entertainer. Musician is a\n",
       "\n",
       "hyponym of entertainer. DJ is an entertainer. Thus, Idris Elba is an English entertainer. So the answer\n",
       "\n",
       "is true.\n",
       "\n",
       "The Breed of Jean, the Vitagraph Dog are Scotch Collie. The Sex of Jean, the Vitagraph Dog are Female.\n",
       "\n",
       "The Born of Jean, the Vitagraph Dog are 1902 Eastport, Maine. The Years active of Jean, the Vitagraph\n",
       "\n",
       "Dog are 1909 - 1916.\n",
       "\n",
       "Question: Jean, the Vitagraph Dog was a Golden Retriever which perform in circus. True or False?\n",
       "\n",
       "Answer: The Breed of Jean, the Vitagraph Dog are Scotch Collie. Collie is a hyponym of dog. Retriever\n",
       "\n",
       "is a hyponym of dog. Thus, Jean, the Vitagraph Dog was not a Golden Retriever which perform in circus.\n",
       "\n",
       "So the answer is false.\n",
       "\n",
       "The Studio of Hydrograd are Sphere Studios, North Hollywood, Los Angeles. The Genre of Hydrograd\n",
       "\n",
       "are Hard rock. The Label of Hydrograd are Roadrunner. The Producer of Hydrograd are Jay Ruston.\n",
       "\n",
       "Question: Hydrograd is in the rap genre. True or False?\n",
       "\n",
       "Answer: The Genre of Hydrograd are Hard rock. Rap is distinct from rock. Thus, Hydrograd is not in\n",
       "\n",
       "the rap genre. So the answer is false.\n",
       "\n",
       "Table 8: The CoT prompt for tabular reasoning.\n",
       "\n",
       "\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 6.48 seconds| Input tokens: 5,777 | Output tokens: 251]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 6.48 seconds| Input tokens: 5,777 | Output tokens: 251]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> json</span><span style=\"background-color: #272822\">                                                                                                    </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Define the slides</span><span style=\"background-color: #272822\">                                                                                            </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">slides </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> [</span><span style=\"background-color: #272822\">                                                                                                     </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    {</span><span style=\"background-color: #272822\">                                                                                                          </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"title\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Rethinking with Retrieval: Faithful Large Language Model Inference\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"content\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: [</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Authors: Hangfeng He, Hongming Zhang, Dan Roth\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                  </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Link: https://arxiv.org/abs/2301.00303v1\"</span><span style=\"background-color: #272822\">                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        ]</span><span style=\"background-color: #272822\">                                                                                                      </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    },</span><span style=\"background-color: #272822\">                                                                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    {</span><span style=\"background-color: #272822\">                                                                                                          </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"title\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Introduction\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"content\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: [</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Large language models (LLMs) have shown exceptional performance across various tasks\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">            </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"However, the knowledge stored in LLMs may inevitably be incomplete, out-of-date, or incorrect\"</span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        ]</span><span style=\"background-color: #272822\">                                                                                                      </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    },</span><span style=\"background-color: #272822\">                                                                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    {</span><span style=\"background-color: #272822\">                                                                                                          </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"title\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Related Work\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"content\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: [</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Enhancing LMs through retrieval\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Incorporating external knowledge into LMs\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                       </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Uncovering latent knowledge in LLMs\"</span><span style=\"background-color: #272822\">                                                              </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        ]</span><span style=\"background-color: #272822\">                                                                                                      </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    },</span><span style=\"background-color: #272822\">                                                                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    {</span><span style=\"background-color: #272822\">                                                                                                          </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"title\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Rethinking with Retrieval\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                  </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"content\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: [</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Overview\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                                        </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Chain-of-thought prompting\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                      </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Sampling diverse reasoning paths\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Knowledge retrieval\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Faithful inference\"</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        ]</span><span style=\"background-color: #272822\">                                                                                                      </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    },</span><span style=\"background-color: #272822\">                                                                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    {</span><span style=\"background-color: #272822\">                                                                                                          </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"title\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Experiments\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                                </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"content\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: [</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Commonsense reasoning\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Temporal reasoning\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                              </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Tabular reasoning\"</span><span style=\"background-color: #272822\">                                                                                </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        ]</span><span style=\"background-color: #272822\">                                                                                                      </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    },</span><span style=\"background-color: #272822\">                                                                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    {</span><span style=\"background-color: #272822\">                                                                                                          </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"title\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Analysis\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"content\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: [</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Limitations of LLMs in reasoning\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Impact of the size of LMs\"</span><span style=\"background-color: #272822\">                                                                        </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        ]</span><span style=\"background-color: #272822\">                                                                                                      </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    },</span><span style=\"background-color: #272822\">                                                                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    {</span><span style=\"background-color: #272822\">                                                                                                          </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"title\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Conclusion\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">,</span><span style=\"background-color: #272822\">                                                                                 </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"content\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">: [</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"The proposed approach is a promising solution for utilizing external knowledge to assist LLMs\"</span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        ]</span><span style=\"background-color: #272822\">                                                                                                      </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    }</span><span style=\"background-color: #272822\">                                                                                                          </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"background-color: #272822\">                                                                                                              </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Convert the slides to a JSON string</span><span style=\"background-color: #272822\">                                                                          </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">slides_json </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> json</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">dumps(slides)</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Create the PowerPoint slide deck</span><span style=\"background-color: #272822\">                                                                             </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">make_powerpoint_slides(slides</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">slides_json, filename</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Rethinking_with_Retrieval.pptx\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                          </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mjson\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Define the slides\u001b[0m\u001b[48;2;39;40;34m                                                                                            \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mslides\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[48;2;39;40;34m                                                                                                     \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m{\u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mRethinking with Retrieval: Faithful Large Language Model Inference\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mcontent\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mAuthors: Hangfeng He, Hongming Zhang, Dan Roth\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLink: https://arxiv.org/abs/2301.00303v1\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                                      \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m}\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m{\u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mIntroduction\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mcontent\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLarge language models (LLMs) have shown exceptional performance across various tasks\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m            \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mHowever, the knowledge stored in LLMs may inevitably be incomplete, out-of-date, or incorrect\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                                      \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m}\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m{\u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mRelated Work\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mcontent\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mEnhancing LMs through retrieval\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mIncorporating external knowledge into LMs\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                       \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mUncovering latent knowledge in LLMs\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                              \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                                      \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m}\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m{\u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mRethinking with Retrieval\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                  \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mcontent\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mOverview\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                                        \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mChain-of-thought prompting\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mSampling diverse reasoning paths\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mKnowledge retrieval\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mFaithful inference\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                                      \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m}\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m{\u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mExperiments\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mcontent\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mCommonsense reasoning\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTemporal reasoning\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                              \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTabular reasoning\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                                      \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m}\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m{\u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mAnalysis\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mcontent\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mLimitations of LLMs in reasoning\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mImpact of the size of LMs\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                        \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                                      \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m}\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m{\u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mConclusion\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mcontent\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mThe proposed approach is a promising solution for utilizing external knowledge to assist LLMs\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                                      \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m}\u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                                              \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Convert the slides to a JSON string\u001b[0m\u001b[48;2;39;40;34m                                                                          \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mslides_json\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mjson\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdumps\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mslides\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Create the PowerPoint slide deck\u001b[0m\u001b[48;2;39;40;34m                                                                             \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mmake_powerpoint_slides\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mslides\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mslides_json\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfilename\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mRethinking_with_Retrieval.pptx\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                          \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPTX\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Out: The Powerpoint Presentation was saved as [Rethinking_with_Retrieval.pptx](Rethinking_with_Retrieval.pptx)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Out: The Powerpoint Presentation was saved as [Rethinking_with_Retrieval.pptx](Rethinking_with_Retrieval.pptx)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 24.52 seconds| Input tokens: 22,399 | Output tokens: 722]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 24.52 seconds| Input tokens: 22,399 | Output tokens: 722]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m4\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"The PowerPoint slide deck is ready.\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                            </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mThe PowerPoint slide deck is ready.\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                            \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Out - Final answer: The PowerPoint slide deck is ready.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mOut - Final answer: The PowerPoint slide deck is ready.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 5.07 seconds| Input tokens: 40,077 | Output tokens: 767]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 4: Duration 5.07 seconds| Input tokens: 40,077 | Output tokens: 767]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The PowerPoint slide deck is ready.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent.run(\"\"\"\n",
    "I need to give a presentation about the latest arxiv paper from the year 2022 that was about LLMs.\n",
    "Please make a powerpoint slide deck about this paper.\n",
    "The first slide should have the same title as the paper, and mention the authors, and give a link to the paper.\n",
    "The following slides are about the individual chapters of the paper.\n",
    "\"\"\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b25732b-daae-4e5e-b90c-a3f9129c543b",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "The following language models are available on the Server. Find out which of those are capable of generating a slide deck.\n",
    "E.g. run the prompt above for every LLM 10 times and count how often a pptx file is created.\n",
    "\n",
    "Hints: \n",
    "* You may have to specify the pptx filename to make this work.\n",
    "* To see what the agent is doing under the hood, consider setting `verbose=True`.\n",
    "\n",
    "Available models are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a4dd4-d43a-49f5-b705-9681fd02da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(base_url=\"https://llm.scads.ai/v1\",\n",
    "                       api_key=os.environ.get('SCADSAI_API_KEY'))\n",
    "\n",
    "print(\"\\n\".join([model.id for model in client.models.list().data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e28a2-898b-4141-8447-76d131aca19b",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Program your own agent that creates a powerpoint presentation for a PDF you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2750f5-e25c-42b8-980b-cdcca6bad0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
